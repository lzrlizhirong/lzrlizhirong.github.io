<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[深入学习Git工作流]]></title>
      <url>%2F2017%2F02%2F16%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Git%E5%B7%A5%E4%BD%9C%E6%B5%81%2F</url>
      <content type="text"><![CDATA[个人在学习git工作流的过程中，从原有的 SVN 模式很难完全理解git的协作模式，直到有一天我看到了下面的文章，好多遗留在心中的困惑迎刃而解。于是我将这部分资料整理如下： 我们以使用SVN的工作流来使用git有什么不妥？ git 方便的branch在哪里，团队多人如何协作？冲突了怎么办？如何进行发布控制？ 经典的master-发布、develop-主开发、hotfix-不过修复如何避免代码不经过验证上线？ 如何在github上面与他人一起协作，star-fork-pull request是怎样的流程？ 原文链接：Git Workflows and Tutorials简体中文：由 oldratlee 翻译在 github 上 git-workflows-and-tutorials 一、译序工作流其实不是一个初级主题，背后的本质问题其实是有效的项目流程管理和高效的开发协同约定，不仅是Git或SVN等VCS或SCM工具的使用。 这篇指南以大家在SVN中已经广为熟悉使用的集中式工作流作为起点，循序渐进地演进到其它高效的分布式工作流，还介绍了如何配合使用便利的Pull Request功能，体系地讲解了各种工作流的应用。 行文中实践原则和操作示例并重，对于Git的资深玩家可以梳理思考提升，而新接触的同学，也可以跟着step-by-step操作来操练学习并在实际工作中上手使用。 关于Git工作流主题，网上体系的中文资料不多，主要是零散的操作说明，希望这篇文章能让你更深入理解并在工作中灵活有效地使用起来。 PS：文中Pull Request的介绍用的是Bitbucket代码托管服务，由于和GitHub基本一样，如果你用的是GitHub（我自己也主要使用GitHub托管代码），不影响理解和操作。 PPS：本指南循序渐进地讲解工作流，如果Git用的不多，可以从前面的讲的工作流开始操练。操作过程去感受指南的讲解：解决什么问题、如何解决问题，这样理解就深了，也方便活用。 Gitflow工作流是经典模型，体现了工作流的经验和精髓。随着项目过程复杂化，会感受到这个工作流中深思熟虑和威力！ Forking工作流是协作的（GitHub风格）可以先看看Github的Help：Fork A Repo和Using pull requests 。照着操作，给一个Github项目贡献你的提交，有操作经验再看指南容易意会。指南中给了自己实现Fork的方法：Fork就是服务端的克隆。在指南的操练中使用代码托管服务（如GitHub、Bitbucket），可以点一下按钮就让开发者完成仓库的fork操作。 :see_no_evil: 自己理解粗浅，翻译中不足和不对之处，欢迎建议（提交Issue）和指正（Fork后提交代码）！ 二、Git工作流指南:point_right: 工作流有各式各样的用法，但也正因此使得在实际工作中如何上手使用变得很头大。这篇指南通过总览公司团队中最常用的几种Git工作流让大家可以上手使用。 在阅读的过程中请记住，本文中的几种工作流是作为方案指导而不是条例规定。在展示了各种工作流可能的用法后，你可以从不同的工作流中挑选或揉合出一个满足你自己需求的工作流。 集中式工作流如果你的开发团队成员已经很熟悉Subversion，集中式工作流让你无需去适应一个全新流程就可以体验Git带来的收益。这个工作流也可以作为向更Git风格工作流迁移的友好过渡。转到分布式版本控制系统看起来像个令人生畏的任务，但不改变已用的工作流你也可以用上Git带来的收益。团队可以用和Subversion完全不变的方式来开发项目。 但使用Git加强开发的工作流，Git有相比SVN的几个优势。首先，每个开发可以有属于自己的整个工程的本地拷贝。隔离的环境让各个开发者的工作和项目的其他部分修改独立开来 ——即自由地提交到自己的本地仓库，先完全忽略上游的开发，直到方便的时候再把修改反馈上去。 其次，Git提供了强壮的分支和合并模型。不像SVN，Git的分支设计成可以做为一种用来在仓库之间集成代码和分享修改的『失败安全』的机制。 工作方式像Subversion一样，集中式工作流以中央仓库作为项目所有修改的单点实体。相比SVN缺省的开发分支trunk，Git叫做master，所有修改提交到这个分支上。本工作流只用到master这一个分支。 开发者开始先克隆中央仓库。在自己的项目拷贝中像SVN一样的编辑文件和提交修改；但修改是存在本地的，和中央仓库是完全隔离的。开发者可以把和上游的同步延后到一个方便时间点。 要发布修改到正式项目中，开发者要把本地master分支的修改『推』到中央仓库中。这相当于svn commit操作，但push操作会把所有还不在中央仓库的本地提交都推上去。 冲突解决中央仓库代表了正式项目，所以提交历史应该被尊重且是稳定不变的。如果开发者本地的提交历史和中央仓库有分歧，Git会拒绝push提交否则会覆盖已经在中央库的正式提交。在开发者提交自己功能修改到中央库前，需要先fetch在中央库的新增提交，rebase自己提交到中央库提交历史之上。这样做的意思是在说，『我要把自己的修改加到别人已经完成的修改上。』最终的结果是一个完美的线性历史，就像以前的SVN的工作流中一样。 如果本地修改和上游提交有冲突，Git会暂停rebase过程，给你手动解决冲突的机会。Git解决合并冲突，用和生成提交一样的git status和git add命令，很一致方便。还有一点，如果解决冲突时遇到麻烦，Git可以很简单中止整个rebase操作，重来一次（或者让别人来帮助解决）。 示例让我们一起逐步分解来看看一个常见的小团队如何用这个工作流来协作的。有两个开发者小明和小红，看他们是如何开发自己的功能并提交到中央仓库上的。 有人先初始化好中央仓库第一步，有人在服务器上创建好中央仓库。如果是新项目，你可以初始化一个空仓库；否则你要导入已有的Git或SVN仓库。 中央仓库应该是个裸仓库（bare repository），即没有工作目录（working directory）的仓库。可以用下面的命令创建：12ssh user@hostgit init --bare /path/to/repo.git 确保写上有效的user（SSH的用户名），host（服务器的域名或IP地址），/path/to/repo.git（你想存放仓库的位置）。注意，为了表示是一个裸仓库，按照约定加上.git扩展名到仓库名上。 所有人克隆中央仓库下一步，各个开发者创建整个项目的本地拷贝。通过git clone命令完成：1git clone ssh://user@host/path/to/repo.git 基于你后续会持续和克隆的仓库做交互的假设，克隆仓库时Git会自动添加远程别名origin指回『父』仓库。 小明开发功能在小明的本地仓库中，他使用标准的Git过程开发功能：编辑、暂存（Stage）和提交。如果你不熟悉暂存区（Staging Area），这里说明一下：暂存区的用来准备一个提交，但可以不用把工作目录中所有的修改内容都包含进来。这样你可以创建一个高度聚焦的提交，尽管你本地修改很多内容。123git status # 查看本地仓库的修改状态git add # 暂存文件git commit # 提交文件 请记住，因为这些命令生成的是本地提交，小明可以按自己需求反复操作多次，而不用担心中央仓库上有了什么操作。对需要多个更简单更原子分块的大功能，这个做法是很有用的。 小红开发功能与此同时，小红在自己的本地仓库中用相同的编辑、暂存和提交过程开发功能。和小明一样，她也不关心中央仓库有没有新提交；当然更不关心小明在他的本地仓库中的操作，因为所有本地仓库都是私有的。 小明发布功能一旦小明完成了他的功能开发，会发布他的本地提交到中央仓库中，这样其它团队成员可以看到他的修改。他可以用下面的git push命令：1git push origin master 注意，origin是在小明克隆仓库时Git创建的远程中央仓库别名。master参数告诉Git推送的分支。由于中央仓库自从小明克隆以来还没有被更新过，所以push操作不会有冲突，成功完成。 小红试着发布功能一起来看看在小明发布修改后，小红push修改会怎么样？她使用完全一样的push命令：1git push origin master 但她的本地历史已经和中央仓库有分岐了，Git拒绝操作并给出下面很长的出错消息：12345error: failed to push some refs to &apos;/path/to/repo.git&apos;hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Merge the remote changes (e.g. &apos;git pull&apos;)hint: before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 这避免了小红覆写正式的提交。她要先pull小明的更新到她的本地仓库合并上她的本地修改后，再重试。 小红在小明的提交之上rebase小红用git pull合并上游的修改到自己的仓库中。这条命令类似svn update——拉取所有上游提交命令到小红的本地仓库，并尝试和她的本地修改合并：1git pull --rebase origin master –rebase选项告诉Git把小红的提交移到同步了中央仓库修改后的master分支的顶部，如下图所示：如果你忘加了这个选项，pull操作仍然可以完成，但每次pull操作要同步中央仓库中别人修改时，提交历史会以一个多余的『合并提交』结尾。对于集中式工作流，最好是使用rebase而不是生成一个合并提交。 小红解决合并冲突rebase操作过程是把本地提交一次一个地迁移到更新了的中央仓库master分支之上。这意味着可能要解决在迁移某个提交时出现的合并冲突，而不是解决包含了所有提交的大型合并时所出现的冲突。这样的方式让你尽可能保持每个提交的聚焦和项目历史的整洁。反过来，简化了哪里引入Bug的分析，如果有必要，回滚修改也可以做到对项目影响最小。 如果小红和小明的功能是相关的，不大可能在rebase过程中有冲突。如果有，Git在合并有冲突的提交处暂停rebase过程，输出下面的信息并带上相关的指令：1CONFLICT (content): Merge conflict in &lt;some-file&gt; Git很赞的一点是，任何人可以解决他自己的冲突。在这个例子中，小红可以简单的运行git status命令来查看哪里有问题。冲突文件列在Unmerged paths（未合并路径）一节中：12345# Unmerged paths:# (use &quot;git reset HEAD &lt;some-file&gt;...&quot; to unstage)# (use &quot;git add/rm &lt;some-file&gt;...&quot; as appropriate to mark resolution)## both modified: &lt;some-file&gt; 接着小红编辑这些文件。修改完成后，用老套路暂存这些文件，并让git rebase完成剩下的事：12git add &lt;some-file&gt; git rebase --continue 要做的就这些了。Git会继续一个一个地合并后面的提交，如其它的提交有冲突就重复这个过程。 如果你碰到了冲突，但发现搞不定，不要惊慌。只要执行下面这条命令，就可以回到你执行git pull –rebase命令前的样子：1git rebase --abort 小红成功发布功能小红完成和中央仓库的同步后，就能成功发布她的修改了：1git push origin master 如你所见，仅使用几个Git命令我们就可以模拟出传统Subversion开发环境。对于要从SVN迁移过来的团队来说这太好了，但没有发挥出Git分布式本质的优势。 如果你的团队适应了集中式工作流，但想要更流畅的协作效果，绝对值得探索一下 功能分支工作流 的收益。通过为一个功能分配一个专门的分支，能够做到一个新增功能集成到正式项目之前对新功能进行深入讨论。 功能分支工作流功能分支工作流以集中式工作流为基础，不同的是为各个新功能分配一个专门的分支来开发。这样可以在把新功能集成到正式项目前，用Pull Requests的方式讨论变更。一旦你玩转了集中式工作流，在开发过程中可以很简单地加上功能分支，用来鼓励开发者之间协作和简化交流。 功能分支工作流背后的核心思路是所有的功能开发应该在一个专门的分支，而不是在master分支上。这个隔离可以方便多个开发者在各自的功能上开发而不会弄乱主干代码。另外，也保证了master分支的代码一定不会是有问题的，极大有利于集成环境。 功能开发隔离也让pull requests工作流成功可能，pull requests工作流能为每个分支发起一个讨论，在分支合入正式项目之前，给其它开发者有表示赞同的机会。另外，如果你在功能开发中有问题卡住了，可以开一个pull requests来向同学们征求建议。这些做法的重点就是，pull requests让团队成员之间互相评论工作变成非常方便！ 工作方式功能分支工作流仍然用中央仓库，并且master分支还是代表了正式项目的历史。但不是直接提交本地历史到各自的本地master分支，开发者每次在开始新功能前先创建一个新分支。功能分支应该有个有描述性的名字，比如animated-menu-items或issue-#1061，这样可以让分支有个清楚且高聚焦的用途。 在master分支和功能分支之间，Git是没有技术上的区别，所以开发者可以用和集中式工作流中完全一样的方式编辑、暂存和提交修改到功能分支上。 另外，功能分支也可以（且应该）push到中央仓库中。这样不修改正式代码就可以和其它开发者分享提交的功能。由于master仅有的一个『特殊』分支，在中央仓库上存多个功能分支不会有任何问题。当然，这样做也可以很方便地备份各自的本地提交。 Pull Requests功能分支除了可以隔离功能的开发，也使得通过Pull Requests讨论变更成为可能。一旦某个开发完成一个功能，不是立即合并到master，而是push到中央仓库的功能分支上并发起一个Pull Request请求去合并修改到master。在修改成为主干代码前，这让其它的开发者有机会先去Review变更。 Code Review是Pull Requests的一个重要的收益，但Pull Requests目的是讨论代码一个通用方式。你可以把Pull Requests作为专门给某个分支的讨论。这意味着可以在更早的开发过程中就可以进行Code Review。比如，一个开发者开发功能需要帮助时，要做的就是发起一个Pull Request，相关的人就会自动收到通知，在相关的提交旁边能看到需要帮助解决的问题。 一旦Pull Request被接受了，发布功能要做的就和集中式工作流就很像了。首先，确定本地的master分支和上游的master分支是同步的。然后合并功能分支到本地master分支并push已经更新的本地master分支到中央仓库。 仓库管理的产品解决方案像Bitbucket或Stash，可以良好地支持Pull Requests。可以看看Stash的Pull Requests文档。 示例下面的示例演示了如何把Pull Requests作为Code Review的方式，但注意Pull Requests可以用于很多其它的目的。 小红开始开发一个新功能在开始开发功能前，小红需要一个独立的分支。使用下面的命令新建一个分支：1git checkout -b marys-feature master 这个命令检出一个基于master名为marys-feature的分支，Git的-b选项表示如果分支还不存在则新建分支。这个新分支上，小红按老套路编辑、暂存和提交修改，按需要提交以实现功能：123git statusgit add &lt;some-file&gt;git commit 小红要去吃个午饭早上小红为新功能添加一些提交。去吃午饭前，push功能分支到中央仓库是很好的做法，这样可以方便地备份，如果和其它开发协作，也让他们可以看到小红的提交。1git push -u origin marys-feature 这条命令push marys-feature分支到中央仓库（origin），-u选项设置本地分支去跟踪远程对应的分支。设置好跟踪的分支后，小红就可以使用git push命令省去指定推送分支的参数。 小红完成功能开发小红吃完午饭回来，完成整个功能的开发。在合并到master之前，她发起一个Pull Request让团队的其它人知道功能已经完成。但首先，她要确认中央仓库中已经有她最近的提交：1git push 然后，在她的Git GUI客户端中发起Pull Request，请求合并marys-feature到master，团队成员会自动收到通知。Pull Request很酷的是可以在相关的提交旁边显示评注，所以你可以很对某个变更集提问。 小黑收到Pull Request小黑收到了Pull Request后会查看marys-feature的修改。决定在合并到正式项目前是否要做些修改，且通过Pull Request和小红来回地讨论。 小红再做修改要再做修改，小红用和功能第一个迭代完全一样的过程。编辑、暂存、提交并push更新到中央仓库。小红这些活动都会显示在Pull Request上，小黑可以断续做评注。 如果小黑有需要，也可以把marys-feature分支拉到本地，自己来修改，他加的提交也会一样显示在Pull Request上。 小红发布她的功能一旦小黑可以的接受Pull Request，就可以合并功能到稳定项目代码中（可以由小黑或是小红来做这个操作）：1234git checkout mastergit pullgit pull origin marys-featuregit push 无论谁来做合并，首先要检出master分支并确认是它是最新的。然后执行git pull origin marys-feature合并marys-feature分支到和已经和远程一致的本地master分支。你可以使用简单git merge marys-feature命令，但前面的命令可以保证总是最新的新功能分支。最后更新的master分支要重新push回到origin。 这个过程常常会生成一个合并提交。有些开发者喜欢有合并提交，因为它像一个新功能和原来代码基线的连通符。但如果你偏爱线性的提交历史，可以在执行合并时rebase新功能到master分支的顶部，这样生成一个快进（fast-forward）的合并。 一些GUI客户端可以只要点一下『接受』按钮执行好上面的命令来自动化Pull Request接受过程。如果你的不能这样，至少在功能合并到master分支后能自动关闭Pull Request。 与此同时，小明在做和小红一样的事当小红和小黑在marys-feature上工作并讨论她的Pull Request的时候，小明在自己的功能分支上做完全一样的事。 通过隔离功能到独立的分支上，每个人都可以自主的工作，当然必要的时候在开发者之间分享变更还是比较繁琐的。 到了这里，但愿你发现了功能分支可以很直接地在 集中式工作流 的仅有的master分支上完成多功能的开发。另外，功能分支还使用了Pull Request，使得可以在你的版本控制GUI客户端中讨论某个提交。 功能分支工作流是开发项目异常灵活的方式。问题是，有时候太灵活了。对于大型团队，常常需要给不同分支分配一个更具体的角色。Gitflow工作流是管理功能开发、发布准备和维护的常用模式。 Gitflow工作流Gitflow工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。这节介绍的Gitflow工作流借鉴自在nvie的Vincent Driessen。 Gitflow工作流定义了一个围绕项目发布的严格分支模型。虽然比功能分支工作流复杂几分，但提供了用于一个健壮的用于管理大型项目的框架。 Gitflow工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。除了使用功能分支，在做准备、维护和记录发布也使用各自的分支。当然你可以用上功能分支工作流所有的好处：Pull Requests、隔离实验性开发和更高效的协作。 工作方式Gitflow工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并push分支到要中央仓库中。 历史分支相对使用仅有的一个master分支，Gitflow工作流使用2个分支来记录项目的历史。master分支存储了正式发布的历史，而develop分支作为功能的集成分支。这样也方便master分支上的所有提交分配一个版本号。剩下要说明的问题围绕着这2个分支的区别展开。 功能分支每个新功能位于一个自己的分支，这样可以push到中央仓库以备份和协作。但功能分支不是从master分支上拉出新分支，而是使用develop分支作为父分支。当新功能完成时，合并回develop分支。新功能提交应该从不直接与master分支交互。注意，从各种含义和目的上来看，功能分支加上develop分支就是功能分支工作流的用法。但Gitflow工作流没有在这里止步。 发布分支一旦develop分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从develop分支上fork一个发布分支。新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上——这个分支只应该做Bug修复、文档生成和其它面向发布任务。一旦对外发布的工作都完成了，发布分支合并到master分支并分配一个版本号打好Tag。另外，这些从新建发布分支以来的做的修改要合并回develop分支。 使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本4.0』，并且在仓库的目录结构中可以实际看到）。 常用的分支约定：123用于新建发布分支的分支: develop用于合并的分支: master分支命名: release-* 或 release/* 维护分支维护分支或说是热修复（hotfix）分支用于生成快速给产品发布版本（production releases）打补丁，这是唯一可以直接从master分支fork出来的分支。修复完成，修改应该马上合并回master分支和develop分支（当前的发布分支），master分支应该用新的版本号打好Tag。 为Bug修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。你可以把维护分支想成是一个直接在master分支上处理的临时发布。 示例下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。 创建开发分支第一步为master分支配套一个develop分支。简单来做可以本地创建一个空的develop分支，push到服务器上：12git branch developgit push -u origin develop 以后这个分支将会包含了项目的全部历史，而master分支将只包含了部分历史。其它开发者这时应该克隆中央仓库，建好develop分支的跟踪分支：12git clone ssh://user@host/path/to/repo.gitgit checkout -b develop origin/develop 现在每个开发都有了这些历史分支的本地拷贝。 小红和小明开始开发新功能这个示例中，小红和小明开始各自的功能开发。他们需要为各自的功能创建相应的分支。新分支不是基于master分支，而是应该基于develop分支：1git checkout -b some-feature develop 他们用老套路添加提交到各自功能分支上：编辑、暂存、提交：123git statusgit add &lt;some-file&gt;git commit 小红完成功能开发添加了提交后，小红觉得她的功能OK了。如果团队使用Pull Requests，这时候可以发起一个用于合并到develop分支。否则她可以直接合并到她本地的develop分支后push到中央仓库：12345git pull origin developgit checkout developgit merge some-featuregit pushgit branch -d some-feature 第一条命令在合并功能前确保develop分支是最新的。注意，功能决不应该直接合并到master分支。冲突解决方法和集中式工作流一样。 小红开始准备发布这个时候小明正在实现他的功能，小红开始准备她的第一个项目正式发布。像功能开发一样，她用一个新的分支来做发布准备。这一步也确定了发布的版本号：1git checkout -b release-0.1 develop 这个分支是清理发布、执行所有测试、更新文档和其它为下个发布做准备操作的地方，像是一个专门用于改善发布的功能分支。 只要小红创建这个分支并push到中央仓库，这个发布就是功能冻结的。任何不在develop分支中的新功能都推到下个发布循环中。 小红完成发布一旦准备好了对外发布，小红合并修改到master分支和develop分支上，删除发布分支。合并回develop分支很重要，因为在发布分支中已经提交的更新需要在后面的新功能中也要是可用的。另外，如果小红的团队要求Code Review，这是一个发起Pull Request的理想时机。1234567git checkout mastergit merge release-0.1git pushgit checkout developgit merge release-0.1git pushgit branch -d release-0.1 发布分支是作为功能开发（develop分支）和对外发布（master分支）间的缓冲。只要有合并到master分支，就应该打好Tag以方便跟踪。12git tag -a 0.1 -m &quot;Initial public release&quot; mastergit push --tags Git有提供各种勾子（hook），即仓库有事件发生时触发执行的脚本。可以配置一个勾子，在你push中央仓库的master分支时，自动构建好对外发布。 最终用户发现Bug对外发布后，小红回去和小明一起做下个发布的新功能开发，直到有最终用户开了一个Ticket抱怨当前版本的一个Bug。为了处理Bug，小红（或小明）从master分支上拉出了一个维护分支，提交修改以解决问题，然后直接合并回master分支：12345git checkout -b issue-#001 master# Fix the buggit checkout mastergit merge issue-#001git push 就像发布分支，维护分支中新加这些重要修改需要包含到develop分支中，所以小红要执行一个合并操作。然后就可以安全地删除这个分支了：1234git checkout developgit merge issue-#001git pushgit branch -d issue-#001 到了这里，但愿你对集中式工作流、功能分支工作流和Gitflow工作流已经感觉很舒适了。你应该也牢固的掌握了本地仓库的潜能，push/pull模式和Git健壮的分支和合并模型。 记住，这里演示的工作流只是可能用法的例子，而不是在实际工作中使用Git不可违逆的条例。所以不要畏惧按自己需要对工作流的用法做取舍。不变的目标就是让Git为你所用。 Forking工作流Forking工作流是分布式工作流，充分利用了Git在分支和克隆上的优势。可以安全可靠地管理大团队的开发者（developer），并能接受不信任贡献者（contributor）的提交。 Forking工作流和前面讨论的几种工作流有根本的不同，这种工作流不是使用单个服务端仓库作为『中央』代码基线，而让各个开发者都有一个服务端仓库。这意味着各个代码贡献者有2个Git仓库而不是1个：一个本地私有的，另一个服务端公开的。Forking工作流的一个主要优势是，贡献的代码可以被集成，而不需要所有人都能push代码到仅有的中央仓库中。开发者push到自己的服务端仓库，而只有项目维护者才能push到正式仓库。这样项目维护者可以接受任何开发者的提交，但无需给他正式代码库的写权限。 效果就是一个分布式的工作流，能为大型、自发性的团队（包括了不受信的第三方）提供灵活的方式来安全的协作。也让这个工作流成为开源项目的理想工作流。 工作方式和其它的Git工作流一样，Forking工作流要先有一个公开的正式仓库存储在服务器上。但一个新的开发者想要在项目上工作时，不是直接从正式仓库克隆，而是fork正式项目在服务器上创建一个拷贝。 这个仓库拷贝作为他个人公开仓库 ——其它开发者不允许push到这个仓库，但可以pull到修改（后面我们很快就会看这点很重要）。在创建了自己服务端拷贝之后，和之前的工作流一样，开发者执行git clone命令克隆仓库到本地机器上，作为私有的开发环境。 要提交本地修改时，push提交到自己公开仓库中 —— 而不是正式仓库中。然后，给正式仓库发起一个pull request，让项目维护者知道有更新已经准备好可以集成了。对于贡献的代码，pull request也可以很方便地作为一个讨论的地方。 为了集成功能到正式代码库，维护者pull贡献者的变更到自己的本地仓库中，检查变更以确保不会让项目出错，合并变更到自己本地的master分支，然后pushmaster分支到服务器的正式仓库中。到此，贡献的提交成为了项目的一部分，其它的开发者应该执行pull操作与正式仓库同步自己本地仓库。 正式仓库在Forking工作流中，『官方』仓库的叫法只是一个约定，理解这点很重要。从技术上来看，各个开发者仓库和正式仓库在Git看来没有任何区别。事实上，让正式仓库之所以正式的唯一原因是它是项目维护者的公开仓库。 Forking工作流的分支使用方式所有的个人公开仓库实际上只是为了方便和其它的开发者共享分支。各个开发者应该用分支隔离各个功能，就像在功能分支工作流和Gitflow工作流一样。唯一的区别是这些分支被共享了。在Forking工作流中这些分支会被pull到另一个开发者的本地仓库中，而在功能分支工作流和Gitflow工作流中是直接被push到正式仓库中。 示例项目维护者初始化正式仓库和任何使用Git项目一样，第一步是创建在服务器上一个正式仓库，让所有团队成员都可以访问到。通常这个仓库也会作为项目维护者的公开仓库。 公开仓库应该是裸仓库，不管是不是正式代码库。所以项目维护者会运行像下面的命令来搭建正式仓库：12ssh user@hostgit init --bare /path/to/repo.git Bitbucket和Stash提供了一个方便的GUI客户端以完成上面命令行做的事。这个搭建中央仓库的过程和前面提到的工作流完全一样。如果有现存的代码库，维护者也要push到这个仓库中。 开发者fork正式仓库其它所有的开发需要fork正式仓库。可以用git clone命令用SSH协议连通到服务器，拷贝仓库到服务器另一个位置 —— 是的，fork操作基本上就只是一个服务端的克隆。Bitbucket和Stash上可以点一下按钮就让开发者完成仓库的fork操作。 这一步完成后，每个开发都在服务端有一个自己的仓库。和正式仓库一样，这些仓库应该是裸仓库。 开发者克隆自己fork出来的仓库下一步，各个开发者要克隆自己的公开仓库，用熟悉的git clone命令。 在这个示例中，假定用Bitbucket托管了仓库。记住，如果这样的话各个开发者需要有各自的Bitbucket账号，使用下面命令克隆服务端自己的仓库：1git clone https://user@bitbucket.org/user/repo.git 相比前面介绍的工作流只用了一个origin远程别名指向中央仓库，Forking工作流需要2个远程别名 ——一个指向正式仓库，另一个指向开发者自己的服务端仓库。别名的名字可以任意命名，常见的约定是使用origin作为远程克隆的仓库的别名（这个别名会在运行git clone自动创建），upstream（上游）作为正式仓库的别名。1git remote add upstream https://bitbucket.org/maintainer/repo 需要自己用上面的命令创建upstream别名。这样可以简单地保持本地仓库和正式仓库的同步更新。注意，如果上游仓库需要认证（比如不是开源的），你需要提供用户：1git remote add upstream https://user@bitbucket.org/maintainer/repo.git 这时在克隆和pull正式仓库时，需要提供用户的密码。 开发者开发自己的功能在刚克隆的本地仓库中，开发者可以像其它工作流一样的编辑代码、提交修改和新建分支：123git checkout -b some-feature# Edit some codegit commit -a -m &quot;Add first draft of some feature&quot; 所有的修改都是私有的直到push到自己公开仓库中。如果正式项目已经往前走了，可以用git pull命令获得新的提交：1git pull upstream master 由于开发者应该都在专门的功能分支上工作，pull操作结果会都是快进合并。 开发者发布自己的功能一旦开发者准备好了分享新功能，需要做二件事。首先，通过push他的贡献代码到自己的公开仓库中，让其它的开发者都可以访问到。他的origin远程别名应该已经有了，所以要做的就是：1git push origin feature-branch 这里和之前的工作流的差异是，origin远程别名指向开发者自己的服务端仓库，而不是正式仓库。 第二件事，开发者要通知项目维护者，想要合并他的新功能到正式库中。Bitbucket和Stash提供了Pull Request按钮，弹出表单让你指定哪个分支要合并到正式仓库。一般你会想集成你的功能分支到上游远程仓库的master分支中。 项目维护者集成开发者的功能当项目维护者收到pull request，他要做的是决定是否集成它到正式代码库中。有二种方式来做： 直接在pull request中查看代码 pull代码到他自己的本地仓库，再手动合并第一种做法更简单，维护者可以在GUI中查看变更的差异，做评注和执行合并。但如果出现了合并冲突，需要第二种做法来解决。这种情况下，维护者需要从开发者的服务端仓库中fetch功能分支，合并到他本地的master分支，解决冲突：1234git fetch https://bitbucket.org/user/repo feature-branch# 查看变更git checkout mastergit merge FETCH_HEAD 变更集成到本地的master分支后，维护者要push变更到服务器上的正式仓库，这样其它的开发者都能访问到：1git push origin master 注意，维护者的origin是指向他自己公开仓库的，即是项目的正式代码库。到此，开发者的贡献完全集成到了项目中。 开发者和正式仓库做同步由于正式代码库往前走了，其它的开发需要和正式仓库做同步：1git pull upstream master 如果你之前是使用SVN，Forking工作流可能看起来像是一个激进的范式切换（paradigm shift）。但不要害怕，这个工作流实际上就是在功能分支工作流之上引入另一个抽象层。不是直接通过单个中央仓库来分享分支，而是把贡献代码发布到开发者自己的服务端仓库中。 示例中解释了，一个贡献如何从一个开发者流到正式的master分支中，但同样的方法可以把贡献集成到任一个仓库中。比如，如果团队的几个人协作实现一个功能，可以在开发之间用相同的方法分享变更，完全不涉及正式仓库。 这使得Forking工作流对于松散组织的团队来说是个非常强大的工具。任一开发者可以方便地和另一开发者分享变更，任何分支都能有效地合并到正式代码库中。 Pull RequestsPull requests是Bitbucket提供的让开发者更方便地进行协作的功能，提供了友好的Web界面可以在提议的修改合并到正式项目之前对修改进行讨论。开发者向团队成员通知功能开发已经完成，Pull Requests是最简单的用法。开发者完成功能开发后，通过Bitbucket账号发起一个Pull Request。这样让涉及这个功能的所有人知道要去做Code Review和合并到master分支。 但是，Pull Request远不止一个简单的通知，而是为讨论提交的功能的一个专门论坛。如果变更有任何问题，团队成员反馈在Pull Request中，甚至push新的提交微调功能。所有的这些活动都直接跟踪在Pull Request中。相比其它的协作模型，这种分享提交的形式有助于打造一个更流畅的工作流。SVN和Git都能通过一个简单的脚本收到通知邮件；但是，讨论变更时，开发者通常只能去回复邮件。这样做会变得杂乱，尤其还要涉及后面的几个提交时。Pull Requests把所有相关功能整合到一个和Bitbucket仓库界面集成的用户友好Web界面中。 解析Pull Request当要发起一个Pull Request，你所要做的就是请求（Request）另一个开发者（比如项目的维护者）来pull你仓库中一个分支到他的仓库中。这意味着你要提供4个信息以发起Pull Request：源仓库、源分支、目的仓库、目的分支。这几值多数Bitbucket都会设置上合适的缺省值。但取决你用的协作工作流，你的团队可能会要指定不同的值。上图显示了一个Pull Request请求合并一个功能分支到正式的master分支上，但可以有多种不同的Pull Request用法。 工作方式Pull Request可以和功能分支工作流、Gitflow工作流或Forking工作流一起使用。但一个Pull Request要求要么分支不同要么仓库不同，所以不能用于集中式工作流。在不同的工作流中使用Pull Request会有一些不同，但基本的过程是这样的： 开发者在本地仓库中新建一个专门的分支开发功能。 开发者push分支修改到公开的Bitbucket仓库中。 开发者通过Bitbucket发起一个Pull Request。 团队的其它成员review code，讨论并修改。 项目维护者合并功能到官方仓库中并关闭Pull Request。 本文后面内容说明，Pull Request在不同协作工作流中如何应用。 在功能分支工作流中使用Pull Request功能分支工作流用一个共享的Bitbucket仓库来管理协作，开发者在专门的分支上开发功能。但不是立即合并到master分支上，而是在合并到主代码库之前开发者应该开一个Pull Request发起功能的讨论。功能分支工作流只有一个公开的仓库，所以Pull Request的目的仓库和源仓库总是同一个。通常开发者会指定他的功能分支作为源分支，master分支作为目的分支。 收到Pull Request后，项目维护者要决定如何做。如果功能没问题，就简单地合并到master分支，关闭Pull Request。但如果提交的变更有问题，他可以在Pull Request中反馈。之后新加的提交也会评论之后接着显示出来。 在功能还没有完全开发完的时候，也可能发起一个Pull Request。比如开发者在实现某个需求时碰到了麻烦，他可以发一个包含正在进行中工作的Pull Request。其它的开发者可以在Pull Request提供建议，或者甚至直接添加提交来解决问题。 在Gitflow工作流中使用Pull RequestGitflow工作流和功能分支工作流类似，但围绕项目发布定义一个严格的分支模型。在Gitflow工作流中使用Pull Request让开发者在发布分支或是维护分支上工作时，可以有个方便的地方对关于发布分支或是维护分支的问题进行交流。Gitflow工作流中Pull Request的使用过程和上一节中完全一致：当一个功能、发布或是热修复分支需要Review时，开发者简单发起一个Pull Request，团队的其它成员会通过Bitbucket收到通知。 新功能一般合并到develop分支，而发布和热修复则要同时合并到develop分支和master分支上。Pull Request可能用做所有合并的正式管理。 在Forking工作流中使用Pull Request在Forking工作流中，开发者push完成的功能到他自己的仓库中，而不是共享仓库。然后，他发起一个Pull Request，让项目维护者知道他的功能已经可以Review了。 在这个工作流，Pull Request的通知功能非常有用，因为项目维护者不可能知道其它开发者在他们自己的仓库添加了提交。由于各个开发有自己的公开仓库，Pull Request的源仓库和目标仓库不是同一个。源仓库是开发者的公开仓库，源分支是包含了修改的分支。如果开发者要合并修改到正式代码库中，那么目标仓库是正式仓库，目标分支是master分支。 Pull Request也可以用于正式项目之外的其它开发者之间的协作。比如，如果一个开发者和一个团队成员一起开发一个功能，他们可以发起一个Pull Request，用团队成员的Bitbucket仓库作为目标，而不是正式项目的仓库。然后使用相同的功能分支作为源和目标分支。2个开发者之间可以在Pull Request中讨论和开发功能。完成开发后，他们可以发起另一个Pull Request，请求合并功能到正式的master分支。在Forking工作流中，这样的灵活性让Pull Request成为一个强有力的协作工具。 示例下面的示例演示了Pull Request如何在在Forking工作流中使用。也同样适用于小团队的开发协作和第三方开发者向开源项目的贡献。 在示例中，小红是个开发，小明是项目维护者。他们各自有一个公开的Bitbucket仓库，而小明的仓库包含了正式工程。 小红fork正式项目小红先要fork小明的Bitbucket仓库，开始项目的开发。她登陆Bitbucket，浏览到小明的仓库页面，点Fork按钮。然后为fork出来的仓库填写名字和描述，这样小红就有了服务端的项目拷贝了。 小红克隆她的Bitbucket仓库下一步，小红克隆自己刚才fork出来的Bitbucket仓库，以在本机上准备出工作拷贝。命令如下：1git clone https://user@bitbucket.org/user/repo.git 请记住，git clone会自动创建origin远程别名，是指向小红fork出来的仓库。 小红开发新功能在开始改代码前，小红要为新功能先新建一个新分支。她会用这个分支作为Pull Request的源分支。123git checkout -b some-feature# 编辑代码git commit -a -m &quot;Add first draft of some feature&quot; 在新功能分支上，小红按需要添加提交。甚至如果小红觉得功能分支上的提交历史太乱了，她可以用交互式rebase来删除或压制提交。对于大型项目，整理功能分支的历史可以让项目维护者更容易看出在Pull Request中做了什么内容。 小红push功能到她的Bitbucket仓库中小红完成了功能后，push功能到她自己的Bitbucket仓库中（不是正式仓库），用下面简单的命令：1git push origin some-branch 这时她的变更可以让项目维护者看到了（或者任何想要看的协作者）。 小红发起Pull RequestBitbucket上有了她的功能分支后，小红可以用她的Bitbucket账号浏览到她的fork出来的仓库页面，点右上角的【Pull Request】按钮，发起一个Pull Request。弹出的表单自动设置小红的仓库为源仓库，询问小红以指定源分支、目标仓库和目标分支。 小红想要合并功能到正式仓库，所以源分支是她的功能分支，目标仓库是小明的公开仓库，而目标分支是master分支。另外，小红需要提供Pull Request的标题和描述信息。如果需要小明以外的人审核批准代码，她可以把这些人填在【Reviewers】文本框中。创建好了Pull Request，通知会通过Bitbucket系统消息或邮件（可选）发给小明。 小明review Pull Request在小明的Bitbucket仓库页面的【Pull Request】Tab可以看到所有人发起的Pull Request。点击小红的Pull Request会显示出Pull Request的描述、功能的提交历史和每个变更的差异（diff）。 如果小明想要合并到项目中，只要点一下【Merge】按钮，就可以同意Pull Request并合并到master分支。 但如果像这个示例中一样小明发现了在小红的代码中的一个小Bug，要小红在合并前修复。小明可以在整个Pull Request上加上评注，或是选择历史中的某个提交加上评注。 小红补加提交如果小红对反馈有任何疑问，可以在Pull Request中响应，把Pull Request当作是她功能讨论的论坛。 小红在她的功能分支新加提交以解决代码问题，并push到她的Bitbucket仓库中，就像前一轮中的做法一样。这些提交会进入的Pull Request，小明在原来的评注旁边可以再次review变更。 小明接受Pull Request最终，小明接受变更，合并功能分支到master分支，并关闭Pull Request。至此，功能集成到项目中，其它的项目开发者可以用标准的git pull命令pull这些变更到自己的本地仓库中。 到了这里，你应该有了所有需要的工具来集成Pull Request到你自己的工作流。请记住，Pull Request并不是为了替代任何 基于Git的协作工作流，而是它们的一个便利的补充，让团队成员间的协作更轻松方便。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[提高命令效率之利器oh-my-zsh]]></title>
      <url>%2F2017%2F02%2F15%2F%E6%8F%90%E9%AB%98%E5%91%BD%E4%BB%A4%E6%95%88%E7%8E%87%E4%B9%8B%E5%88%A9%E5%99%A8oh-my-zsh%2F</url>
      <content type="text"><![CDATA[当我知道了zsh，并体验了5分钟的时候，我决定将zsh作为我的默认 shell 终端。为什么？效率提高实在是太简单啦！从这里你可能也就知道了zsh是 shell 的一种，当然还包括目前估计是你默认的bash ，输入下面的命令，就能看到你的系统中提供了多少的 shell ：1cat /etc/shells 前人已经有好多使用zsh的，所以这类的文章也很多，包括怎么安装、使用技巧等等，请看： 池建强-终极 Shell 使用 zsh 的九个理由 ZSH Tips by ZZapper-很全的zsh命令汇总 我在用的mac软件(2)-终端环境之zsh和z(*nix都适用) 详细介绍作者使用的一些命令，通俗简单。 a-beginners-guide-to-the-best-command-line-tools 我所使用的几个 plugin 如下 ： autojump git colored-man colorize copydir command-not-found history sublime brew 挑选你自己的 plugin…… 更多plugin查看 - awesome-zsh-plugins 使用方法很简单，在~/.zshrc文件的plugin下面添加上你想要的插件名称就ok1plugins=(git autojump colored-man colorize copydir history sublime command-not-found) 如果你想要定制化你自己的zsh，访问官网 http://ohmyz.sh/ ，上面有你需要的 plugin、theme，有意思的还有T恤…… 感觉默认主题不适合你，可以访问 https://github.com/unixorn/awesome-zsh-plugins#themes 查看更多的主题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[iterm2技巧]]></title>
      <url>%2F2017%2F02%2F15%2Fiterm2%E6%8A%80%E5%B7%A7%2F</url>
      <content type="text"><![CDATA[Mac 上最适合开发使用的终极终端 iTerm2，比自带的 term 终端有很多特性，比如竖屏操作、历史剪贴板、选中即复制、搜索高亮自动复制搜索内容、像 secureRT 样子的复制会话，记住密码登录，tux 集成等等。至于 Shell，一定要选则 zsh ，Mac 下已经自带，安装 oh my zsh 不用复杂的配置即可使用一些常用的提升效率的快捷键和插件整理如下: 快捷键官方的介绍特点: ⌘ + 数字在各 tab 标签直接来回切换 选择即复制 + 鼠标中键粘贴，这个很实用 ⌘ + f 所查找的内容会被自动复制 ⌘ + d 横着分屏 / ⌘ + shift + d 竖着分屏 ⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏 ctrl + u 清空当前行，无论光标在什么位置 输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令 ⌘ + shift + h 会列出剪切板历史 可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现 一些常用的快捷键如下: 新建标签：command + t 关闭标签：command + w 切换标签：command + 数字 command + 左右方向键 切换全屏：command + enter 查找：command + f 垂直分屏：command + d 水平分屏：command + shift + d 切换屏幕：command + option + 方向键 command + [ 或 command + ] 查看历史命令：command + 查看剪贴板历史：command + shift + h 清除当前行：ctrl + u 到行首：ctrl + a 到行尾：ctrl + e 前进后退：ctrl + f/b (相当于左右方向键) 上一条命令：ctrl + p 搜索命令历史：ctrl + r 删除当前光标的字符：ctrl + d 删除光标之前的字符：ctrl + h 删除光标之前的单词：ctrl + w 删除到文本末尾：ctrl + k 交换光标处文本：ctrl + t 清屏1：command + r 清屏2：ctrl + l 推荐插件一些插件能显著提高效率，自己使用的一些在下面，方法很简单，在~/.zshrc文件的plugin 下面添加上你想要的插件名称就ok，oh-my-sh 自带了很多插件，可以通过 ls ~/.oh-my-zsh/plugins 来查看。1plugins=(git-extras git mvn svn osx brew brew-cask npm colored-man colorize copydir history sublime command-not-found zsh-syntax-highlighting Z) git：当你处于一个 git 受控的目录下时，Shell 会明确显示 「git」和 branch，如上图所示，另外对 git 很多命令进行了简化，例如 gco=’git checkout’、gd=’git diff’、gst=’git status’、g=’git’等等，熟练使用可以大大减少 git 的命令长度，命令内容可以参考~/.oh-my-zsh/plugins/git/git.plugin.zsh textmate：mr可以创建 ruby 的框架项目，tm finename 可以用 textmate 打开指定文件。 osx：tab 增强，quick-look filename 可以直接预览文件，man-preview grep 可以生成 grep手册 的pdf 版本等。 git-extras: Git extras 工具与 zsh 的继承，很方便，在 git 仓库目录下试试git summary即可看到整个仓库的汇总信息。 sublime : 此插件能够在终端下使用命令stt 在 SublimeText 中打开当前文件夹，使用 subl 或者 st 来编辑某个特定文件，比如 st 1.txt，当然前提你得安装了 SublimeText。 zsh-syntax-highlighting: 让你终端的每一条命令智能显示颜色，就像在 IDE 里面写代码一样，强烈推荐，安装及介绍参考 GitHub z : Z is awesome ，让你在不同的目录中快速跳转，比如我想访问 ~/work/code/project/testApp ，只要是之前访问过，直接输入z testApp 按 tab 键直接显示完整目录，按 enter 键直接进入当前目录，即使只输入了z testa 也能完成同样的工作，大大提升效率，此插件是自带的可以直接使用。 ag: 终端里面快速搜索当前目录下所有文件中所匹配的关键字的命令，类似与 awk，但是速度极快，速度极快，速度极快，使用brew install ，其实就是组件 the_silver_searcher ，详细参考地址 GitHub 。 tree : mac 下的 tree 命令，方便排查问题，直接 brew install tree即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac键盘快捷键]]></title>
      <url>%2F2017%2F02%2F15%2FMac%E9%94%AE%E7%9B%98%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
      <content type="text"><![CDATA[要使用键盘快捷键，请按住一个或多个修饰键，同时按快捷键的最后一个键。例如，要使用快捷键 Command-C（拷贝），请按住 Command 键并按 C 键，然后同时松开这两个键。Mac 菜单和键盘通常使用某些按键的符号，其中包括以下修饰键： Command ⌘ Shift ⇧ Option ⌥ Control ⌃ Caps Lock ⇪ Fn 如果你使用的是 Windows PC 专用键盘，请用 Alt 键代替 Option 键，用 Windows 标志键代替 Command 键。有些 Mac 键盘在顶行中设有特殊按键，快捷键中也会用到它们；这些按键上有音量图标、显示屏亮度图标和其他功能图标。按下图标键可执行相应功能，将其与 Fn 键组合可用作 F1、F2、F3 或其他标准功能键。 常用快捷键1234567891011121314151617181920212223快捷键 | 描述Command-X | 剪切：删除所选项并将其拷贝到剪贴板。Command-C | 将所选项拷贝到剪贴板。这同样适用于 Finder 中的文件。Command-V | 将剪贴板的内容粘贴到当前文稿或 app 中。这同样适用于 Finder 中的文件。Command-Z | 撤销前一个命令。随后你可以按 Command-Shift-Z 来重做，从而反向执行撤销命令。在某些 app 中，你可以撤销和重做多个命令。Command-A | 全选各项。Command-F | 查找：打开“查找”窗口，或在文稿中查找项目。Command-G | 再次查找：查找之前所找到项目出现的下一个位置。要查找出现的上一个位置，请按 Command-Shift-G。Command-H | 隐藏最前面的 app 的窗口。要查看最前面的 app 但隐藏所有其他 app，请按 Command-Option-H。Command-M | 将最前面的窗口最小化至 Dock。要最小化最前面的 app 的所有窗口，请按 Command-Option-M。Command-N | 新建：打开一个新文稿或窗口。Command-O | 打开所选项，或打开一个对话框以选择要打开的文件。Command-P | 打印当前文稿。Command-S | 存储当前文稿。Command-W | 关闭最前面的窗口。要关闭该 app 的所有窗口，请按 Command-Option-W。Command-Q | 退出 app。空格键 | 快速查看：使用快速查看预览所选项。Command-Tab | 切换 app：在打开的 app 中切换到下一个最近使用的 app。Shift-Command-3 | 屏幕快照：拍摄整个屏幕的屏幕快照。了解更多屏幕快照快捷键。Command-逗号 (,) | 偏好设置：打开最前面的 app 的偏好设置。Option-Command-Esc | 强制退出：选择要强制退出的 app。或者，按住 Command-Shift-Option-Esc 3 秒钟来仅强制最前面的 app 退出。Command–空格键 | Spotlight：显示或隐藏 Spotlight 搜索栏。要从 Finder 窗口执行 Spotlight 搜索，请按 Command–Option–空格键。如果你使用多个输入源以便用不同的语言键入内容，这些快捷键会更改输入源而非显示 Spotlight。Shift-Command-波浪号 (~) | 切换窗口：切换到最前端应用中下一个最近使用的窗口。 文档快捷键123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 快捷键 | 描述Command-B | 以粗体显示所选文本，或者打开或关闭粗体显示功能。 Command-I | 以斜体显示所选文本，或者打开或关闭斜体显示功能。Command-U | 对所选文本加下划线，或者打开或关闭加下划线功能。Command-T | 显示或隐藏“字体”窗口.Command-D | 从“打开”对话框或“存储”对话框中选择“桌面”文件夹。Control-Command-D | 显示或隐藏所选字词的定义。Shift-Command-冒号 (:) | 显示“拼写和语法”窗口。Command-分号 (;) | 查找文稿中拼写错误的字词。Option-Delete | 删除插入点左边的字词。Control-H | 删除插入点左边的字符。也可以使用 Delete 键。Control-D | 删除插入点右边的字符。也可以使用 Fn-Delete。Fn-Delete | 在没有向前删除 键的键盘上向前删除。也可以使用 Control-D。Control-K | 删除插入点与行或段落末尾处之间的文本。Command-Delete | 在包含“删除”或“不存储”按钮的对话框中选择“删除”或“不存储”。Fn–上箭头 | 向上翻页：向上滚动一页。 Fn–下箭头 | 向下翻页：向下滚动一页。Fn–左箭头 | 开头：滚动到文稿开头。Fn–右箭头 | 结尾：滚动到文稿末尾。Command–上箭头 | 将插入点移至文稿开头。Command–下箭头 | 将插入点移至文稿末尾。Command–左箭头 | 将插入点移至当前行的行首。Command–右箭头 | 将插入点移至当前行的行尾。Option–左箭头 | 将插入点移至上一字词的词首。Option–右箭头 | 将插入点移至下一字词的词尾。Shift–Command–上箭头 | 选中插入点与文稿开头之间的文本。Shift–Command–下箭头 | 选中插入点与文稿末尾之间的文本。Shift–Command–左箭头 | 选中插入点与当前行行首之间的文本。Shift–Command–右箭头 | 选中插入点与当前行行尾之间的文本。Shift–上箭头 | 将文本选择范围扩展到上一行相同水平位置的最近字符处。Shift–下箭头 | 将文本选择范围扩展到下一行相同水平位置的最近字符处。Shift–左箭头 | 将文本选择范围向左扩展一个字符。Shift–右箭头 | 将文本选择范围向右扩展一个字符。Option–Shift–上箭头 | 将文本选择范围扩展到当前段落的段首，再按一次则扩展到下一段落的段首。Option–Shift–下箭头 | 将文本选择范围扩展到当前段落的段尾，再按一次则扩展到下一段落的段尾。Option–Shift–左箭头 | 将文本选择范围扩展到当前字词的词首，再按一次则扩展到后一字词的词首。Option–Shift–右箭头 | 将文本选择范围扩展到当前字词的词尾，再按一次则扩展到后一字词的词尾。Control-A | 移至行或段落的开头。Control-E | 移至行或段落的末尾。Control-F | 向前移动一个字符。Control-B | 向后移动一个字符。Control-L | 将光标或所选内容置于可见区域中央。Control-P | 上移一行。Control-N | 下移一行。Control-O | 在插入点后插入一行。Control-T | 将插入点后面的字符与插入点前面的字符交换。Command–左花括号 (&#123;) | 左对齐。Command–右花括号 (&#125;) | 右对齐。Shift–Command–竖线 | 居中对齐。Option-Command-F | 前往搜索栏。 Option-Command-T | 显示或隐藏应用中的工具栏。Option-Command-C | 拷贝样式：将所选项的格式设置拷贝到剪贴板。Option-Command-V | 粘贴样式：将拷贝的样式应用到所选项。Option-Shift-Command-V | 粘贴并匹配样式：将周围内容的样式应用到粘贴在该内容中的项目。Option-Command-I | 显示或隐藏检查器窗口。Shift-Command-P | 页面设置：显示用于选择文稿设置的窗口。Shift-Command-S | 显示“存储为”对话框或复制当前文稿。Shift–Command–减号 (-) | 缩小所选项。Shift–Command–加号 (+) | 放大所选项。Command–等号 (=) 可执行相同的功能。Shift–Command–问号 (?) | 打开“帮助”菜单。 截图操作123456截图快捷键 | 含义command+shift+3 | 全屏截图，保存截图到桌面文件command+shift+4 | 鼠标选定区域截图，保存截图到桌面文件command+shift+control+3 | 全屏截图，保存到剪贴板command+shift+control+4 | 鼠标选定区域截图，保存到剪贴板command+shift(+control)+4 | 然后按下空格键，鼠标变成小相机，选择某一窗口后点击鼠标左键对单个窗口截图。不必担心其它窗口的遮挡。 Finder 快捷键12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364快捷键 | 描述Command-D | 复制所选文件。Command-E | 推出所选磁盘或宗卷。Command-F | 在 Finder 窗口中开始 Spotlight 搜索。Command-I | 显示所选文件的“显示简介”窗口。Shift-Command-C | 打开“电脑”窗口。Shift-Command-D | 打开“桌面”文件夹。Shift-Command-F | 打开“我的所有文件”窗口。Shift-Command-G | 打开“前往文件夹”窗口。Shift-Command-H | 打开当前 macOS 用户帐户的个人文件夹。Shift-Command-I | 打开 iCloud Drive。Shift-Command-K | 打开“网络”窗口。Option-Command-L| 打开“下载”文件夹。Shift-Command-O | 打开“文稿”文件夹。Shift-Command-R | 打开“AirDrop”窗口。Shift-Command-T | 将所选的 Finder 项目添加到 Dock（OS X Mountain Lion 或较早版本）Control-Shift-Command-T| 将所选的 Finder 项目添加到 Dock（OS X Mavericks 或更高版本）Shift-Command-U | 打开“实用工具”文件夹。Option-Command-D | 显示或隐藏 Dock。即使您未打开 Finder，此快捷键通常也有效。Control-Command-T | 将所选项添加到边栏（OS X Mavericks 或更高版本）。Option-Command-P | 隐藏或显示 Finder 窗口中的路径栏。Option-Command-S | 隐藏或显示 Finder 窗口中的边栏。Command–斜线 (/) | 隐藏或显示 Finder 窗口中的状态栏。Command-J | 调出“显示”选项。Command-K | 打开“连接服务器”窗口。Command-L | 为所选项制作替身。Command-N | 打开一个新的 Finder 窗口。Shift-Command-N | 新建文件夹。Option-Command-N | 新建智能文件夹。Command-R | 显示所选替身的原始文件。Command-T | 在当前 Finder 窗口中打开单个标签时显示或隐藏标签栏。Shift-Command-T | 显示或隐藏 Finder 标签。Option-Command-T | 在当前 Finder 窗口中打开单个标签时显示或隐藏工具栏。Option-Command-V | 移动：将剪贴板中的文件从其原始位置移动到当前位置。Option-Command-Y | 显示所选文件的快速查看幻灯片显示。Command-Y| 使用“快速查看”预览所选文件。Command-1| 以图标方式显示 Finder 窗口中的项目。Command-2| 以列表方式显示 Finder 窗口中的项目。Command-3| 以分栏方式显示 Finder 窗口中的项目。 Command-4| 以 Cover Flow 方式显示 Finder 窗口中的项目。Command–左中括号 ([) | 前往上一文件夹。Command–右中括号 (]) | 前往下一文件夹。Command–上箭头 | 打开包含当前文件夹的文件夹。Command–Control–上箭头 | 在新窗口中打开包含当前文件夹的文件夹。Command–下箭头 | 打开所选项。Command–Mission Control | 显示桌面。即使您未打开 Finder，此快捷键也有效。Command–调高亮度 | 开启或关闭目标显示器模式。Command–调低亮度 | 当 Mac 连接到多个显示器时打开或关闭显示器镜像功能。右箭头 | 打开所选文件夹。此快捷键仅在列表视图中有效。左箭头 | 关闭所选文件夹。此快捷键仅在列表视图中有效。Option-连按 | 在单独窗口中打开文件夹，并关闭当前窗口。Command-连按 | 在单独标签或窗口中打开文件夹。Command-Delete | 将所选项移到废纸篓。Shift-Command-Delete| 清倒废纸篓。Option-Shift-Command-Delete| 清倒废纸篓（不显示确认对话框）。Command-Y | 使用“快速查看”预览文件。Option–调高亮度 | 打开“显示器”偏好设置。此快捷键可与任一亮度键搭配使用。Option–Mission Control | 打开“Mission Control”偏好设置。Option–调高音量 | 打开“声音”偏好设置。此快捷键可与任一音量键搭配使用。拖移时按 Command 键| 将拖移的项目移到其他宗卷或位置。拖移项目时指针会随之变化。拖移时按 Option 键 | 拷贝拖移的项目。拖移项目时指针会随之变化。拖移时按下 Option-Command | 为拖移的项目制作替身。拖移项目时指针会随之变化。Option-点按伸缩三角形| 打开所选文件夹内的所有文件夹。此快捷键仅在列表视图中有效。Command-点按窗口标题 | 查看包含当前文件夹的文件夹。 睡眠、注销和关机快捷键12345678快捷键 | 描述电源按钮 | 轻点可打开 Mac 或将 Mac 从睡眠状态唤醒。 当 Mac 处于唤醒状态时，按住此按钮 1.5 秒钟会显示一个对话框，询问您是要重新启动、睡眠还是关机。如果您不想等待 1.5 秒钟，请按下 Control–电源按钮或 Control–介质推出键 。按住此按钮 5 秒钟会强制 Mac 关机。Control–Command–电源按钮 | 强制 Mac 重新启动。Control–Shift–（电源按钮或介质推出键 ） | 将显示器置于睡眠状态。Control–Command–介质推出键 | 退出所有 app，然后重新启动 Mac。如果任何打开的文稿有未存储的更改，系统将询问您是否要存储这些更改。Control–Option–Command–（电源按钮或介质推出键 ） | 退出所有 app，然后关闭 Mac。如果任何打开的文稿有未存储的更改，系统将询问您是否要存储这些更改。Shift-Command-Q | 注销您的 macOS 用户帐户。系统将提示您确认。Option-Shift-Command-Q | 立即注销您的 macOS 用户帐户，且系统不提示您确认。 Mac 的启动组合键在启动期间按住某些键可以使用一些 Mac 功能。 请在 Mac 开机并听到启动声后立即按住这些键。请一直按住，直至所述行为出现。以下组合适用于基于 Intel 的 Mac 电脑。12345678910111213141516在启动期间按住 | 描述Shift ⇧ | 以安全模式启动。Option ⌥ | 启动进入启动管理器。C | 从可引导的 CD、DVD 或 USB 闪存驱动器（如 OS X 安装介质）启动。D | 启动进入 Apple Hardware Test 或 Apple Diagnostics，具体取决于您正在使用的 Mac。Option-D | 通过互联网启动进入 Apple Hardware Test 或 Apple Diagnostics。N | 从兼容的 NetBoot 服务器启动。Option-N | 使用默认的启动映像从 NetBoot 服务器启动。T | 以目标磁盘模式启动。X | 从 OS X 启动宗卷启动，否则 Mac 将从非 OS X 启动宗卷启动。Command (⌘)-R | 从 OS X 恢复功能启动。Command-Option-R | 通过互联网从 OS X 恢复功能启动。Command-Option-P-R | 重置 NVRAM。当再次听到启动声后，请松开这些键。Command-S | 以单用户模式启动。Command-V | 以详细模式启动。推出键 (⏏)、F12、鼠标键或触控板按钮 | 推出可移动介质，如光盘。 整理了那么多快捷键，一时半时根本记不住，怎么办？除了有意识的经常使用、练习外，还有一款神奇软件 CheatSheet 在任何应用程序下面长按Command ⌘ 键，即可以查看这款软件的快捷键操作。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(8)--分布式文档存储]]></title>
      <url>%2F2017%2F02%2F14%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-8-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E6%A1%A3%E5%AD%98%E5%82%A8-wait-for-active-shards%E6%96%B0%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[学完ES分布式集群的工作原理以及一些基本的将数据放入索引然后检索它们的所有方法，我们可以继续学习在分布式系统中，每个分片的文档是被如何索引和查询的。 路由首先，我们需要明白，文档和分片之间是如何匹配的，这就是路由。当你索引一个文档，它被存储在单独一个主分片上。Elasticsearch是如何知道文档属于哪个分片的呢？当你创建一个新文档，它是如何知道是应该存储在分片1还是分片2上的呢？ 进程不能是随机的，因为我们将来要检索文档。事实上，它根据一个简单的算法决定：1shard = hash(routing) % number_of_primary_shards routing值是一个任意字符串，它默认是_id但也可以自定义。这个routing字符串通过哈希函数生成一个数字，然后除以主切片的数量得到一个余数(remainder)，余数的范围永远是0到number_of_primary_shards - 1，这个数字就是特定文档所在的分片。 这也解释了为什么主分片的数量只能在创建索引时定义且不能修改：如果主分片的数量在未来改变了，所有先前的路由值就失效了，文档也就永远找不到了。 所有的文档API（get、index、delete、bulk、update、mget）都接收一个routing参数，它用来自定义文档到分片的映射。自定义路由值可以确保所有相关文档——例如属于同一个人的文档——被保存在同一分片上。 例如，可以这样设置参数：123456POST twitter/tweet?routing=kimchy&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125; 主分片和复制分片如何交互在文档确认存储到哪个主分片以后，接下来就是主分片将数据复制到复制分片的任务，为了阐述意图，我们假设有三个节点的集群。它包含一个叫做blogs的索引并拥有两个主分片。每个主分片有两个复制分片。相同的分片不会放在同一个节点上，所以我们的集群是这样的：我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点，所以也可以将请求转发到需要的节点。下面的例子中，我们将发送所有请求给Node 1，这个节点我们将会称之为请求节点(requesting node)。一般情况下，当我们发送请求，最好的做法是循环通过所有节点请求，这样可以平衡负载。 新建、索引和删除文档新建、索引和删除请求都是写(write)操作，它们必须在主分片上成功完成才能复制到相关的复制分片上。 下面是在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤： 客户端给Node 1发送新建、索引或删除请求。 节点使用文档的_id确定文档属于分片0。它转发请求到Node 3，分片0位于这个节点上。 Node 3在主分片上执行请求，如果成功，它转发请求到相应的位于Node 1和Node 2的复制节点上。当所有的复制节点报告成功，Node 3报告成功到请求的节点，请求的节点再报告给客户端。客户端接收到成功响应的时候，文档的修改已经被应用于主分片和所有的复制分片。你的修改生效了。 有很多可选的请求参数允许你更改这一过程。你可能想牺牲一些安全来提高性能。这些选项很少使用因为Elasticsearch已经足够快。 注意：下面的参数只对ElasticSearch 5.0以下的版本有效，在ElasticSearch 5.0之后貌似使用wait_for_active_shards代替了consistency。所以之前的参数了解即可，实际可以参考：Create Index—Wait For Active Shards。 replication（注意在ElasticSearch 5.0开始被废弃）复制默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。 如果你设置replication为async，请求在主分片上被执行后就会返回给客户端。它依旧会转发请求给复制节点，但你将不知道复制节点成功与否。 上面的这个选项不建议使用。默认的sync复制允许Elasticsearch强制反馈传输。async复制可能会因为在不等待其它分片就绪的情况下发送过多的请求而使Elasticsearch过载。 consistency（注意在ElasticSearch 5.0开始被废弃）默认主分片在尝试写入时需要规定数量(quorum)或过半的分片（可以是主节点或复制节点）可用。这是防止数据被写入到错的网络分区。规定的数量计算公式如下：1int( (primary + number_of_replicas) / 2 ) + 1 consistency允许的值为one（只有一个主分片），all（所有主分片和复制分片）或者默认的quorum或过半分片。 注意number_of_replicas是在索引中的的设置，用来定义复制分片的数量，而不是现在活动的复制节点的数量。如果你定义了索引有3个复制节点，那规定数量是：1int( (primary + 3 replicas) / 2 ) + 1 = 3 但如果你只有2个节点，那你的活动分片不够规定数量，也就不能索引或删除任何文档。 注意： 新索引默认有1个复制分片，这意味着为了满足quorum的要求需要两个活动的分片。当然，这个默认设置将阻止我们在单一节点集群中进行操作。为了避开这个问题，规定数量只有在number_of_replicas大于一时才生效。 一个疑惑，是不是primary值一直都只会是1？？？ wait_for_active_shards（新参数）在ElasticSearch 5.0中可以用wait_for_active_shards参数表示：等待活动的分片，具体的值和consistency类似，下面用wait_for_active_shards演示一个实际使用的例子。 开始我们先设置一个新的索引：1234567PUT /active&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 3 &#125;&#125; 我们默认先只打开两个节点，等下我们设置wait_for_active_shards值为3，按照上面讲解的我们如果只有两个节点，那么活动的分片最多也就2个，所以是不够的，会等待新的活动节点的到来。（这里我们只能通过少一个节点的方法演示缺少活动分片，因为我们不方便演示出让某个分片处于不活动的状态。）因为我们只有两个节点，所以活动的分片最多也只有两个。下面我们执行文档存储操作，并且添加参数wait_for_active_shards=3：可以发现，确实开始处于等待状态，没有马上返回结果，下面我们参数开启第三个节点，让索引拥有第三个活动分片：可以看到一旦我们的节点开启，文档的存储马上就会返回成功。 教程中关于这部分网上很多朋友不太理解，我们可以通过查看官方文档和实践去证明自己的想法，希望上面的分析大家可以理解一些，还有不对的地方大家可以一起学习。 timeout当分片副本不足时会怎样？Elasticsearch会等待更多的分片出现。默认等待一分钟。如果需要，你可以设置timeout参数让它终止的更早：100表示100毫秒，30s表示30秒。 检索文档文档能够从主分片或任意一个复制分片被检索。下面我们罗列在主分片或复制分片上检索一个文档必要的顺序步骤： 客户端给Node 1发送get请求。 节点使用文档的_id确定文档属于分片0。分片0对应的复制分片在三个节点上都有。此时，它转发请求到Node 2。 Node 2返回文档(document)给Node 1然后返回给客户端。对于读请求，为了平衡负载，请求节点会为每个请求选择不同的分片——它会循环所有分片副本（包括主分片）。 可能的情况是，一个被索引的文档已经存在于主分片上却还没来得及同步到复制分片上。这时复制分片会报告文档未找到，主分片会成功返回文档。一旦索引请求成功返回给用户，文档则在主分片和复制分片都是可用的。 更新文档update API结合了之前提到的读和写的模式。下面我们罗列执行局部更新必要的顺序步骤： 客户端给Node 1发送更新请求。 它转发请求到主分片所在节点Node 3。 Node 3从主分片检索出文档，修改_source字段的JSON，然后在主分片上重建索引。如果有其他进程修改了文档，它以retry_on_conflict设置的次数重复步骤3，都未成功则放弃。 如果Node 3成功更新文档，它同时转发文档的新版本到Node 1和Node 2上的复制节点以重建索引。当所有复制节点报告成功，Node 3返回成功给请求节点，然后返回给客户端。 update API还接受routing、replication（弃）、consistency（弃）和timout参数。 基于文档的复制当主分片转发更改给复制分片时，并不是转发更新请求，而是转发整个文档的新版本。记住这些修改转发到复制节点是异步的，它们并不能保证到达的顺序与发送相同。如果Elasticsearch转发的仅仅是修改请求，修改的顺序可能是错误的，那得到的就是个损坏的文档。 多文档模式mget和bulk API与单独的文档类似。差别是请求节点知道每个文档所在的分片。它把多文档请求拆成每个分片的对文档请求，然后转发每个参与的节点。 一旦接收到每个节点的应答，然后整理这些响应组合为一个单独的响应，最后返回给客户端。下面我们将罗列通过一个mget请求检索多个文档的顺序步骤： 客户端向Node 1发送mget请求。 Node 1为每个分片构建一个多条数据检索请求，然后转发到这些请求所需的主分片或复制分片上。当所有回复被接收，Node 1构建响应并返回给客户端。 routing 参数可以被docs中的每个文档设置。下面我们将罗列使用一个bulk执行多个create、index、delete和update请求的顺序步骤： 客户端向Node 1发送bulk请求。 Node 1为每个分片构建批量请求，然后转发到这些请求所需的主分片上。 主分片一个接一个的按序执行操作。当一个操作执行完，主分片转发新文档（或者删除部分）给对应的复制节点，然后执行下一个操作。一旦所有复制节点报告所有操作已成功完成，节点就报告success给请求节点，后者(请求节点)整理响应并返回给客户端。 bulk API还可以在最上层使用replication（弃）和consistency（弃）参数，routing参数则在每个请求的元数据中使用。 总结以上就是关于在分布式系统中，每个分片的文档是被如何索引和查询的。虽然版本的更新有一些参数会更新，但是整体的内部实现应该不会有太大的变化，分享一个学习方法，学习的时候把新旧的版本内容通过对比，不仅可以更好理解知识，而且可以加深印象。更何况旧的不会被很快淘汰，学了又何妨！ 出处：http://www.cnblogs.com/wxw16/p/6192549.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(7)——分布式集群学习2]]></title>
      <url>%2F2017%2F02%2F13%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-7-%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AD%A6%E4%B9%A02%2F</url>
      <content type="text"><![CDATA[前面主要学习了ElasticSearch分布式集群的存储过程中集群、节点和分片的知识（ES-5.0.2-学习(6)——分布式集群学习1），下面主要分享应对故障的一些实践。 应对故障前面说了很多关于复制分片可以应对节点失效，很好保证集群的安全性，下面我们可以尝试杀掉第一个节点的进程，我们的集群变化成如下（所有的操作都是ElasticSearch自动处理）：我们杀掉的节点是一个主节点。一个集群必须要有一个主节点才能使其功能正常，所以集群做的第一件事就是各节点选举了一个新的主节点：Node 2。 主分片1和2在我们杀掉Node 1时已经丢失，我们的索引在丢失主分片时不能正常工作。如果此时我们检查集群健康，我们将看到状态red：不是所有主分片都可用！ 幸运的是丢失的两个主分片的完整拷贝存在于其他节点上，所以新主节点做的第一件事是把这些在Node 2和Node 3上的复制分片升级为主分片，这时集群健康回到yellow状态。这个提升是瞬间完成的，就好像按了一下开关。 为什么集群健康状态是yellow而不是green？我们有三个主分片，但是我们指定了每个主分片对应两个复制分片，当前却只有一个复制分片被分配，这就是集群状态无法达到green的原因，不过不用太担心这个：当我们杀掉Node 2，我们的程序依然可以在没有丢失数据的情况下继续运行，因为Node 3还有每个分片的拷贝。 如果我们重启Node 1，集群将能够重新分配丢失的复制分片，集群状况与上一节的图5：增加number_of_replicas到2 类似。如果Node 1依旧有旧分片的拷贝，它将会尝试再利用它们，它只会从主分片上复制在故障期间有数据变更的那一部分。 故障实践1上面是关于ElasticSearch在遇到故障时候的理论部分，下面我们开始实际操作。 查看目前集群状态我们回顾一下之前的blogs索引，在结束最后的状态：1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, (主分片个数) &quot;number_of_replicas&quot; : 2, (每个主分片的复制分片个数) &#125;&#125; 切断节点为了模拟这种情况，在我们自己的电脑上，直接用kill命令即可:12ps -ef | grep elasticsearch #获取到elasticsearch的进程id，例如5891kill 5891 查看集群的状态很正确，就是理论内容所描述中间会存在red的瞬间。等·等·等·可是等了半天，结果一直是red状态的结果，这是为什么呢？注意看提示无法连接到http://localhost:9200，突然意识到，我们关闭的节点正好是9200端口的Node 1节点。所以我们需要修改kibana.yml配置文件的elasticsearch.url项： 再次查看集群的状态终于，可以看到我们想要的结果，ElasticSearch集群正如上面所说的重新选Node 2作为新的主节点：我们还可以注意到集群的健康状况从绿色变成了黄色，这是因为我们设置每个主节点2个复制分片，而现在还有一个复制节点处于不可用状态。 故障实践2回顾之前的一个集群状态，blogs索引只设置一个复制分片的情况下：如果在这种情况下，我们把其中的任何一个节点关闭，会出现什么效果呢？我们分析看，至少我们关闭任何一个节点都能保所有的分片都还能存在。比如我们删除Node 2节点，正常情况下，Node 2中的分片0作为主分片被删除后，主节点会分配Node 1节点下复制分片0重新作为主分片0，而Node 2中的分片1本身是复制分片，直接删除即可，但是ElasticSearch集群，除此之外还会不会有其他操作。那就是，从新在两个节点中把所有的复制分片都置为可用。下面我们看结果：首先我们看到的和我们前面分析的一样，主节点会分配Node 1节点下复制分片0重新作为主分片0，但是也可以看到现在集群的健康状况是黄色，因为存在复制节点处于不可用状态。我们继续等。。。：终于我们可以看到，ElasticSearch集群确实会把所有的复制节点又都置为可用状态，因为节点存在它不拥有的分片，就可以创建这个节点，最大程度的保证高可用性。 实践注意点在测试过程中，ElasticSearch集群确实可以帮助我们重新分配分片的状态，但是需要注意的是，每次一个节点关闭的时候，集群需要一定的时间去管理，如果这时候我们很快的将两个节点关闭，ElasticSearch集群将无法挽救回没有主分片，也没有复制分片的那些数据，所以测试的时候需要知道这一点。 不过这也反映我们在学习分享1中描述的，如果我们的复制节点足够多的话，我们可以保证高可用的能力就却强大，因为允许节点故障的次数更多，而且我们的节点故障以后，运维又可以将节点重启，继续斗争！！！ 总结现在我们对分片如何使Elasticsearch可以水平扩展并保证数据安全有了一个清晰的认识。真正感受到Elasticsearch天生就是分布式的，确实很强大！ 出处：http://www.cnblogs.com/wxw16/p/6188560.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(6)——分布式集群学习1]]></title>
      <url>%2F2017%2F02%2F13%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-6-%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AD%A6%E4%B9%A01%2F</url>
      <content type="text"><![CDATA[在使用中我们把文档存入ElasticSearch，但是如果能够了解ElasticSearch内部是如何存储的，将会对我们学习ElasticSearch有很清晰的认识。本文中的所使用的ElasticSearch集群环境，可以通过查看 ES-5.0.2-学习(3)——单台服务器部署多个节点 搭建学习。ElasticSearch用于构建高可用和可扩展的系统。扩展的方式可以是购买更好的服务器(纵向扩展(vertical scale or scaling up))或者购买更多的服务器（横向扩展(horizontal scale or scaling out)）。 Elasticsearch虽然能从更强大的硬件中获得更好的性能，但是纵向扩展有它的局限性。真正的扩展应该是横向的，它通过增加节点来均摊负载和增加可靠性。 对于大多数数据库而言，横向扩展意味着你的程序将做非常大的改动才能利用这些新添加的设备。对比来说，Elasticsearch天生就是分布式的：它知道如何管理节点来提供高扩展和高可用。这意味着你的程序不需要关心这些。 下面的例子主要围绕着集群(cluster)、节点(node)和分片(shard)讲解，相信学习以后，对于学习Elasticsearch会有很大收获。 空集群如果我们启动一个单独的节点，它还没有数据和索引，这个集群看起来如下图：只有一个空节点的集群。一个节点(node)就是一个Elasticsearch实例，而一个集群(cluster)由一个或多个节点组成，它们具有相同cluster.name，它们协同工作，分享数据和负载。当加入新的节点或者删除一个节点时，集群就会感知到并平衡数据。 集群中一个节点会被选举为主节点(master)，它将临时管理集群级别的一些变更，例如新建或删除索引、增加或移除节点等。主节点不参与文档级别的变更或搜索，这意味着在流量增长的时候，该主节点不会成为集群的瓶颈。任何节点都可以成为主节点。我们例子中的集群只有一个节点，所以它会充当主节点的角色。 作为用户，我们能够与集群中的任何节点通信，包括主节点。每一个节点都知道文档存在于哪个节点上，它们可以转发请求到相应的节点上。我们访问的节点负责收集各节点返回的数据，最后一起返回给客户端。这一切都由Elasticsearch处理。 集群健康在Elasticsearch集群中可以监控统计很多信息，但是只有一个是最重要的：集群健康(cluster health)。集群健康有三种状态：green、yellow或red，健康状况在后面会有很多体现。1GET /_cluster/health 在一个没有索引的空集群中运行如上查询，将返回这些信息： { “cluster_name”: “elasticsearch”, “status”: “green”, “timed_out”: false, “number_of_nodes”: 1, “number_of_data_nodes”: 1, “active_primary_shards”: 0, “active_shards”: 0, “relocating_shards”: 0, “initializing_shards”: 0, “unassigned_shards”: 0} status： 是我们最感兴趣的字段。status字段提供一个综合的指标来表示集群的的服务状况。三种颜色各自的含义： green：所有主要分片和复制分片都可用。 yellow：所有主要分片可用，但不是所有复制分片都可用。 red：不是所有的主要分片都可用。 添加索引为了将数据添加到Elasticsearch，我们需要索引(index)——一个存储关联数据的地方。实际上，索引只是一个用来指向一个或多个分片(shards)的“逻辑命名空间(logical namespace)”. 一个分片(shard)是一个最小级别“工作单元(worker unit)”,它只是保存了索引中所有数据的一部分。并且先初步知道分片就是一个Lucene实例，它本身就是一个完整的搜索引擎。我们的文档存储在分片中，并且在分片中被索引，但是我们的应用程序不会直接与它们通信，取而代之的是，直接与索引通信。 分片是Elasticsearch在集群中分发数据的关键。把分片想象成数据的容器。文档存储在分片中，然后分片分配到你集群中的节点上。当你的集群扩容或缩小，Elasticsearch将会自动在你的节点间迁移分片，以使集群保持平衡。 分片可以是主分片(primary shard)或者是复制分片(replica shard)。你索引中的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整。 复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的分片取回文档。 让我们在集群中唯一一个空节点上创建一个叫做blogs的索引。默认情况下，一个索引被分配5个主分片，但是为了演示的目的，我们只分配3个主分片和一个复制分片（每个主分片都有一个复制分片）：1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 我们的集群现在看起来就像上图——三个主分片都被分配到Node 1。如果我们现在检查集群健康(cluster-health)，我们将见到以下信息：123456789101112&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;yellow&quot;, &lt;1&gt; &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 3, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 3 &lt;2&gt;&#125; 集群的状态现在是 yellow 我们的三个复制分片还没有被分配到节点上 下面我们可以看下，在Kibana监控工具中查看具体情况，如下图：集群的健康状态yellow表示所有的主分片(primary shards)启动并且正常运行了——集群已经可以正常处理任何请求——但是复制分片(replica shards)还没有全部可用。事实上所有的三个复制分片现在都是unassigned状态——它们还未被分配给节点。在同一个节点上保存相同的数据副本是没有必要的，如果这个节点故障了，那所有的数据副本也会丢失。 现在我们的集群已经功能完备，但是依旧存在因硬件故障而导致数据丢失的风险。 添加故障转移在单一节点上运行意味着有单点故障的风险——没有数据备份。幸运的是，要防止单点故障，我们唯一需要做的就是启动另一个节点。 具体启动方式可以查看 ES-5.0.2-学习(3)——单台服务器部署多个节点 。 如果我们启动了第二个节点，这个集群看起来就像下图。双节点集群——所有的主分片和复制分片都已分配:第二个节点已经加入集群，三个复制分片(replica shards)也已经被分配了——分别对应三个主分片，这意味着在丢失任意一个节点的情况下依旧可以保证数据的完整性。 文档的索引将首先被存储在主分片中，然后并发复制到对应的复制节点上。这可以确保我们的数据在主节点和复制节点上都可以被检索。 cluster-health 现在的状态是 green，这意味着所有的6个分片（三个主分片和三个复制分片）都已可用：123456789101112&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, &quot;number_of_data_nodes&quot;: 2, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 6, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0&#125; 集群的状态是green，我们的集群不仅是功能完备的，而且是高可用的。同样我们可以看下实际的操作结果： 横向扩展随着应用需求的增长，我们该如何扩展？如果我们启动第三个节点，我们的集群会重新组织自己，包含3个节点的集群——分片已经被重新分配以平衡负载：Node 3包含了分别来自Node 1和Node 2的一个分片，这样每个节点就有两个分片，和之前相比少了一个，这意味着每个节点上的分片将获得更多的硬件资源（CPU、RAM、I/O）。 分片本身就是一个完整的搜索引擎，它可以使用单一节点的所有资源。我们拥有6个分片（3个主分片和三个复制分片），最多可以扩展到6个节点，每个节点上有一个分片，每个分片可以100%使用这个节点的资源。 同样我们可以看下实际的操作结果： 继续扩展如果我们要扩展到6个以上的节点，要怎么做？ 主分片的数量在创建索引时已经确定。实际上，这个数量定义了能存储到索引里数据的最大数量（实际的数量取决于你的数据、硬件和应用场景）。然而，主分片或者复制分片都可以处理读请求——搜索或文档检索，所以数据的冗余越多，我们能处理的搜索吞吐量就越大。 复制分片的数量可以在运行中的集群中动态地变更，这允许我们可以根据需求扩大或者缩小规模。让我们把复制分片的数量从原来的1增加到2：1234PUT /blogs/_settings&#123; &quot;number_of_replicas&quot; : 2&#125; 从图中可以看出，blogs索引现在有9个分片：3个主分片和6个复制分片。这意味着我们能够扩展到9个节点，再次变成每个节点一个分片。这样使我们的搜索性能相比原始的三节点集群增加“三倍”。 实际操作也是同样的效果：注意：可以看到上面的“三倍”我们用加了引号，因为在同样数量的节点上增加更多的复制分片并不一定提高性能，因为这样做的话平均每个分片的所占有的硬件资源就减少了（大部分请求都聚集到了分片少的节点，导致一个节点吞吐量太大，反而降低性能），你需要增加硬件来提高吞吐量。所以说添加复制分片和添加节点，在保证成本的情况下，需要有一个平衡点。 不过这些额外的复制节点还是有另外一个好处，使我们有更多的冗余：通过以上对节点的设置，我们能够承受两个节点故障而不丢失数据。 总结对于ES分布式集群如果对节点、分片的处理基本学习完毕，可以感受到ES分布式集群的自动化，对于用户来说几乎完全透明化。但是，一个分布式集群主要看它的高性能、高并发和高可用。上面的内容虽然体现了一些，但是还包括对故障的处理能力，这部分将在 ES-5.0.2-学习(7)——分布式集群学习2 继续和大家分享。 出处：http://www.cnblogs.com/wxw16/p/6188044.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(5)--第一个ES例子]]></title>
      <url>%2F2017%2F02%2F10%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-3-%E7%AC%AC%E4%B8%80%E4%B8%AAES%E4%BE%8B%E5%AD%90%2F</url>
      <content type="text"><![CDATA[想要知道ElasticSearch是如何使用的，最快的方式就是通过一个简单的例子，第一个例子将会包括基本概念如索引、搜索、和聚合等，需求是关于公司管理员工的一些业务。# 员工文档索引业务首先需要存储员工数据。这将采取一个员工文档的形式：单个文档表示单个员工。在Elasticsearch中存储数据的行为称为索引，但是在索引文档之前，我们需要决定在哪里存储它。在Elasticsearch中，文档属于某个类型，这些类型位于索引中。可以绘制一些（粗略）与传统关系数据库的对比： Relational DB ⇒ Databases ⇒ Tables ⇒ Rows ⇒ Columns Elasticsearch ⇒ Indices ⇒ Types ⇒ Documents ⇒ FieldsElasticsearch集群可以包含多个索引（数据库），这些索引又包含多个类型（表）。这些类型包含多个文档（行），每个文档都有多个字段（列）。你可能已经注意到，在Elasticsearch的上下文中，索引被重载了几个含义。如下： 索引（名词）：正如前面所解释的那样，索引就像传统的关系数据库中的数据库一样。它是存储相关文档的地方。index的复数形式是indices或indexes。 索引（动词）：索引一个文档是将一个文档存储在索引（名词）中，以便它可以检索和查询。它很像插入关键词SQL。此外，如果文档已经存在，新的文档将取代旧的。 倒排索引：关系数据库中增加一个索引，如B-树索引，对特定列为了提高数据检索的速度。Elasticsearch和Lucene提供相同目的的索引称为倒排索引。默认情况下，文档中的每个字段索引（有一个倒排索引）这样的搜索。一个没有倒排索引字段不可搜索因此我们的员工目录，我们需要处理如下事情： 索引的每个文档，包含每个员工的所有细节。 每个文档都属于**employee类型。 类型都包含在*megacop索引中。 该索引将驻留我们Elasticsearch集群内。下面通过命令去索引第一个员工：12345678PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125;注意 */megacorp/employee/1 包含的信息:&gt; megacorp：索引的名称 emplogee：类型的名称 1：*员工的id成功执行返回的是一个JSON文本，包含所有关于该员工的信息。 注意：1. 如果执行过程中失败了，可能存在的原因是elasticsearch默认配置中不允许自动创建索引，所以我们可以先简单在elasticsearch.yml配置文件添加action.auto_create_index：true，允许自动创建索引。2. 没有必要首先执行任何管理任务，如创建一个索引或指定每个字段所包含的数据类型。我们可以直接索引一个文档。Elasticsearch附带默认的一切，因此所有必要的管理任务都会使用默认值在后台处理。在目录中添加更多的员工：1234567891011121314151617PUT /megacorp/employee/2&#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125;PUT /megacorp/employee/3&#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ]&#125;# 检索文档现在我们有一些数据存储在Elasticsearch中，我们可以开始处理这个应用程序的业务需求。第一个要求是检索单个员工数据的能力。这在Elasticsearch中很容易。我们只需执行HTTP GET请求并指定文档的地址——索引，类型和ID。使用这三个信息，我们可以返回原始的JSON文档，并且响应包含有关文档的一些元数据，以及Douglas Fir的原始JSON文档作为_source字段：可以查看：ES-5.0.2-学习(4)–简单搜索以同样的方式，我们将HTTP动词从PUT更改为GET以便检索文档，我们可以使用DELETE动词删除文档，并使用HEAD动词检查文档是否存在。要用更新的版本替换现有文档，我们只需再次PUT。GET很简单，可以得到要求的文件。尝试一些更高级的东西，我们可以搜索所有员工，请求：1GET /megacorp/employee/_search你可以看到我们仍在使用索引megacorp和类型employee，但是我们现在使用_search端点，而不是指定文档ID。响应包括我们在hits数组中的所有三个文档。默认情况下，搜索将返回前10个结果。响应不仅告诉我们哪些文档匹配，而且还包括整个文档本身，以便向用户显示搜索结果的所有信息。接下来，让我们尝试搜索在其姓氏中有“Smith”的员工。为此，我们将使用一个轻松的搜索方法，它很容易从命令行使用。此方法通常称为查询字符串搜索，因为我们将搜索作为URL查询字符串参数传递：# DSL查询查询字符串搜索对于从命令行进行搜索非常方便，但它有其局限性。Elasticsearch提供了一种丰富，灵活的查询语言，称为查询DSL，它允许我们构建更复杂，更健壮的查询。使用JSON请求正文指定域特定语言（DSL）。我们可以代表所有以前的搜索，像这样：12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125;这将返回与上一个查询相同的结果。可以看到一些事情已经改变。例如，我们不再使用查询字符串参数，而是使用请求正文。此请求体是使用JSON构建的，并使用匹配查询。让我们让搜索更复杂一点。我们仍然希望找到所有名字为Smith的员工，但我们只想要30岁以上的员工。我们的查询将稍微改变一点，以容纳一个过滤器，这使我们能够有效地执行结构化搜索：1234567891011121314151617GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;smith&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125; &#125; &#125; &#125; &#125;&#125;我们添加了一个过滤器，执行范围搜索，并重复使用与以前相同的匹配查询。现在我们的结果显示只有一个员工刚好是32并被命名为smith：*注意：**关于过滤器在Elasticsearch2.0开始有很大的更新，所以有些过滤操作可能会报错。例如：filtered query已经被废弃。# 全文搜索（Full-Text Search）到目前为止的搜索很简单：单个名字，按年龄过滤。让我们尝试更高级的全文搜索，传统数据库真正难以胜任的任务。我们将寻找所有喜欢攀岩的员工：12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125;您可以看到我们使用与之前相同的匹配查询来搜索关于“攀岩”字段。我们得到两个匹配的文档：默认情况下，Elasticsearch按匹配结果的相关性分值（即每个文档与查询匹配程度）对匹配结果进行排序。第一个和最高分的结果是显而易见的：John·Smith关于字段清楚地说“攀岩”。但为什么Jane·Smith也返回了？她的文档被返回的原因是因为在她的字段中提到了“rock”这个词。因为只有“岩石”被提及，而不是“攀登”，她的分数低于John的。这是Elasticsearch如何在全文字段中进行搜索并返回最相关的结果的一个很好的例子。这种相关性的概念对于Elasticsearch很重要，并且是一个完全与传统关系数据库无关的概念，其中记录匹配或不匹配。# 精确字段搜索在字段中查找单个字词是很好的，但有时你想要匹配字词或短语的确切序列。例如，我们可以执行一个查询，该查询将仅匹配包含“rock”和“climbing”的员工记录，并在短语“rock climbing”中显示彼此相邻的单词。为此，我们使用改为match_phrase查询：12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125;仅返回John Smith的文档# 高亮搜索结果许多应用程序喜欢从每个搜索结果突出显示文本片段，以便用户可以看到文档与查询匹配的原因。在Elasticsearch中检索突出显示的片段很容易。让我们重新运行我们以前的查询，但添加一个新的highlight参数：12345678910111213GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;about&quot; : &#123;&#125; &#125; &#125;&#125;当我们运行此查询时，将返回与之前相同的返回，但现在我们在响应中得到一个新的部分，称为突出显示。这包含来自about字段的文字片段，其中包含在 HTML标记中包含的匹配单词：# 分析最后，我们来到我们的最后一个业务需求：允许管理员在员工目录上运行分析。Elasticsearch具有称为聚合的功能，允许您对数据生成复杂的分析。它类似于GROUP BY中的SQL，但功能更强大。例如，让我们找到我们的员工最喜欢的兴趣：12345678GET /megacorp/employee/_search&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125;如果Elasticsearch 5版本以前，将会返回：12345678910111213141516171819202122&#123; ... &quot;hits&quot;: &#123; ... &#125;, &quot;aggregations&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125;我们可以看到，两个员工对音乐感兴趣，一个在林业，一个在体育。这些聚合不是预先计算的，它们是从与当前查询匹配的文档即时生成的。然而如果我们使用的是Elasticsearch 5版本以上的话，将会出现如下异常：我们可以查看 Elasticsearch 5.0文档——Fielddata is disabled on text fields by default*大概的意思是：Fielddata可以消耗大量的堆空间，特别是在加载高基数文本字段时。一旦fielddata已经加载到堆中，它在该段的生存期内保持。此外，加载fielddata是一个昂贵的过程，可以导致用户体验延迟命中。所以fielddata默认禁用。如果尝试对文本字段上的脚本进行排序，聚合或访问值，就会看到这个异常，具体使用可以参考手册。# 总结这个小例子是一个很好的演示了什么是Elasticsearch。它只是很肤浅的介绍了简单的使用，许多功能被省略，以保持简短。但是这也突出了开始构建高级搜索功能是多么容易。 出处：http://www.cnblogs.com/wxw16/p/6185378.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(4)--简单搜索]]></title>
      <url>%2F2017%2F02%2F10%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-3-%E7%AE%80%E5%8D%95%E6%90%9C%E7%B4%A2%2F</url>
      <content type="text"><![CDATA[空搜索1GET /_search 注：以下操作均是在kibana中，如图所示hits： total 总数 hits 前10条数据 hits 数组中的每个结果都包含_index、_type和文档的_id字段，被加入到_source字段中这意味着在搜索结果中我们将可以直接使用全部文档。 每个节点都有一个_score字段，这是相关性得分(relevance score)，它衡量了文档与查询的匹配程度。默认的，返回的结果中关联性最大的文档排在首位；这意味着，它是按照_score降序排列的。没有指定任何查询，所以所有文档的相关性是一样的，因此所有结果的_score都是取得一个中间值1。 took：整个搜索请求花费的毫秒数。_shards：节点告诉我们参与查询的分片数（total字段），有多少是成功的（successful），有多少的是失败的（failed）。time_out：告诉我们查询超时与否。一般的，搜索请求不会超时。如果响应速度比完整的结果更重要，你可以定义timeout参数为10或者10ms（10毫秒），或者1s（1秒）1GET /_search?timeout=10ms Elasticsearch将返回在请求超时前收集到的结果。注意： timeout不会停止执行查询，它仅仅告诉你目前顺利返回结果的节点然后关闭连接。在后台，其他分片可能依旧执行查询，尽管结果已经被发送。使用超时是因为对于你的业务需求来说非常重要，而不是因为你想中断执行长时间运行的查询。 多索引和多类别在所有索引的所有类型中搜索：/_search在索引gb的所有类型中搜索：/gb/_search在索引gb和us的所有类型中搜索：/gb,us/_search在以g或u开头的索引的所有类型中搜索：/g*,u*/_search在索引gb的类型user中搜索：/gb/user/_search在索引gb和us的类型为user和tweet中搜索：/gb,us/user,tweet/_search在所有索引的user和tweet中搜索：/_all/user,tweet/_search当你搜索包含单一索引时，Elasticsearch转发搜索请求到这个索引的主分片或每个分片的复制分片上，然后聚集每个分片的结果。搜索包含多个索引也是同样的方式——只不过会有更多的分片被关联。 分页如果你想每页显示5个结果，页码从1到3，那请求如下：123GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 应该当心分页太深或者一次请求太多的结果。结果在返回前会被排序。但是记住一个搜索请求常常涉及多个分片。每个分片生成自己排好序的结果，它们接着需要集中起来排序以确保整体排序正确。现在假设我们请求第1000页——结果10001到10010。工作方式都相同，不同的是每个分片都必须产生顶端的10010个结果。然后请求节点排序这50050个结果并丢弃50040个！ 简易搜索search API有两种表单：一种是“简易版”的查询字符串(query string)将所有参数通过查询字符串定义，另一种版本使用JSON完整的表示请求体(request body)，这种富搜索语言叫做结构化查询语句（DSL）。查询字符串搜索对于在命令行下运行特定情况下查询特别有用。例如这个语句查询所有类型为tweet并在tweet字段中包含elasticsearch字符的文档：1GET /_all/tweet/_search?q=tweet:elasticsearch 下一个语句查找name字段中包含”john”和tweet字段包含”mary”的结果。实际的查询只需要：1+name:john +tweet:mary 但是url编码需要将查询字符串参数变得更加神秘：1GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary “+”前缀表示语句匹配条件必须被满足。类似的”-“前缀表示条件必须不被满足。所有条件如果没有+或-表示是可选的——匹配越多，相关的文档就越多。 _all字段返回包含”mary”字符的所有文档的简单搜索：1GET /_search?q=mary 当你索引一个文档，Elasticsearch把所有字符串字段值连接起来放在一个大字符串中，它被索引为一个特殊的字段_all。例如，当索引这个文档：123456&#123; &quot;tweet&quot;: &quot;However did I manage before Elasticsearch?&quot;, &quot;date&quot;: &quot;2014-09-14&quot;, &quot;name&quot;: &quot;Mary Jones&quot;, &quot;user_id&quot;: 1&#125; 这好比我们增加了一个叫做_all的额外字段值：1&quot;However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1&quot; 若没有指定字段，查询字符串搜索（即q=xxx）使用_all字段搜索。 更复杂的语句下一个搜索的语句：_all field name字段包含”mary”或”john” date晚于2014-09-10 _all字段包含”aggregations”或”geo” 1+name:(mary john) +date:&gt;2014-09-10 +(aggregations geo) 编码后的查询字符串变得不太容易阅读1?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo) 就像你上面看到的例子，简单查询字符串搜索惊人的强大。允许我们简洁明快的表示复杂的查询。这对于命令行下一次性查询或者开发模式下非常有用。然而，你可以看到简洁带来了隐晦和调试困难。而且它很脆弱——查询字符串中一个细小的语法错误，像-、:、/或”错位就会导致返回错误而不是结果。最后，查询字符串搜索允许任意用户在索引中任何一个字段上运行潜在的慢查询语句，可能暴露私有信息甚至使你的集群瘫痪。取而代之的，生产环境我们一般依赖全功能的请求体搜索API，它能完成前面所有的事情，甚至更多。 出处：http://www.cnblogs.com/wxw16/p/6171016.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(3)——单台服务器部署多个节点]]></title>
      <url>%2F2017%2F02%2F10%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-3-%E2%80%94%E2%80%94%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E5%A4%9A%E4%B8%AA%E8%8A%82%E7%82%B9%2F</url>
      <content type="text"><![CDATA[一般情况下单台服务器只会部署一个ElasticSearch node，但是在学习过程中，很多情况下会需要实现ElasticSearch的分布式效果，所以需要启动多个节点，但是学习开发环境（不想开多个虚拟机实现多个服务器的效果），所以就想着在一台服务器上部署多个结点（下文以2个结点作为例子），两个节点分别称为实例一、二。 1、首先将elasticsearch-5.0.2文件夹再复制一份1cp -R elasticsearch-5.0.2 elasticsearch-5.0.2-node-2 2、主要工作就是修改elasticsearch.yml配置文件。实例二：config目录下的elasticsearch.yml内容将node.name: node-1 修改为 node-2 3、分别开启两个节点1./bin/elasticsearch 4、查询是否成功浏览器访问 http://localhost:9200/_cluster/health?pretty若出现类似如下则表示成功123456789101112131415161718&#123; &quot;cluster_name&quot;: &quot;es-lzr&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, &quot;number_of_data_nodes&quot;: 2, &quot;active_primary_shards&quot;: 17, &quot;active_shards&quot;: 34, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot;: 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 100.0&#125; 也可以通过Kibana查看节点效果 http://localhost:5601 账号 elastic 密码 changeme 踩过的坑1.如果修改了配置文件的 http.port、transport.tcp.port项，一定要将各个节点的值设置不同，否则会出现占用的情况。正常如果不修改，默认会分配值。2.示例二开启时，实例一报警告（实际操作中可以忽略）： [2016-12-11T18:06:43,678][WARN ][o.e.d.z.ElectMasterService] [node-1] value for setting “discovery.zen.minimum_master_nodes” is too low. This can result in data loss! Please set it to at least a quorum of master-eligible nodes (current value: [-1], total number of master-eligible nodes used for publishing in this round: [2])是因为默认情况下 discovery.zen.minimum_master_nodes=1 一台服务器只能有一个主节点，所以在实例二的配置文件中可以添加 node.master: false 。 3.示例二不能开启，报如下错误： [2016-12-11T16:53:02,711][INFO ][o.e.d.z.ZenDiscovery ] [node-2] failed to send join request to master [{node-1}{vP19PMOyT2ilJKRAqgn78w}{jDULCExERXGHp4VXpbyuJA}{127.0.0.1}{127.0.0.1:9300}], reason [RemoteTransportException[[node-1][127.0.0.1:9300][internal:discovery/zen/join]]; nested: IllegalArgumentException[can’t add node {node-2}{vP19PMOyT2ilJKRAqgn78w}{qhDDVzwZQ0GXZXhIMmpGKA}{127.0.0.1}{127.0.0.1:9301}, found existing node {node-1}{vP19PMOyT2ilJKRAqgn78w}{jDULCExERXGHp4VXpbyuJA}{127.0.0.1}{127.0.0.1:9300} with the same id but is a different node instance]; ][2016-12-11T16:53:02,911][INFO ][o.e.x.m.e.Exporters ] [node-2] skipping exporter [default_local] as it isn’t ready yet[2016-12-11T16:53:02,912][ERROR][o.e.x.m.AgentService ] [node-2] exception when exporting documentsorg.elasticsearch.xpack.monitoring.exporter.ExportException: exporters are either not ready or faulty at org.elasticsearch.xpack.monitoring.exporter.Exporters.export(Exporters.java:188) ~[x-pack-5.0.2.jar:5.0.2] at org.elasticsearch.xpack.monitoring.AgentService$ExportingWorker.run(AgentService.java:208) [x-pack-5.0.2.jar:5.0.2] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111] 是因为复制的elasticsearch文件夹下包含了data文件中示例一的节点数据，需要把示例二data文件下的文件清空。 出处：http://www.cnblogs.com/wxw16/p/6160186.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为了保持逼格，不要停止写作]]></title>
      <url>%2F2017%2F02%2F10%2F%E4%B8%BA%E4%BA%86%E4%BF%9D%E6%8C%81%E9%80%BC%E6%A0%BC%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%81%9C%E6%AD%A2%E5%86%99%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[“逼格”一词，网络上常见的解释是“装逼的格调”，听起来有一些嘲讽的味道。但我认为，“逼格”应该是一种大隐隐于市的淡定，一种出淤泥而不染的清高，一种观望世事变迁而不为之动容的气质，一种身处外部世界的混乱却依然能听从内部世界的召唤的能力。你觉得我这番解释是装逼也无所谓，因为我的理想一直以来就是做一个有逼格的人。 追求逼格的过程不是一蹴而就的，更不是一劳永逸的，而更像是逆水行舟，一刻都万万不可以松懈。忍不住让我想起柏邦妮说过的一句话：“哪有什么胜利可言，挺住就是一切。” 所以，追求逼格可以被看做是一种坚持不懈的抗争，一种保护内部世界的完整饱足而不被外部世界侵蚀的抗争。这种抗争，本身就足以令人肃然起敬。如此看来，逼格恰恰存在于追求逼格的过程之中。 保护内部世界之完整饱足的方法有很多，而我独爱写作。我一直信奉，虽然不是每个人都可以把写作当成自己的职业，但所有人都一定可以把写作当成自己的一种状态。 在我眼中，世间的写作不过两种，对内的和对外的。今天来聊聊我理想中的写作吧。 对内的写作，是内部世界在巩固自己的版图，是往下扎根。典型的形式包括写日记——你是作者，你也是唯一的读者。如果，你在落笔的当下决定，读者除了你自己还可以包括其他人，即使再少数的其他人都好，那都不能被归为“对内的写作”了。当然，如果你已经完成了写作的这个部分，只不过在之后的阅读过程中觉得，公布出来被他人看见也无妨，那又另当别论。这里讨论的，只是写作这个动作进行之时的内心向度。 对内的写作应该是繁杂凌乱的、忠诚于自己所念所想的，应该完完全全原汁原味地呈现一个自由的内心世界，所以没有必要进行自我审查。想起以前学摄影的时候，教授叮嘱我们拿起相机就一定要无所顾忌、疯狂地拍，把照片导出来之后才开始进行筛选，千万不要把创作和编辑的过程混在一块儿，千万不要在拍的时候就缩手缩脚，或者直接在相机上删掉所谓的不满意的照片。同理，对内的写作应该是零编辑、只关注在创作之上的，这是至关重要的资本原始积累，所以格局一定要大。 对内的写作是一个认识自己的过程，所以要诚实地面对内心的所有情感，尤其是那些不足为外人道也的阴暗的部分。比如说，跟你走得很近的人里有一个你很鄙视的人，但是你又不得不装作和ta相处起来很愉快，那你不妨可以在对内的写作过程中梳理这种情感。挖掘自己的邪恶天赋，并承认它们，是帮助自己成长为一个丰富的人的必经之路。 对内的写作也是一个用来讨好自己的过程。在我以前还很热衷于写日记的时候，我的日记本是一本红色软皮抄。我喜欢它封面的颜色，更喜欢它米色的空白内页。内页纸张的触感很好，而且不会晕墨，真的是难能可贵的品质。我喜欢用墨蓝或者深绿的墨水写日记，尤其喜欢用英雄616钢笔。偏好的场所包括图书馆的静音区和自己的房间。我尤其喜欢物理意义上的下笔，虽然有的时候也在电脑上写日记，但总感觉动笔的信息量更大，毕竟笔迹可以直白地展示当时的心理状态，而且我总喜欢看自己在不同时期的笔迹的变化。这些癖好让我觉得对内的写作是一种仪式，是一种褒奖，是一种可以独享的时光。那本红色软皮抄用完之后我仿佛丧失了写日记的兴趣，因为再也没有找到媲美它的日记本了，不得不说，很可惜。 对外的写作，是内部世界朝外部的扩张。我个人主张它必须是功利的。所谓功利，是指功效和实用性，说直白一点就是，既然你选择了对外写作，就应该对你传播的这些信息负责，让它们尽可能高效地被读者所接收。所以对外的写作应该是方便理解的、易于消化的。要呈现这种特性，在对外写作的过程中就务必有相当一部分的精力花在编辑之上。如果说对内的写作单纯需要用心，对外的写作则更需要用脑。 写到这里，不得不提写作与朗读之间的密不可分的关系。 我在上写作课的时候，教授给我们讲过这样一个有趣的故事。说是有一个写作界的大神，看人很准，曾经试过一连好几年的普利策获奖者都是他曾经相中过表扬过的新人。于是有人去问大神，他眼光为什么这么厉害。大神说，我去编辑部里面看啊，看到那些写稿子的时候嘴里在念念有词的人，我就知道他们有戏了。 教授给我们讲这个故事，意图是要让我们爱上朗读自己的作品，不但如此，更要用朗读的方式去体会其他优秀的写作。朗读，实际上是一场非常严肃的检阅。从编辑方面的效用来看，在朗读的过程中，很容易就能挑出错别字、语病、累赘用词等等小毛病，也可以发现段落间逻辑衔接有没有跳脱这样的结构上的问题。 曾经有人评价我的行文是轻松的、不费力的，大概也是因为我总是以口语化的要求来规范自己的写作吧。也有人曾经给我提意见说，你能不能不要念，专心写，因为你的文笔远胜于你的口头表达，要念出来的话等于限制了你文采的发挥。我当然承认，为了口语化理念的贯彻，不得不让渡出一部分高深的辞藻和逻辑。但口语化的理念可以让写作者更多地采用主谓宾的语序，减少繁杂的修饰语，抛弃冗长的句式结构，实际上是一种更生活、更亲近的表达方式。这种亲近可以让写作者的内部世界建立与外部世界友好的交流，而信息的顺畅流通恰好又可以维持一个更饱满鲜活的内部世界。出于这种理念，我个人也非常不喜欢故作傲慢和刻意艰涩的写作，总让人感到一种封闭和敌意。 我本科的最后一年花了整整两个学期在上写作课，但结果却是对写作像一个新朋友一样。 我并不恐慌，相反的，我对这个变化感到无比的兴奋。想到之前写作课教授给我们援引过的一句话，那是美国著名诗人、作家卡尔·桑德堡在72岁的时候说的： I’m still studying verbs and the mystery of how they connect nouns. I am more suspicious of adjectives than at any other time in all my born days. “我仍旧在学习动词，以及它们和名词连接的奥秘。现在的我比任何时候都更怀疑形容词。” 就像是一个幼儿园小朋友在堆积木的时候的有趣的发现。真是一种有美感的状态。 我希望，我也能活出那样的一天。 转自新浪微博 @恢复吃素的F小姐。 说一个我认为最有逼格的故事吧：某日跟某死党微信对话如下:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(2)——Kibana+X-Pack介绍使用]]></title>
      <url>%2F2017%2F02%2F09%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-1-%E2%80%94%E2%80%94Kibana-X-Pack%E4%BB%8B%E7%BB%8D%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[Kibana是一个为 ElasticSearch 提供的数据分析的 Web 接口。可使用它对日志进行高效的搜索、可视化、分析等各种操作。Kibana目前最新的版本5.0.2，回顾一下Kibana 3和Kibana 4的界面。下面的图展示的是Kibana 3的界面，所有的仪表盘直接放置主页。下面的图展示的是Kibana 4的界面，和Kibana 3最大的区别是将原来的主体分成三个部分，分别是发现页、可视化、仪表盘。下面是目前Kibana 5最新版本的界面。相比较Kibana 4除了界面的风格变化，最主要是功能栏上添加了Timeline、Management和Dev Tools选项。 Discover You can interactively explore your data from the Discover page. You have access to every document in every index that matches the selected index pattern. You can submit search queries, filter the search results, and view document data. You can also see the number of documents that match the search query and get field value statistics. If a time field is configured for the selected index pattern, the distribution of documents over time is displayed in a histogram at the top of the page. 从发现页可以交互地探索ES的数据。可以访问与所选索引模式相匹配的每一个索引中的每一个文档。您可以提交搜索查询、筛选搜索结果和查看文档数据。还可以看到匹配搜索查询和获取字段值统计的文档的数量。如果一个时间字段被配置为所选择的索引模式，则文档的分布随着时间的推移显示在页面顶部的直方图中。 Visualize Visualize enables you to create visualizations of the data in your Elasticsearch indices. You can then build dashboards that display related visualizations.Kibana visualizations are based on Elasticsearch queries. By using a series of Elasticsearch aggregations to extract and process your data, you can create charts that show you the trends, spikes, and dips you need to know about.You can create visualizations from a search saved from Discover or start with a new search query. 可视化能使你创造你的Elasticsearch指标数据的可视化。然后你可以建立仪表板显示相关的可视化。Kibana的可视化是基于Elasticsearch查询。通过一系列的Elasticsearch聚合提取和处理您的数据，您可以创建图表显示你需要知道的关于趋势，峰值和骤降。您可以从搜索保存的搜索中创建可视化或从一个新的搜索查询开始。 Dashboard A Kibana dashboard displays a collection of saved visualizations. You can arrange and resize the visualizations as needed and save dashboards so they be reloaded and shared. 一个仪表板显示Kibana保存的一系列可视化。你可以根据需要安排和调整可视化，并保存仪表盘，可以被加载和共享。 Monitoring从图中可以发现，默认Kibana是没有该选项的。其实，Monitoring是由X-Pack集成提供的。 The X-Pack monitoring components enable you to easily monitor Elasticsearch through Kibana. You can view cluster health and performance in real time as well as analyze past cluster, index, and node metrics. In addition, you can monitor the performance of Kibana itself.When you install X-Pack on your cluster, a monitoring agent runs on each node to collect and index metrics from Elasticsearch. With X-Pack installed in Kibana, you can then view the monitoring data through a set of specialized dashboards. 该X-pack监控组件使您可以通过Kibana轻松地监控ElasticSearch。您可以实时查看集群的健康和性能，以及分析过去的集群、索引和节点度量。此外，您可以监视Kibana本身性能。当你安装X-pack在群集上，监控代理运行在每个节点上收集和指数指标从Elasticsearch。安装在X-pack在Kibana上，您可以查看通过一套专门的仪表板监控数据。回顾安装过程：ES-5-0-2-学习-1-——安装Elasticsearch、Kibana和X-Pack/，可以发现，在安装X-pack的时候分别在ElasticSearch根目录和Kibana根目录下操作。 Graph The X-Pack graph capabilities enable you to discover how items in an Elasticsearch index are related. You can explore the connections between indexed terms and see which connections are the most meaningful. This can be useful in a variety of applications, from fraud detection to recommendation engines.For example, graph exploration could help you uncover website vulnerabilities that hackers are targeting so you can harden your website. Or, you might provide graph-based personalized recommendations to your e-commerce customers.X-Pack provides a simple, yet powerful graph exploration API, and an interactive graph visualization tool for Kibana. Both work with out of the box with existing Elasticsearch indices—you don’t need to store any additional data to use the X-Pack graph features. X-Pack图的能力使你发现一个Elasticsearch索引项是如何相关联的。你可以探索索引条款之间的连接，看看哪些连接是最有意义的。从欺诈检测到推荐引擎，对各种应用中这都是有用的，例如，图的探索可以帮助你发现网站上黑客的目标的漏洞，所以你可以硬化你的网站。或者，您可以为您的电子商务客户提供基于图表的个性化推荐。X-pack提供简单，但功能强大的图形开发API，和Kibana交互式图形可视化工具。使用X-pack图有工作与开销与现有Elasticsearch指标你不需要任何额外的数据存储的特征。 Timelion Timelion is a time series data visualizer that enables you to combine totally independent data sources within a single visualization. It’s driven by a simple expression language you use to retrieve time series data, perform calculations to tease out the answers to complex questions, and visualize the results. Timelion是一个时间序列数据的可视化，可以结合在一个单一的可视化完全独立的数据源。它是由一个简单的表达式语言驱动的，你用来检索时间序列数据，进行计算，找出复杂的问题的答案，并可视化的结果。这个功能由一系列的功能函数组成，同样的查询的结果，也可以通过Dashboard显示查看。 Management The Management application is where you perform your runtime configuration of Kibana, including both the initial setup and ongoing configuration of index patterns, advanced settings that tweak the behaviors of Kibana itself, and the various “objects” that you can save throughout Kibana such as searches, visualizations, and dashboards.This section is pluginable, so in addition to the out of the box capabitilies, packs such as X-Pack can add additional management capabilities to Kibana.管理中的应用是在你执行你的运行时配置kibana，包括初始设置和指标进行配置模式，高级设置，调整自己的行为和Kibana，各种“对象”，你可以查看保存在整个Kibana的内容如发现页，可视化和仪表板。这部分是pluginable，除此之外，X-pack可以给Kibana增加额外的管理能力。You can use X-Pack Security to control what Elasticsearch data users can access through Kibana.When you install X-Pack, Kibana users have to log in. They need to have the kibana_user role as well as access to the indices they will be working with in Kibana.If a user loads a Kibana dashboard that accesses data in an index that they are not authorized to view, they get an error that indicates the index does not exist. X-Pack Security does not currently provide a way to control which users can load which dashboards. 你可以使用X-pack安全控制哪些用户可以访问Elasticsearch数据通过Kibana。当你安装X-pack，Kibana用户登录。他们需要有kibana_user作用以及获得的指标，他们将在Kibana的工作。如果用户加载Kibana仪表板，访问数据的一个索引，他们未被授权查看，他们得到一个错误，表明指数不存在。X-pack安全目前并不提供一种方法来控制哪些用户可以负荷的仪表板。 Dev Tools原先的交互式控制台Sense，使用户方便的通过浏览器直接与Elasticsearch进行交互。从Kibana 5开始改名并直接内建在Kibana，就是Dev Tools选项。注意如果是Kibana 5以上，不能通过以下命令安装Sense。(踩过的坑)1./bin/kibana plugin --install elastic/sense 或者1./bin/kibana-plugin install elastic/sense instead 总结内容比较简单，主要是对Kibana工具的整体功能总结，方便接下来对ElasticSearch 5的学习，其中X-Pack主要是添加身份权限的验证，以及原先需要安装其他各种Marvel、Hand等各种功能插件添加到Kibana上使用才能使用的功能。学习链接：X-Pack：https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.htmlKibana：https://www.elastic.co/guide/en/kibana/current/introduction.html 出处：http://www.cnblogs.com/wxw16/p/6156335.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Markdown——入门指南]]></title>
      <url>%2F2017%2F02%2F09%2FMarkdown%E2%80%94%E2%80%94%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%2F</url>
      <content type="text"><![CDATA[导语：Markdown 是一种轻量级的「标记语言」，它的优点很多，目前也被越来越多的写作爱好者，撰稿者广泛使用。看到这里请不要被「标记」、「语言」所迷惑，Markdown 的语法十分简单。常用的标记符号也不超过十个，这种相对于更为复杂的 HTML 标记语言来说，Markdown 可谓是十分轻量的，学习成本也不需要太多，且一旦熟悉这种语法规则，会有一劳永逸的效果。 一、认识 Markdown在刚才的导语里提到，Markdown 是一种用来写作的轻量级「标记语言」，它用简洁的语法代替排版，而不像一般我们用的字处理软件 Word 或 Pages 有大量的排版、字体设置。它使我们专心于码字，用「标记」语法，来代替常见的排版格式。例如此文从内容到格式，甚至插图，键盘就可以通通搞定了。目前来看，支持 Markdown 语法的编辑器有很多，包括很多网站（例如简书）也支持了 Markdown 的文字录入。Markdown 从写作到完成，导出格式随心所欲，你可以导出 HTML 格式的文件用来网站发布，也可以十分方便的导出 PDF 格式，这种格式写出的简历更能得到 HR 的好感。甚至可以利用 CloudApp 这种云服务工具直接上传至网页用来分享你的文章，全球最大的轻博客平台 Tumblr，也支持 Mou 这类 Markdown 工具的直接上传。 Markdown 官方文档这里可以看到官方的 Markdown 语法规则文档，当然，后文我也会用自己的方式阐述这些语法的具体用法。 创始人 John Gruber 的 Markdown 语法说明 Markdown 中文版语法说明 使用 Markdown 的优点 专注你的文字内容而不是排版样式，安心写作。 轻松的导出 HTML、PDF 和本身的 .md 文件。 纯文本内容，兼容所有的文本编辑器与字处理软件。 随时修改你的文章版本，不必像字处理软件生成若干文件版本导致混乱。 可读、直观、学习成本低。 使用 Markdown 的误区 We believe that writing is about content, about what you want to say – not about fancy formatting. 我们坚信写作写的是内容，所思所想，而不是花样格式。 — Ulysses for Mac Markdown 旨在简洁、高效，也由于 Markdown 的易读易写，人们用不同的编程语言实现了多个版本的解析器和生成器，这就导致了目前不同的 Markdown 工具集成了不同的功能（基础功能大致相同），例如流程图与时序图，复杂表格与复杂公式的呈现，虽然功能的丰富并没有什么本质的缺点，但终归有些背离初衷，何况在编写的过程中很费神，不如使用专业的工具撰写来的更有效率，所以如果你需实现复杂功能，专业的图形界面工具会更加方便。当然，如果你对折腾这些不同客户端对 Markdown 的定制所带来高阶功能感到愉悦的话，那也是无可厚非的。 二、Markdown 语法的简要规则标题标题是每篇文章都需要也是最常用的格式，在 Markdown 中，如果一段文字被定义为标题，只要在这段文字前加 # 号即可。 #一级标题 ##二级标题 ###三级标题 以此类推，总共六级标题，建议在井号后加一个空格，这是最标准的 Markdown 语法。 列表熟悉 HTML 的同学肯定知道有序列表与无序列表的区别，在 Markdown 下，列表的显示只需要在文字前加上 - 或 * 即可变为无序列表，有序列表则直接在文字前加1. 2. 3. 符号要和文字之间加上一个字符的空格。 引用如果你需要引用一小段别处的句子，那么就要用引用的格式。只需要在文本前加入 &gt; 这种尖括号（大于号）即可 图片与链接注：使用markdown写文章，插入图片的格式为图片名称，这里要说的是链接地址怎么写。对于hexo，有两种方式：使用本地路径：在hexo/source目录下新建一个img文件夹，将图片放入该文件夹下，插入图片时链接即为/img/图片名称。使用微博图床，地址http://weibotuchuang.sinaapp.com/，将图片拖入区域中，会生成图片的URL，这就是链接地址。 粗体与斜体Markdown 的粗体和斜体也非常简单，用两个 包含一段文本就是粗体的语法，用一个 包含一段文本就是斜体的语法。例如：这里是粗体 这里是斜体 表格表格是我觉得 Markdown 比较累人的地方，例子如下：这种语法生成的表格如下： 代码框如果你是个程序猿，需要在文章里优雅的引用代码框，在 Markdown下实现也非常简单，只需要用两个 把中间的代码包裹起来。this is code`使用 tab 键即可缩进。 块代码两种方式1.代码每一行的前面都加4个空格或一个tab2.第一行和最后一行都是3个，中间的行是代码12this is code block 1this is code block 2 分割线分割线的语法只需要三个 * 号，例如： 三、相关推荐:图床工具用来上传图片获取 URL 地址 Droplr Cloudapp ezShare for Mac 围脖图床修复计划]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(1)——安装Elasticsearch、Kibana和X-Pack]]></title>
      <url>%2F2017%2F02%2F09%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-1-%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85Elasticsearch%E3%80%81Kibana%E5%92%8CX-Pack%2F</url>
      <content type="text"><![CDATA[安装准备：安装Elasticsearch唯一的要求是安装官方新版的Java，包括对应的Jdk。 安装Elasticsearch首先到官网下载最新版本的Elasticsearch压缩包。可以使用命令，注意将最新的可用的下载链接填入：123curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/ elasticsearch-5.0.2.zipunzip elasticsearch-5.0.2.zipcd elasticsearch-5.0.2 运行ElasticsearchElasticsearch已经准备就绪，执行以下命令可在前台启动：1./bin/elasticsearch 如果想在后台以守护进程模式运行，添加-d参数。打开另一个终端进行测试：1curl &apos;http://localhost:9200/?pretty&apos; 你能看到以下返回信息：12345678910111213&#123;&quot;name&quot;: &quot;vP19PMO&quot;,&quot;cluster_name&quot;: &quot;elasticsearch&quot;,&quot;cluster_uuid&quot;: &quot;IMKMfkMsSrKODIYg5gxgeQ&quot;,&quot;version&quot;: &#123; &quot;number&quot;: &quot;5.0.2&quot;, &quot;build_hash&quot;: &quot;f6b4951&quot;, &quot;build_date&quot;: &quot;2016-11-24T10:07:18.101Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;6.2.1&quot;&#125;,&quot;tagline&quot;: &quot;You Know, for Search&quot;&#125; 这说明你的ELasticsearch集群已经启动并且正常运行。 安装KiabnaKibana是一个为 ElasticSearch 提供的数据分析的 Web 接口。可使用它对日志进行高效的搜索、可视化、分析等各种操作。首先到官网下载最新版本的Kiabna压缩包。可以使用如下命令，注意将最新的可用的下载链接填入：1234wget https://artifacts.elastic.co/downloads/kibana/kibana-5.1.1-linux-x86_64.tar.gzsha1sum kibana-5.1.1-linux-x86_64.tar.gztar -xzf kibana-5.1.1-linux-x86_64.tar.gzcd kibana/ 注意：https://www.elastic.co/downloads/kibana 可以在该地址获取下载链接，一定要选择对于系统和版本。 按照文档的要求，一般情况下kibana的版本必须和Elasticsearch安装的版本一致。 安装X-PackX-Pack是一个Elastic Stack的扩展，将安全，警报，监视，报告和图形功能包含在一个易于安装的软件包中。在Elasticsearch 5.0.0之前，您必须安装单独的Shield，Watcher和Marvel插件才能获得在X-Pack中所有的功能。 下载前提Elasticsearch 5.0.2Kibana 5.0.2 Elasticsearch下载X-Pack在Es的根目录（每个节点），运行 bin/elasticsearch-plugin 进行安装。1bin/elasticsearch-plugin install x-pack/Users/root/Downloads/markdown/763363-20161209181135163-1294099591.png 安装过程中跳出选项现在y即可。如果你在Elasticsearch已禁用自动索引的创建，在elasticsearch.yml配置action.auto_create_index允许X-pack创造以下指标：1action.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history* 运行Elasticsearch。1bin/elasticsearch Kibana下载X-Pack在Kibana根目录运行 bin/kibana-plugin 进行安装。1bin/kibana-plugin install x-pack/Users/root/Downloads/markdown/1.png 安装过程会比较久，耐心等待。运行Kibana。1bin/kibana 验证X-Pack在浏览器上输入：http://localhost:5601/ ，可以打开Kibana，此时需要输入用户名和密码登录，默认分别是 elastic 和 changeme。 安装参考：每个操作系统安装Elasticsearch的文件选择不同，参考：https://www.elastic.co/downloads/elasticsearch，选择对应的文件下载。 安装Kiabna需要根据操作系统做选择，参考：https://www.elastic.co/guide/en/kibana/current/install.html，选择对应的文件下载。*安装X-Pack需要根据Elasticsearch安装不同的方式提供不同的安装方法，参考：https://www.elastic.co/guide/en/x-pack/5.0/installing-xpack.html#installing-xpack。 名词解释在刚接触Elasticsearch的时候，会有很多名词不能理解，或者不知道其中的关系。其中很多是为不同版本的Elasticsearch而存在的。 MarvelMarvel插件：在簇中从每个节点汇集数据。这个插件必须每个节点都得安装。Marvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了Sense。 Sense交互式控制台，使用户方便的通过浏览器直接与Elasticsearch进行交互。 Hand在学习Elasticsearch的过程中，必不可少需要通过一些工具查看es的运行状态以及数据。如果都是通过rest请求，未免太过麻烦，而且也不够人性化。此时，Head插件可以实现基本信息的查看，rest请求的模拟，数据的检索等等。 X-packx-pack是elasticsearch的一个扩展包，将安全，警告，监视，图形和报告功能捆绑在一个易于安装的软件包中，也是官方推荐的。 Kibanakibana是一个与elasticsearch一起工作的开源的分析和可视化的平台。使用kibana可以查询、查看并与存储在elasticsearch索引的数据进行交互操作。使用kibana能执行高级的数据分析，并能以图表、表格和地图的形式查看数据。kibana使得理解大容量的数据变得非常容易。它非常简单，基于浏览器的接口使我们能够快速的创建和分享显示elasticsearch查询结果实时变化的仪表盘。在Elasticsearch 5版本之前，一般都是通过安装Kibana，而后将Marvel、Hand等各种功能插件添加到Kibana上使用。在Elasticsearch 5版本之后，一般情况下只需要安装一个官方推荐的X-pack扩展包即可。 出处：http://www.cnblogs.com/wxw16/p/6150681.html]]></content>
    </entry>

    
  
  
</search>
