<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[welcome]]></title>
      <url>%2F2117%2F02%2F28%2Fwelcome%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode刷题笔记总纲]]></title>
      <url>%2F2019%2F03%2F29%2FLeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BA%B2%2F</url>
      <content type="text"><![CDATA[本文主要记录博主LeetCode上刷题的历程，会总结解题思路及难点要点。数组类001、Two Sum 两数之和026、Remove Duplicates from Sorted Array 从排序数组中删除重复项027、Remove Element 移除元素]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[027.移除元素]]></title>
      <url>%2F2018%2F03%2F29%2FLeetCode-027-%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0%2F</url>
      <content type="text"><![CDATA[题目:给定一个数组和一个值，在这个数组中原地移除指定值和返回移除后新的数组长度。不要为其他数组分配额外空间，你必须使用 O(1) 的额外内存原地修改这个输入数组。元素的顺序可以改变。超过返回的新的数组长度以外的数据无论是什么都没关系。示例:12给定 nums = [3,2,2,3]，val = 3，你的函数应该返回 长度 = 2，数组的前两个元素是 2。 英文版：Remove ElementGiven an array and a value, remove all instances of that value in place and return the new length.The order of elements can be changed. It doesn’t matter what you leave beyond the new length.Example:123Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 思路：这道题让我们移除一个数组中和给定值相同的数字，并返回新的数组的长度。是一道比较容易的题，我们只需要一个变量用来计数，然后遍历原数组，如果当前的值和给定值不同，我们就把当前值覆盖计数变量的位置，并将计数变量加1。代码如下：123456789class Solution &#123; public int removeElement(int[] nums, int val) &#123; int newLength = 0; for (int i = 0; i &lt; nums.length; ++i) &#123; if (nums[i] != val) nums[newLength++] = nums[i]; &#125; return newLength; &#125;&#125; 返回LeetCode刷题笔记总纲]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[026.从排序数组中删除重复项]]></title>
      <url>%2F2018%2F03%2F29%2FLeetCode-Remove-Duplicates-from-Sorted-Array-%E4%BB%8E%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%88%A0%E9%99%A4%E9%87%8D%E5%A4%8D%E9%A1%B9%2F</url>
      <content type="text"><![CDATA[题目:给定一个有序数组，你需要原地删除其中的重复内容，使每个元素只出现一次,并返回新的长度。不要另外定义一个数组，您必须通过用 O(1) 额外内存原地修改输入的数组来做到这一点。示例:123给定数组: nums = [1,1,2],你的函数应该返回新长度 2, 并且原数组nums的前两个元素必须是1和2不需要理会新的数组长度后面的元素 英文版：Remove Duplicates from Sorted ArrayGiven a sorted array, remove the duplicates in place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this in place with constant memory.Example:12Given input array A = [1,1,2],Your function should return length = 2, and A is now [1,2]. 思路：这道题要我们从有序数组中去除重复项，和之前那道 Remove Duplicates from Sorted List(移除有序链表中的重复项)的题很类似，但是要简单一些，因为毕竟数组的值可以通过下标直接访问，而链表不行。那么这道题的解题思路是，我们使用快慢指针来记录遍历的坐标，最开始时两个指针都指向第一个数字，如果两个指针指的数字相同，则快指针向前走一步，如果不同，则两个指针都向前走一步，这样当快指针走完整个数组后，慢指针当前的坐标加1就是数组中不同数字的个数.代码如下： 解法一：123456789101112class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; if (nums.length == 0) return 0; int pre = 0, cur = 0, n = nums.length; while (cur &lt; n) &#123; if (nums[pre] == nums[cur]) ++cur; else nums[++pre] = nums[cur++]; &#125; return pre + 1; &#125;&#125; 我们也可以用for循环来写，这里的j就是上面解法中的pre，i就是cur，所以本质上都是一样的。 解法二：12345678910class Solution &#123; public int removeDuplicates(int[] nums) &#123; if (nums.length == 0) return 0; int j = 0, n = nums.length; for (int i = 0; i &lt; n; ++i) &#123; if (nums[i] != nums[j]) nums[++j] = nums[i]; &#125; return j + 1; &#125;&#125; 类似题目：Remove Duplicates from Sorted ListRemove Duplicates from Sorted List IIRemove Duplicates from Sorted Array IIRemove Element 返回LeetCode刷题笔记总纲]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[001.两数之和]]></title>
      <url>%2F2018%2F03%2F29%2FLeetCode-Two-Sum-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%2F</url>
      <content type="text"><![CDATA[题目:给定一个整数数列，找出其中和为特定值的那两个数。你可以假设每个输入都只会有一种答案，同样的元素不能被重用。示例:123给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 英文版：Two SumGiven an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution.Example:123Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 思路：这道题一看就知道用暴力搜索肯定没问题，而且猜到OJ肯定不会允许用暴力搜索这么简单的方法，于是去试了一下，果然是Time Limit Exceeded，这个算法的时间复杂度是O(n^2)。那么只能想个O(n)的算法来实现，整个实现步骤为：先遍历一遍数组，建立map数据，然后再遍历一遍，开始查找，找到则记录index。代码如下： 解法一：123456789101112131415161718public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; m = new HashMap&lt;Integer, Integer&gt;(); int[] res = new int[2]; for (int i = 0; i &lt; nums.length; ++i) &#123; m.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; ++i) &#123; int t = target - nums[i]; if (m.containsKey(t) &amp;&amp; m.get(t) != i) &#123; res[0] = i; res[1] = m.get(t); break; &#125; &#125; return res; &#125;&#125; 可以写的更加简洁一些，把两个for循环合并成一个。 解法二：123456789101112131415public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; m = new HashMap&lt;Integer, Integer&gt;(); int[] res = new int[2]; for (int i = 0; i &lt; nums.length; ++i) &#123; if (m.containsKey(target - nums[i])) &#123; res[0] = i; res[1] = m.get(target - nums[i]); break; &#125; m.put(nums[i], i); &#125; return res; &#125;&#125; 类似题目：4Sum3Sum Smaller3Sum Closest3SumTwo Sum III - Data structure designTwo Sum II - Input array is sorted 返回LeetCode刷题笔记总纲]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo基础命令]]></title>
      <url>%2F2018%2F03%2F29%2Fhexo%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[hexo n “题目” #完整命令为hexo new “题目”,用于新建一篇名为“题目”的文章 hexo g #完整命令为hexo generate,用于生成静态文件 hexo s #完整命令为hexo server,用于启动服务器，主要用来本地预览 hexo d #完整命令为hexo deploy,用于将本地文件发布到github等git仓库上 http://localhost:4000/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Regular-Expression]]></title>
      <url>%2F2017%2F03%2F17%2FRegular-Expression%2F</url>
      <content type="text"><![CDATA[正则表达式相关的材料收集，持续更新….]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[柔软的时光]]></title>
      <url>%2F2017%2F03%2F03%2Fthe-soft-time%2F</url>
      <content type="text"><![CDATA[美，是绝大多数人所向往的。无论是心灵美，还是外在美。 人们都知道多读书能够让人的心灵更美。那外在美呢？人们最常用的方法就是化妆打扮和衣着服饰。 但是这些方式仍然是一种肤浅的外在美。 毕淑敏在她的《柔软的时光》一书中告诉我们，相由心生，追求貌美的最简单方式也还是读书！做一个美好的人，我相信是绝大多数人的心愿。除了心灵的美好，外表也需美好。 为了这份美好，人们使出了万千手段。比如刀兵相见的整容，比如涂脂抹粉的化妆。为了抚平脸上的皱纹，竟然发明了用肉毒杆菌的毒素在眉眼间注射，让人胆战心惊。 其实，有一个最简单的美容之法，却被人们忽视，那就是读书！ 读书让人懂得倾听 读书的时候，人是专注的。 因为你在聆听一些高贵的灵魂自言自语，不由自主地谦逊和聚精会神。 即使是读闲书，看到妙处，也会忍不住拍案叫绝…… 长久的读书可以使人养成恭敬的习惯，知道这个世界上可以为师的人太多了，在生活中也会沿袭洗耳倾听的姿态。而倾听，是让人神采倍添的绝好方式。 所有的人都渴望被重视，而每一个生命也都不应被忽视。你重视他人，魅力就降临在你的双眸。 读书让人常常微笑 读书的时候，常常会会心一笑。 那些智慧和精彩，那些英明与穿透，让我们在惊叹的同时拈页展颜。 微笑是最好的敷粉和装点，微笑可以传达比所有的语言更丰富的善意与温暖。 读书让人心神欢愉 书是一座快乐的富矿，储存了大量的浓缩的欢愉因子。 当你静夜抚卷的时候，那些因子如同香氛蒸腾，迷住了你的双眼。 你眉飞色舞，中了蛊似的笑了起来，独享其乐。 也许有人说，我读书的时候，时有哭泣！ 这样的哭，其实也是一种广义的微笑，因为灵魂在这一个瞬间舒展，尽情宣泄。 读书让人自知不足 读书让我们知道了天地间很多奥秘，而且知道还有更多奥秘，不曾被人揭露，我们就不敢用目空一切的眼神睥睨天下。 你在书籍里看到了无休无止的时间流淌，你就不敢奢侈，不敢口出狂言。 自知是一切美好的基石。当你把他人的聪慧加上你自己的理解，恰如其分地轻轻说出的时候，你的红唇就比任何美丽色彩的涂抹，都更加光艳夺目。 你想美好吗？那就读书吧。 不需要花费很多的金钱，但要花费很多的时间。 坚持下去，持之以恒，优美就像五月的花环，某一天飘然而至，簇拥你颈间。 一个人静静的捧着一本书，给自己的心灵找个可以栖息的地方，那里是一个春有百花秋有月，夏有凉风冬有雪的地方，是一个可以肆意做梦的地方，一个让你欢喜让你忧的地方……]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从今天开始换一种生活方式--极简生活]]></title>
      <url>%2F2017%2F03%2F01%2Fminimalist-lifestyle%2F</url>
      <content type="text"><![CDATA[一 今天3月1日，2017年已悄然流逝六分之一，想想都觉得可怕，你在年初立下的目标有好好在执行吗？ “断舍离”想必出现在很多人的新年计划清单中，今天，我们来谈谈为什么极简主义生活让人觉得更幸福？ 在谈极简主义生活带来的幸福感受之前，得先谈一谈什么是极简主义生活方式。 二 极简主义生活方式不仅仅是物质上的极简，同时追求精神上的专注，舍弃不需要的多余的东西，专注在自己真正想要的事情上，从而获得最大的幸福和精神上的自由。看过网上总结的极简主义生活方式，深以为妙： 欲望、精神极简 了解自己的真实欲望，不受外界影响，不盲从，不跟风。 把自己的精力集中在自己最迫切的欲望上，要清楚自己内心真正想要的东西，放弃其他无用的欲望，不要浪费时间在不值得的人和事情上。专注在自己真正想从事的精神活动中，把时间和精力放在提升自我上。 物质极简 把超过一年不用的物品丢弃、送人、出售或者捐赠掉，例如看过的杂志、书、不再穿的衣服等。在日常生活中，买东西前，先想想是不是真的需要，不囤东西，不花冤枉钱，也不要为了省钱买便宜货、次品，不重复购买。 信息极简 减少花在社交网络上的时间，少刷微博和朋友圈。 不关注与自己无关八卦新闻。 关注要少而精，删除长期不打开的App 表达极简 不走套路，直截了当。 工作极简 不拖延、不拖延、不拖延 一次只专注一件事 学会管理时间，提高做事效率 生活极简 慢点、慢点、再慢点 静点、静点、再静点 在慢生活中感受当下生活 不参与无效社交 坚持健身、锻炼，养成习惯 健康饮食 三 极简主义生活方式给了我们重新审视自己与物品和周边环境的关系，顺便整理内心的垃圾。 蛮欲缭身，何以解脱？唯断、舍、离。被欲望左右的人，总是在计较得失中与幸福失之交臂。 想要幸福生活，当从断绝一切不需之物开始，当你拥有的东西变少后，就不需要花大量时间在整理和清洁工作上，你的生活也变得更加简单高效起来，通过不断清理生活中不必要的东西，你会越来越清楚自己真正想要的是什么。 不再是物质的奴隶，极低的物欲让你有更多的时间静下心专注检视自己的精神世界。 极简的社交生活让你有更多的时间花在跟自己相处上，同时与另一半、家人或者好友相处的质量也能得到提高，舍弃无用社交，有缘即往无缘即去，一任清风送白云。 现在很多人跟朋友聚会或者旅行，目光都是停留在手机上的，我们都活在网络里，而忘了切身去领略和感受生活本身之美。 生活中没有任何不必要的杂物和杂事，就只有你真正需要的东西相伴，会让你的生活更轻松、充实、幸福。 从今天起，过一种简单的生活，做一个简单的人，把时间和精力投入在自己真正喜欢的事情上，重新认识自我，用心感受生活之美、发现生命的无限可能。 end]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SQL优化大全]]></title>
      <url>%2F2017%2F02%2F28%2Fmake-SQL-to-be-better%2F</url>
      <content type="text"><![CDATA[1. 优化SQL步骤 1.1. 通过 show status和应用特点了解各种 SQL的执行频率 通过 SHOW STATUS 可以提供服务器状态信息，也可以使用 mysqladmin extende d-status 命令获得。 SHOW STATUS 可以根据需要显示 session 级别的统计结果和 global级别的统计结果。 如显示当前session： SHOW STATUS like “Com_%”; 全局级别：show global status;以下几个参数对 Myisam 和 Innodb 存储引擎都计数： Com_select 执行 select 操作的次数，一次查询只累加 1 ； Com_insert 执行 insert 操作的次数，对于批量插入的 insert 操作，只累加一次 ； Com_update 执行 update 操作的次数； Com_delete 执行 delete 操作的次数； 以下几个参数是针对 Innodb 存储引擎计数的，累加的算法也略有不同： Innodb_rows_read select 查询返回的行数； Innodb_rows_inserted 执行 Insert 操作插入的行数； Innodb_rows_updated 执行 update 操作更新的行数； Innodb_rows_deleted 执行 delete 操作删除的行数； 通过以上几个参数，可以很容易的了解当前数据库的应用是以插入更新为主还 是以查询操作为主，以及各种类型的 SQL大致的执行比例是多少。对于更新操作的计 数，是对执行次数的计数，不论提交还是回滚都会累加。 对于事务型的应用，通过 Com_commit 和 Com_rollback 可以了解事务提交和回 滚的情况，对于回滚操作非常频繁的数据库，可能意味着应用编写存在问题。此外，以下几个参数便于我们了解数据库的基本情况： Connections 试图连接 MySQL 服务器的次数 Uptime 服务器工作时间 Slow_queries 慢查询的次数 1.2. 定位执行效率较低的SQL语句 可以通过以下两种方式定位执行效率较低的 SQL 语句： 可以通过慢查询日志定位那些执行效率较低的 sql 语句，用 –log-slow-queries[=file_name] 选项启动时， mysqld 写一个包含所有执行时间超过long_query_time 秒的 SQL 语句的日志文件。可以链接到管理维护中的相关章节。 使用 show processlist查看当前MYSQL的线程，命令慢查询日志在查询结束以后才纪录，所以在应用反映执行效率出现问题的时候查 询慢查询日志并不能定位问题，可以使用 show processlist 命令查看当前 MySQL 在进行的线程，包括线程的状态，是否锁表等等，可以实时的查看 SQL 执行情况， 同时对一些锁表操作进行优化。 1.3. 通过EXPLAIN 分析低效 SQL的执行计划： 通过以上步骤查询到效率低的 SQL 后，我们可以通过 explain 或者 desc 获取MySQL 如何执行 SELECT 语句的信息，包括 select 语句执行过程表如何连接和连接 的次序。 2. MySQL索引 2.1. mysql如何使用索引 索引用于快速找出在某个列中有一特定值的行。对相关列使用索引是提高SELECT 操作性能的最佳途径。 查询要使用索引最主要的条件是查询条件中需要使用索引关键字，如果是多列 索引，那么只有查询条件使用了多列关键字最左边的前缀时（前缀索引），才可以使用索引，否则 将不能使用索引。 下列情况下， Mysql 不会使用已有的索引： 1、如果 mysql 估计使用索引比全表扫描更慢，则不使用索引。例如：如果 key_part 1均匀分布在 1 和 100 之间，下列查询中使用索引就不是很好：SELECT * FROM table_name where key_part1 &gt; 1 and key_part1 &lt; 902、如果使用 heap 表并且 where 条件中不用＝索引列，其他 &gt; 、 &lt; 、 &gt;= 、 &lt;= 均不使 用索引（MyISAM和innodb表使用索引）；3、使用or分割的条件，如果or前的条件中的列有索引，后面的列中没有索引，那么涉及到的索引都不会使用。4、如果创建复合索引，如果条件中使用的列不是索引列的第一部分；（不是前缀索引）5、如果 like 是以％开始；6、对 where 后边条件为字符串的一定要加引号，字符串如果为数字 mysql 会自动转 为字符串，但是不使用索引。 2.2. 查看索引使用情况 如果索引正在工作， Handler_read_key 的值将很高，这个值代表了一个行被索引值读的次数，很低的值表明增加索引得到的性能改善不高，因为索引并不经常使 用。 Handler_read_rnd_next 的值高则意味着查询运行低效，并且应该建立索引补救。这个值的含义是在数据文件中读下一行的请求数。如果你正进行大量的表扫描， 该值较高。通常说明表索引不正确或写入的查询没有利用索引。 语法： mysql&gt; show status like ‘Handler_read%’; 3. 具体优化查询语句 1. 避免全表扫描 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引 尝试下面的技巧以避免优化器错选了表扫描： 使用ANALYZE TABLEtbl_name为扫描的表更新关键字分布。 对扫描的表使用FORCEINDEX告知MySQL，相对于使用给定的索引表扫描将非常耗时。SELECT * FROM t1, t2 FORCE INDEX (index_for_column) WHERE t1.col_name=t2.col_name； 用–max-seeks-for-key=1000选项启动mysqld或使用SET max_seeks_for_key=1000告知优化器假设关键字扫描不会超过1,000次关键字搜索。 1). where中避免null值判断 否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null NULL对于大多数数据库都需要特殊处理，MySQL也不例外，它需要更多的代码，更多的检查和特殊的索引逻辑，有些开发人员完全没有意识到，创建表时NULL是默认值，但大多数时候应该使用NOT NULL，或者使用一个特殊的值，如0，-1作为默 认值。 不能用null作索引，任何包含null值的列都将不会被包含在索引中。即使索引有多列这样的情况下，只要这些列中有一列含有null，该列 就会从索引中排除。也就是说如果某列存在空值，即使对该列建索引也不会提高性能。 任何在where子句中使用is null或is not null的语句优化器是不允许使用索引的。 此例可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 2).where中避免!=或&lt;&gt;操作符 否则将引擎放弃使用索引而进行全表扫描。 MySQL只有对以下操作符才使用索引：&lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN，以及某些时候的LIKE。 可以在LIKE操作中使用索引的情形是指另一个操作数不是以通配符（%或者_）开头的情形。例如:12SELECT id FROM t WHERE col LIKE &apos;Mich%&apos;; # 这个查询将使用索引，SELECT id FROM t WHERE col LIKE &apos;%ike&apos;; #这个查询不会使用索引。 3).where中避免or 否则将导致引擎放弃使用索引而进行全表扫描，如：1select id from t where num=10 or num=20 可以使用UNION合并查询：1 select id from t where num=10 union all select id from t where num=20 在某些情况下，or条件可以避免全表扫描的。 1 .where 语句里面如果带有or条件, myisam表能用到索引， innodb不行。2 .必须所有的or条件都必须是独立索引 4) .in和not in会导致全表扫描 如：select id from t where num in(1,2,3)对于连续的数值，能用 between 就不要用 in ：Select id from t where num between 1 and 3 5).下面的查询也将导致全表扫描： select id from t where name like ‘%abc%’ select id from t where name like ‘%abc’ 若要提高效率，可以考虑全文检索。 而select id from t where name like ‘abc%’ 才用到索引 6).where中使用参数，也会导致全表扫描 因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推 迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num 可以改为强制查询使用索引： select id from t with(index(索引名)) where num=@num 7). where中避免对字段进行表达式操作 这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num/2=100 应改为: select id from t where num=100*2 8). where中避免对字段进行函数操作 这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)=’abc’ –nameselect id from t where datediff(day,createdate,’2005-11-30’)=0–‘2005-11-30’ 生成的id 应改为:select id from t where name like ‘abc%’select id from t where createdate&gt;=’2005-11-30’ and createdate&lt;’2005-12-1’ 9).where子句避免“=”左边进行函数、算术运算或其他表达式运算 否则系统将可能无法正确使用索引。 10). 索引字段不是复合索引的前缀索引 例如 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 2 .其他一些注意优化 1). 不要写一些没有意义的查询 如需要生成一个空表结构： select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：create table #t(…) 2). 很多时候用 exists 代替 in 是一个好的选择 select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num=a.num) 3). 并不是所有索引对查询都有效 SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 4). 索引并不是越多越好 索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 5).避免更新 clustered 索引数据列 因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 6).尽量使用数字型字段 若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 7).使用 varchar/nvarchar 代替 char/nchar 因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 8).避免“*“返回所有 用具体的字段列表代替“*”，不要返回用不到的任何字段。 3. 临时表的问题 1). 尽量使用表变量来代替临时表 如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 2).避免频繁创建和删除临时表，以减少系统表资源的消耗3).临时表并不是不可使用 适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 4).避免造成大量 log 如：在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度 如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 5). 及时删除临时表 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 4. 游标的问题 1).尽量避免使用游标 因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 2).使用基于游标的方法或临时表方法之前 应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 3).游标并不是不可使用 对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 4).存储过程和触发器 在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。 无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 5. 事务的问题 1).尽量避免大事务操作，提高系统并发能力。6. 数据量的问题 1).尽量避免向客户端返回大数据量 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 7. COUNT优化 1) count(*) 优于count(1)和count(primary_key) 很多人为了统计记录条数，就使用 count(1) 和 count(primary_key) 而不是 count(*) ，他们认为这样性能更好，其实这是一个误区。对于有些场景，这样做可能性能会更差，应为数据库对 count(*) 计数操作做了一些特别的优化。 2）count(column) 和 count(*) 是不一样的 这个误区甚至在很多的资深工程师或者是 DBA 中都普遍存在，很多人都会认为这是理所当然的。实际上，count(column) 和 count(*) 是一个完全不一样的操作，所代表的意义也完全不一样。 count(column) 是表示结果集中有多少个column字段不为空的记录 count(*) 是表示整个结果集有多少条记录 8. 优化order by语句 基于索引的排序 MySQL的弱点之一是它的排序。虽然MySQL可以在1秒中查询大约15,000条记录，但由于MySQL在查询时最多只能使用一个索引。因此，如果WHERE条件已经占用了索引，那么在排序中就不使用索引了，这将大大降低查询的速度。我们可以看看如下的SQL语句: SELECT * FROM SALES WHERE NAME = “name” ORDER BY SALE_DATE DESC; 在以上的SQL的WHERE子句中已经使用了NAME字段上的索引，因此，在对SALE_DATE进行排序时将不再使用索引。为了解决这个问题，我们可以对SALES表建立复合索引: ALTER TABLE SALES DROP INDEX NAME, ADD INDEX (NAME,SALE_DATE) 这样再使用上述的SELECT语句进行查询时速度就会大副提升。但要注意，在使用这个方法时，要确保WHERE子句中没有排序字段，在上例中就是不能用SALE_DATE进行查询，否则虽然排序快了，但是SALE_DATE字段上没有单独的索引，因此查询又会慢下来。 在某些情况中， MySQL可以使用一个索引来满足 ORDER BY子句，而不需要额外的排序。 where条件和order by使用相同的索引，并且order by 的顺序和索引顺序相 同，并且order by的字段都是升序或者都是降序。例如：下列sql可以使用索引。 SELECT FROM t1 ORDER BY key_part1,key_part2,… ; SELECT FROM t1 WHERE key_part1=1 ORDER BY key_part1 DESC, key_part2 DESC; SELECT FROM t1 ORDER BY key_part1 DESC, key_part2 DESC; 但是以下情况不使用索引： SELECT FROM t1 ORDER BY key_part1 DESC, key_part2 ASC ； –order by 的字段混合 ASC 和 DESC SELECT FROM t1 WHERE key2=constant ORDER BY key1 ；– 用于查询行的关键字与 ORDER BY 中所使用的不相同 SELECT FROM t1 ORDER BY key1, key2 ；– 对不同的关键字使用 ORDER BY ： 9. 优化GROUP BY 默认情况下， MySQL 排序所有 GROUP BY col1 ， col2 ， …. 。查询的方法如同在查询中指定 ORDER BY col1 ， col2 ， … 。如果显式包括一个包含相同的列的 ORDER BY子句， MySQL 可以毫不减速地对它进行优化，尽管仍然进行排序。如果查询包括 GROUP BY 但你想要避免排序结果的消耗，你可以指定 ORDER BY NULL禁止排序。 例如 ： INSERT INTO foo SELECT a, COUNT(*) FROM bar GROUP BY a ORDER BY NULL; 10. 优化 OR 具体详解看：mysql or条件可以使用索引而避免全表 4. SQL核心语句(非常实用的几个技巧) 1) 插入数据1234INSERT mytable (first_column,second_column,third_column) VALUES (&apos;some data&apos;,&apos;some more data&apos;,&apos;yet more data&apos;) , VALUES (&apos;some data&apos;,&apos;some more data&apos;,&apos;yet more data&apos;) , VALUES (&apos;some data&apos;,&apos;some more data&apos;,&apos;yet more data&apos;) 2）.清空数据表1TRUNCATE TABLE `mytable` 注意：删除表中的所有记录，应使用TRUNCATE TABLE语句。注意这里为什么要用TRUNCATE TABLE语句代替DELETE语句:当你使用TRUNCATE TABLE语句时，记录的删除是不作记录的。也就是说，这意味着TRUNCATE TABLE要比DELETE快得多。 3）用SELECT创建记录和表 INSERT语句与DELETE语句和UPDATE语句有一点不同，它一次只操作一个记录。然而，有一个方法可以使INSERT 语句一次添加多个记录。要作到这一点，你需要把INSERT语句与SELECT语句结合起来，象这样:12INSERT mytable(first_column,second_column) SELECT another_first,another_second FROM anothertable WHERE another_first=&apos;Copy Me!&apos;; 这个语句从anothertable拷贝记录到mytable.只有表anothertable中字段another_first的值为’Copy Me!’的记录才被拷贝。 当为一个表中的记录建立备份时，这种形式的INSERT语句是非常有用的。在删除一个表中的记录之前，你可以先用这种方法把它们拷贝到另一个表中。 如果你需要拷贝整个表，你可以使用SELECT INTO语句。例如，下面的语句创建了一个名为newtable的新表，该表包含表mytable的所有数据:1SELECT * INTO newtable FROM mytable; 你也可以指定只有特定的字段被用来创建这个新表。要做到这一点，只需在字段列表中指定你想要拷贝的字段。另外，你可以使用WHERE子句来限制拷贝到新表中的记录。下面的例子只拷贝字段second_columnd的值等于’Copy Me!’的记录的first_column字段。123SELECT first_column INTO newtable FROM mytable WHERE second_column=&apos;Copy Me!&apos;; 使用SQL修改已经建立的表是很困难的。例如，如果你向一个表中添加了一个字段，没有容易的办法来去除它。另外，如果你不小心把一个字段的数据类型给错了，你将没有办法改变它。但是，使用本节中讲述的SQL语句，你可以绕过这两个问题。 例如，假设你想从一个表中删除一个字段。使用SELECT INTO语句，你可以创建该表的一个拷贝，但不包含要删除的字段。这使你既删除了该字段，又保留了不想删除的数据。 如果你想改变一个字段的数据类型，你可以创建一个包含正确数据类型字段的新表。创建好该表后，你就可以结合使用UPDATE语句和SELECT语句，把原来表中的所有数据拷贝到新表中。通过这种方法，你既可以修改表的结构，又能保存原有的数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[100个Linux常用命令总结]]></title>
      <url>%2F2017%2F02%2F27%2F100-linux-commands%2F</url>
      <content type="text"><![CDATA[1.ls [选项] [目录名 | 列出相关目录下的所有目录和文件123456-a 列出包括.a开头的隐藏文件的所有文件-A 通-a，但不列出&quot;.&quot;和&quot;..&quot;-l 列出文件的详细信息-c 根据ctime排序显示-t 根据文件修改时间排序---color[=WHEN] 用色彩辨别文件类型 WHEN 可以是&apos;never&apos;、&apos;always&apos;或&apos;auto&apos;其中之一 白色：表示普通文件 蓝色：表示目录 绿色：表示可执行文件 红色：表示压缩文件 浅蓝色：链接文件 红色闪烁：表示链接的文件有问题 黄色：表示设备文件 灰色：表示其它文件 2.mv [选项] 源文件或目录 目录或多个源文件 | 移动或重命名文件12345-b 覆盖前做备份-f 如存在不询问而强制覆盖-i 如存在则询问是否覆盖-u 较新才覆盖-t 将多个源文件移动到统一目录下，目录参数在前，文件参数在后 1234eg: mv a /tmp/ 将文件a移动到 /tmp目录下 mv a b 将a命名为b mv /home/zenghao test1.txt test2.txt test3.txt 3.cp [选项] 源文件或目录 目录或多个源文件 | 将源文件复制至目标文件，或将多个源文件复制至目标目录。12345-r -R 递归复制该目录及其子目录内容-p 连同档案属性一起复制过去-f 不询问而强制复制-s 生成快捷方式-a 将档案的所有特性都一起复制 4.scp [参数] [原路径] [目标路径] | 在Linux服务器之间复制文件和目录12-v 详细显示输出的具体情况-r 递归复制整个目录 (1) 复制文件：命令格式：1234567scp local_file remote_username@remote_ip:remote_folder 或者 scp local_file remote_username@remote_ip:remote_file 或者 scp local_file remote_ip:remote_folder 或者 scp local_file remote_ip:remote_file 第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名 (2) 复制目录：命令格式：123scp -r local_folder remote_username@remote_ip:remote_folder 或者 scp -r local_folder remote_ip:remote_folder 第1个指定了用户名，命令执行后需要输入用户密码；第2个没有指定用户名，命令执行后需要输入用户名和密码；eg:1234从 本地 复制到 远程scp /home/daisy/full.tar.gz root@172.19.2.75:/home/root 从 远程 复制到 本地scp root@/172.19.2.75:/home/root/full.tar.gz /home/daisy/full.tar.gz 5.rm [选项] 文件 | 删除文件1234-r 删除文件夹-f 删除不提示-i 删除提示-v 详细显示进行步骤 6.touch [选项] 文件 | 创建空文件或更新文件时间12345-a 只修改存取时间-m 值修改变动时间-r eg:touch -r a b ,使b的时间和a相同-t 指定特定的时间 eg:touch -t 201211142234.50 log.log -t time [[CC]YY]MMDDhhmm[.SS],C:年前两位 7.pwd 查看当前所在路径8.cd 改变当前目录1234\- ：返回上层目录.. :返回上层目录回车 ：返回主目录/ :根目录 9.mkdir [选项] 目录… | 创建新目录123-p 递归创建目录，若父目录不存在则依次创建-m 自定义创建目录的权限 eg:mkdir -m 777 hehe-v 显示创建目录的详细信息 10.rmdir 删除空目录12-v 显示执行过程-p 若自父母删除后父目录为空则一并删除 11.rm [选项] 文件… | 一个或多个文件或目录1234-f 忽略不存在的文件，不给出提示-i 交互式删除-r 将列出的目录及其子目录递归删除-v 列出详细信息 12.echo：显示内容12345-n 输出后不换行-e 遇到转义字符特殊处理 eg: echo &quot;he\nhe&quot; 显示he\nhe ehco -e &quot;he\nhe&quot; 显示he(换行了)he 13.cat [选项] [文件]..| 一次显示整个文件或从键盘创建一个文件或将几个文件合并成一个文件12-n 编号文件内容再输出-E 在结束行提示$ 14.tac | 反向显示15.more | 按页查看文章内容，从前向后读取文件，因此在启动时就加载整个文件12345+n 从第n行开始显示-n 每次查看n行数据+/String 搜寻String字符串位置，从其前两行开始查看-c 清屏再显示-p 换页时清屏 16.less | 可前后移动地逐屏查看文章内容，在查看前不会加载整个文件12345678-m 显示类似于more命令的百分比-N 显示行号/ 字符串：向下搜索“字符串”的功能? 字符串：向上搜索“字符串”的功能n 重复前一个搜索（与 / 或 ? 有关）N 反向重复前一个搜索（与 / 或 ? 有关）b 向后翻一页d 向后翻半页 17.nl [选项]… [文件]… | 将输出内容自动加上行号1234-b -b a 不论是否有空行，都列出行号（类似 cat -n) -b t 空行则不列行号（默认） -n 有ln rn rz三个参数，分别为再最左方显示，最右方显示不加0，最右方显示加0 18.head [参数]… [文件]… | 显示档案开头，默认开头10行1234-v 显示文件名-c number 显示前number个字符,若number为负数,则显示除最后number个字符的所有内容-number/n (+)number 显示前number行内容，-n number 若number为负数，则显示除最后number行数据的所有内容 19.tail [必要参数] [选择参数] [文件] | 显示文件结尾内容123456-v 显示详细的处理信息-q 不显示处理信息-num/-n (-)num 显示最后num行内容-n +num 从第num行开始显示后面的数据-c 显示最后c个字符-f 循环读取 20.vi 编辑文件:w filename 将文章以指定的文件名保存起来:wq 保存并退出:q! 不保存而强制退出命令行模式功能键 1）插入模式按「i」切换进入插入模式「insert mode」，按”i”进入插入模式后是从光标当前位置开始输入文件；按「a」进入插入模式后，是从目前光标所在位置的下一个位置开始输入文字；按「o」进入插入模式后，是插入新的一行，从行首开始输入文字。 2）从插入模式切换为命令行模式 按「ESC」键。 3）移动光标 vi可以直接用键盘上的光标来上下左右移动，但正规的vi是用小写英文字母「h」、「j」、「k」、「l」，分别控制光标左、下、上、右移一格。 按「ctrl」+「b」：屏幕往”后”移动一页。 按「ctrl」+「f」：屏幕往”前”移动一页。 按「ctrl」+「u」：屏幕往”后”移动半页。 按「ctrl」+「d」：屏幕往”前”移动半页。 按数字「0」：移到文章的开头。 按「G」：移动到文章的最后。 按「$」：移动到光标所在行的”行尾”。 按「^」：移动到光标所在行的”行首” 按「w」：光标跳到下个字的开头 按「e」：光标跳到下个字的字尾 按「b」：光标回到上个字的开头 按「#l」：光标移到该行的第#个位置，如：5l,56l。 4）删除文字 「x」：每按一次，删除光标所在位置的”后面”一个字符。 「#x」：例如，「6x」表示删除光标所在位置的”后面”6个字符。 「X」：大写的X，每按一次，删除光标所在位置的”前面”一个字符。 「#X」：例如，「20X」表示删除光标所在位置的”前面”20个字符。 「dd」：删除光标所在行。 「#dd」：从光标所在行开始删除#行 5）复制 「yw」：将光标所在之处到字尾的字符复制到缓冲区中。 「#yw」：复制#个字到缓冲区 「yy」：复制光标所在行到缓冲区。 「#yy」：例如，「6yy」表示拷贝从光标所在的该行”往下数”6行文字。 「p」：将缓冲区内的字符贴到光标所在位置。注意：所有与”y”有关的复制命令都必须与”p”配合才能完成复制与粘贴功能。 6）替换 「r」：替换光标所在处的字符。 「R」：替换光标所到之处的字符，直到按下「ESC」键为止。 7）回复上一次操作 「u」：如果您误执行一个命令，可以马上按下「u」，回到上一个操作。按多次”u”可以执行多次回复。 8）更改 「cw」：更改光标所在处的字到字尾处 「c#w」：例如，「c3w」表示更改3个字 9）跳至指定的行 「ctrl」+「g」列出光标所在行的行号。 「#G」：例如，「15G」，表示移动光标至文章的第15行行首。 21.which 可执行文件名称 | 查看可执行文件的位置，在PATH变量指定的路径中查看系统命令是否存在及其位置22.whereis [-bmsu] [BMS 目录名 -f ] 文件名| 定位可执行文件、源代码文件、帮助文件在文件系统中的位置1234567-b 定位可执行文件。-m 定位帮助文件。-s 定位源代码文件。-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。-B 指定搜索可执行文件的路径。-M 指定搜索帮助文件的路径。-S 指定搜索源代码文件的路径。 23.locate | 通过搜寻数据库快速搜寻档案1-r 使用正规运算式做寻找的条件 24.find find [PATH] [option] [action] | 在文件树种查找文件，并作出相应的处理选项与参数： 1). 与时间有关的选项：共有 -atime, -ctime 与 -mtime 和-amin,-cmin与-mmin，以 -mtime 说明1234-mtime n ：n 为数字，意义为在 n 天之前的『一天之内』被更动过内容的档案；-mtime +n ：列出在 n 天之前(不含 n 天本身)被更动过内容的档案档名；-mtime -n ：列出在 n 天之内(含 n 天本身)被更动过内容的档案档名。-newer file ：file 为一个存在的档案，列出比 file 还要新的档案档名 2). 与使用者或组名有关的参数：123456-uid n ：n 为数字，这个数字是用户的账号 ID，亦即 UID-gid n ：n 为数字，这个数字是组名的 ID，亦即 GID-user name ：name 为使用者账号名称！例如 dmtsai-group name：name 为组名，例如 users ；-nouser ：寻找档案的拥有者不存在 /etc/passwd 的人！-nogroup ：寻找档案的拥有群组不存在于 /etc/group 的档案！ 3). 与档案权限及名称有关的参数：1234567891011121314-name filename：搜寻文件名为 filename 的档案（可使用通配符）-size [+-]SIZE：搜寻比 SIZE 还要大(+)或小(-)的档案。这个 SIZE 的规格有： c: 代表 byte k: 代表 1024bytes。所以，要找比 50KB还要大的档案，就是『 -size +50k 』-type TYPE ：搜寻档案的类型为 TYPE 的，类型主要有： 一般正规档案 (f) 装置档案 (b, c) 目录 (d) 连结档 (l) socket (s) FIFO (p)-perm mode ：搜寻档案权限『刚好等于』 mode的档案，这个mode为类似chmod的属性值，举例来说，-rwsr-xr-x 的属性为4755！-perm -mode ：搜寻档案权限『必须要全部囊括 mode 的权限』的档案，举例来说,我们要搜寻-rwxr--r-- 亦即 0744 的档案，使用-perm -0744，当一个档案的权限为 -rwsr-xr-x ，亦即4755 时，也会被列出来，因为 -rwsr-xr-x 的属性已经囊括了 -rwxr--r-- 的属性了。-perm +mode ：搜寻档案权限『包含任一 mode 的权限』的档案，举例来说，我们搜寻-rwxr-xr-x ，亦即 -perm +755 时，但一个文件属性为 -rw-------也会被列出来，因为他有 -rw.... 的属性存在！ 25.grep ‘正则表达式’ 文件名 | 用正则表达式搜索文本，并把匹配的行打印出来12345-c 只输出匹配行的计数。-I 不区分大小写(只适用于单字符)。-l 只显示文件名-v 显示不包含匹配文本的所有行。-n 显示匹配行数据及其行号 26.gzip [-cdtv#] 檔名 | 压缩、解压缩，源文件都不再存在1234-d 进行解压缩-c 将压缩的数据输出到屏幕上-v :显示原档案/压缩文件案的压缩比等信息-# ：压缩等级，-1最快，但压缩比最差，=9最慢，但压缩比最好 27.gunzip | 解压缩28.bzip2 | 压缩、解压缩123456-d :解压-z :压缩-k :保留源文件-c ：将压缩的过程产生的数据输出到屏幕上！-v ：可以显示出原档案/压缩文件案的压缩比等信息；-# ：与 gzip 同样的，都是在计算压缩比的参数， -9 最佳， -1 最快！ 29.bzcat 读取数据而无需解压30.tar [主选项+辅选项] 文件或者目录 | 多个目录或档案打包、压缩成一个大档案主选项：123-c 建立打包档案，可搭配 -v 来察看过程中被打包的档名(filename)-t 察看打包档案的内容含有哪些档名，重点在察看『档名』就是了；-x 解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 辅选项：12345678-j 透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 \*.tar.bz2-z 透过 gzip 的支持进行压缩/解压缩：此时档名最好为 \*.tar.gz-v 在压缩/解压缩的过程中，将正在处理的文件名显示出来！-f filename -f 后面要立刻接要被处理的档名！-C 目录 这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。--exclude FILE：在压缩打包过程中忽略某文件 eg: tar --exclude /home/zenghao -zcvf myfile.tar.gz /home/* /etc-p 保留备份数据的原本权限与属性，常用于备份(-c)重要的配置文件-P(大写） 保留绝对路径，亦即允许备份数据中含有根目录存在之意； eg:123压 缩：tar -jcvf filename.tar.bz2 要被压缩的档案或目录名称查 询：tar -jtvf filename.tar.bz2解压缩：tar -jxvf filename.tar.bz2 -C 欲解压缩的目录 31.exit 退出当前shell32.logout 退出登录shell33.shutdown -h now34.users 显示当前登录系统地用户35.who 登录在本机的用户与来源1-H或--heading 显示各栏位的标题信息列。 36.w 登录在本机的用户及其运行的程序12-s 使用简洁格式列表，不显示用户登入时间，终端机阶段作业和程序所耗费的CPU时间。-h 不显示各栏位的标题信息列。 37.write 给当前联机的用户发消息38.wall 给所有登录再本机的用户发消息39.last 查看用户的登陆日志40.lastlog 查看每个用户最后的登陆时间41.finger [选项] [使用者] [用户@主机] | 查看用户信息123-s 显示用户的注册名、实际姓名、终端名称、写状态、停滞时间、登录时间等信息-l 除了用-s选项显示的信息外，还显示用户主目录、登录shell、邮件状态等信息，以及用户主目录下的.plan、.project和.forward文件的内容。-p 除了不显示.plan文件和.project文件以外，与-l选项相同 42.hostname 查看主机名43.alias ii = “ls -l” | 添加别名44.unalias ii | 清除别名45.useradd [-u UID] [-g 初始群组] [-G 次要群组] [-c 说明栏] [-d 家目录绝对路径] [-s shell] 使用者账号名 | 新增用户12345-M 不建立用户家目录！(系统账号默认值)-m 建立用户家目录！(一般账号默认值)-r 建立一个系统的账号，这个账号的 UID 会有限制 -e 账号失效日期，格式为『YYYY-MM-DD』-D 查看useradd的各项默认值 46.passwd | 修改密码12345678-l 使密码失效-u 与-l相对，用户解锁-S 列出登陆用户passwd文件内的相关参数-n 后面接天数，shadow 的第 4 字段，多久不可修改密码天数-x 后面接天数，shadow 的第 5 字段，多久内必须要更动密码-w 后面接天数，shadow 的第 6 字段，密码过期前的警告天数-i 后面接『日期』，shadow 的第 7 字段，密码失效日期使用管道刘设置密码：echo &quot;zeng&quot; | passwd --stdin zenghao 47.userdel 删除用户1-r 用户文件一并删除 48.chage [-ldEImMW] 账号名 | 修改用户密码的相关属性1234567-l 列出该账号的详细密码参数；-d 后面接日期，修改 shadow 第三字段(最近一次更改密码的日期)，格式YYYY-MM-DD-E 后面接日期，修改 shadow 第八字段(账号失效日)，格式 YYYY-MM-DD-I 后面接天数，修改 shadow 第七字段(密码失效日期)-m 后面接天数，修改 shadow 第四字段(密码最短保留天数)-M 后面接天数，修改 shadow 第五字段(密码多久需要进行变更)-W 后面接天数，修改 shadow 第六字段(密码过期前警告日期) 49.usermod [-cdegGlsuLU] username | 修改用户的相关属性1234567891011-c 后面接账号的说明，即 /etc/passwd 第五栏的说明栏，可以加入一些账号的说明。-d 后面接账号的家目录，即修改 /etc/passwd 的第六栏；-e 后面接日期，格式是 YYYY-MM-DD 也就是在 /etc/shadow 内的第八个字段数据啦！-f 后面接天数为 shadow 的第七字段。-g 后面接初始群组，修改 /etc/passwd 的第四个字段，亦即是GID的字段！-G 后面接次要群组，修改这个使用者能够支持的群组-l 后面接账号名称。亦即是修改账号名称， /etc/passwd 的第一栏！-s 后面接 Shell 的实际档案，例如 /bin/bash 或 /bin/csh 等等。-u 后面接 UID 数字啦！即 /etc/passwd 第三栏的资料；-L 冻结密码-U 解冻密码 50.id [username] | 查看用户相关的id信息，还可以用来判断用户是否存在51.groups 查看登陆用户支持的群组， 第一个输出的群组为有效群组52.newgrp 切换有效群组53.groupadd [-g gid] 组名 | 添加组1-g 设定添加组的特定组id 54.groupmod [-g gid] [-n group_name] 群组名 | 修改组信息12-g 修改既有的 GID 数字-n 修改既有的组名 55.groupdel [groupname] | 删除群组56.gpasswd | 群组管理员功能root管理员动作：123456-gpasswd groupname 设定密码-gpasswd [-A user1,...] [-M user3,...] groupname -A 将 groupname 的主控权交由后面的使用者管理(该群组的管理员) -M 将某些账号加入这个群组当中-gpasswd [-r] groupname -r 将 groupname 的密码移除 群组管理员动作：123- gpasswd [-ad] user groupname -a 将某位使用者加入到 groupname 这个群组当中 -d 将某位使用者移除出 groupname 这个群组当中 57.chfn修改个人信息58.mount [-t vfstype] [-o options] device dir123-ro 采用只读方式挂接设备-rw 采用读写方式挂接设备eg:mount /home/mydisk.iso /tmp/mnt 通过mnt访问mydisk内的内容 59.umount 取消挂载60.cut1234-b ：以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。-c ：以字符为单位进行分割。-d ：自定义分隔符，默认为制表符。-f ：与-d一起使用，指定显示哪个区域。 61.sort12345-n 依照数值的大小排序。-o&lt;输出文件&gt; 将排序后的结果存入指定的文件。-r 以相反的顺序来排序。-t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。-k 选择以哪个区间进行排序。 62.wc 统计指定文件中的字节数、字数、行数, 并将统计结果显示输出1234-l filename 报告行数-c filename 报告字节数-m filename 报告字符数-w filename 报告单词数 63.uniq 去除文件中相邻的重复行12清空/新建文件，将内容重定向输入进去 &amp;&gt; 正确、错误都重定向过去 64.set 显示环境变量和普通变量65.env 显示环境变量66.export 把普通变量变成环境变量67.unset 删除一个环境变量1aaa()&#123;&#125; 定义函数 68.read12-p 接提示字符-t 接等待的秒数 69.declare、typeset1234-i 声明为整数-a 声明为数组-f 声明为函数-r 声明为只读 70.ulimit 限制使用者的某些系统资源1-f 此 shell 可以建立的最大档案容量 (一般可能设定为 2GB)单位为 Kbytes eg: ulimit -f 1024 限制使用者仅能建立 1MBytes 以下的容量的档案 71.df [选项] [文件] | 显示指定磁盘文件的可用空间,如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示12345-a 显示全部文件系统-h 文件大小友好显示-l 只显示本地文件系统-i 显示inode信息-T 显示文件系统类型 72.du [选项] [文件] | 显示每个文件和目录的磁盘使用空间12-h 方便阅读的方式-s 只显示总和的大小 73.ln [参数] [源文件或目录] [目标文件或目录] | 某一个文件在另外一个位置建立一个同步的链接12-s 建立软连接 -v 显示详细的处理过程 74.diff [参数] [文件1或目录1] [文件2或目录2] | 比较单个文件或者目录内容12345-b 不检查空格字符的不同。-B 不检查空白行。-i 不检查大小写-q 仅显示差异而不显示详细信息eg: diff a b &gt; parch.log 比较两个文件的不同并产生补丁 75.date [参数]… [+格式] | 显示或设定系统的日期与时间12345%H 小时(以00-23来表示)。 %M 分钟(以00-59来表示)。 %P AM或PM。%D 日期(含年月日)%U 该年中的周数。 12345date -s “2015-10-17 01:01:01″ //时间设定date +%Y%m%d //显示前天年月日date +%Y%m%d --date=&quot;+1 day/month/year&quot; //显示前一天/月/年的日期date +%Y%m%d --date=&quot;-1 day/month/year&quot; //显示后一天/月/年的日期date -d &apos;2 weeks&apos; 2周后的日期 76.cal [参数] 月份] [年份] | 查看日历123456-1 显示当月的月历-3 显示前、当、后一个月的日历-m 显示星期一为一个星期的第一天-s （默认）星期天为第一天-j 显示当月是一年中的第几天的日历-y 显示当前年份的日历 77.ps | 列出当前进程的快照12345678910a 显示所有的进程-a 显示同一终端下的所有程序e 显示环境变量f 显示进程间的关系-H 显示树状结构r 显示当前终端的程序T 显示当前终端的所有程序-au 显示更详细的信息-aux 显示所有包含其他使用者的行程 -u 指定用户的所有进程 78.top [参数] | 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等79.kill [参数] [进程号] | 杀死进程80.free [参数] | 显示Linux系统中空闲的、已用的物理内存及swap内存,及被内核使用的buffer81.vmstat | 对操作系统的虚拟内存、进程、CPU活动进行监控82.iostat [参数] [时间t] [次数n](每隔t时间刷新一次，最多刷新n次）| 对系统的磁盘操作活动进行监视,汇报磁盘活动统计情况，同时也会汇报出CPU使用情况1-p[磁盘] 显示磁盘和分区的情况 83.watch [参数] [命令] |重复执行某一命令以观察变化12-n 时隔多少秒刷新-d 高亮显示动态变化 84.at [参数] [时间] | 在一个指定的时间执行一个指定任务，只能执行一次HH:MM[am|pm] + number [minutes|hours|days|weeks] 强制在某年某月某日的某时刻进行该项任务123atq 查看系统未执行的任务atrm n 删除编号为n的任务at -c n 显示编号为n的任务的内容 85.crontab | 定时任务调度file 载入crontab123-e 编辑某个用户的crontab文件内容-l 显示某个用户的crontab文件内容-r 删除某个用户的crontab文件 86.ifconfig [网络设备] [参数] | 查看和配置网络设备87.route | 显示和操作IP路由表88.ping [参数] [主机名或IP地址] | 测试与目标主机的连通性1-q 只显示最后的结果 89.netstat | 显示与IP、TCP、UDP和ICMP协议相关的统计数据90.telnet [参数] [主机] | 用于远程登录，采用明文传送报文，安全性不好91.rcp [参数] [源文件] [目标文件] | 远程文件拷贝123-r 递归复制-p 保留源文件的属性usage: rcp –r remote_hostname:remote_dir local_dir 92.wget [参数] [URL地址] | 直接从网络上下载文件12-o FILE 把记录写到FILE文件中 eg : wget -O a.txt URLwget --limit-rate=300k URL 限速下载 93.awk12-F 分隔符 以分隔符分隔内容&#123;&#125; 要执行的脚本内容 eg:cat /etc/passwd |awk -F &apos;:&apos; &apos;&#123;print $1&quot;\t&quot;$7&#125;&apos; 94.sed 对数据行进行替换、删除、新增、选取等操作1234a 新增，在新的下一行出现c 取代，取代 n1,n2 之间的行 eg: sed &apos;1,2c Hi&apos; abd 删除i 插入，在新的上一行出现 95.paste 合并文件，需确保合并的两文件行数相同12-d 指定不同于空格或tab键的域分隔符-s 按行合并，单独一个文件为一行 96.su [参数] user | 切换登陆12-l 切换时连同环境变量、工作目录一起改变-c command 执行command变回原来的使用者 97.sudo | 以特定用户的权限执行特定命令12-l 列出当前用户可执行的命令-u username#uid 以指定用户执行命令]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark快速大数据分析]]></title>
      <url>%2F2017%2F02%2F26%2FSpark%E5%BF%AB%E9%80%9F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[原书电子版下载地址：http://download.csdn.net/download/wangcunlin/9547494整本书梳理如下：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[推荐引擎系统结构]]></title>
      <url>%2F2017%2F02%2F26%2F%E6%8E%A8%E8%8D%90%E5%BC%95%E6%93%8E%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84%2F</url>
      <content type="text"><![CDATA[推荐引擎系统结构图如下：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为什么这么多人认为到西藏旅游就能找到人生真谛？]]></title>
      <url>%2F2017%2F02%2F24%2Fwhy-do-so-many-people-think-that-travel-to-tibet-will-be-able-to-find-the-true-meaning-of-life%2F</url>
      <content type="text"><![CDATA[偶然看到的一篇短文，分享给有缘的你。 因为他们走过的地方还不够多。他们所谓的真谛是如此的肤浅。 在这个信仰缺失的国度和信仰缺失的时代，一些人高喊着辞职去XX，去XX流浪，心灵的旅行之类的总总话语。奔赴某地，看到了一些日常生活中不曾见识的人文抑或自然景观，自以为心受洗礼，旅行结束回到家中，却与俗人无异。标榜旅行经历炫耀所见所闻的人多之又多，而真正因为旅行而获得真谛，改变人生的人少之又少。 我也曾入藏，所见所闻虽然震撼，可是说到真谛，可真是无法总结无法表达。 回到丽江后接待沙发客，数来也有百十来个，有揣着复古交卷相机的豆瓣女青年，有日行80公里胡子拉碴的徒步者，有骑摩托环游中国的大龄未婚女青年，有浮夸牛逼旅途的背包客。 在众多人之中，只有一个女孩真正打动过我。 皮肤黝黑，穿着拖鞋，一个学生式的书包，没有任何户外设备和相机。 签证到期，她从印度回国，重游云南，除非我问到，她从不主动谈自己那些旅行的故事。 她在国内旅游三年，在国外七年，经费靠打工和家中支持。有一个瑞士的男友。 她不写游记，不拍照片，上上网只查查基础的信息。 她坐在我身旁，我完全能感受到内心的平和和宁静。 深夜，我和她坐在古城的院子里喝酒。 我问她，你去过这么多地方，哪里让你感觉最好？ 她说，哪都好呀。 我问她，你会一直这样玩下去么？ 她说，再看吧。 我问她，你出来这么久，走过这么地方，有没有什么深刻的感悟。 她说，哪有那么多感悟，我只是想多走走看看，我觉得这样挺有意思的。 就是这样平和的对话，她是我至今最欣赏的旅者。 哪有那么多真谛。 大音希声，大象无形。 心中安定的人，走到哪里都是春天。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[人为什么要旅行]]></title>
      <url>%2F2017%2F02%2F24%2Fwhy-do-people-travel%2F</url>
      <content type="text"><![CDATA[“有些人，一辈子缩在一个角落里， 连窗外都懒得看，更别说踏出门……” 你不出去走走，真的会以为这就是世界。 生命不是一场赛跑，而是一次旅行。比赛在乎终点，而旅行在乎沿途风景。常人说：不登山，不知山高；不涉水，不晓水深；不赏奇景，怎知其绝妙。 读万卷书，还须行万里路。旅行，可以使你中断每天周而复始的凡人琐事，对平凡俗气的生活，是一种暂时的解脱，让自己的胸怀得以舒展，心灵得以净化！ 早上，迎着新一轮的朝阳，伴着晨起的钟声，开始我一天的旅程；傍晚，看着火红的夕阳西下，微笑着收获着一天的美好；晚上，看着满天繁星点点，轻轻地对自己说晚安，明天，将会是一段全新的征程。 趁年轻，趁还有梦想，想去的地方，现在就要去。想做的事情，现在就去做。哪怕搭车、睡沙发、住客栈，享受在路上，看风景是不变的信念！ 人的一生需要考虑的太多太多，经历的也太多太多，得空往自己的心里书写一个坚定的“静”字，放下那些虚幻，真心实意的放松，这些年，这一生，哪有时间会留给你思考和休息？你的昨天，是多少人不曾经历过又奢求不来的；你的今天，又是多少人想尽一切方法都回不来的曾经啊。 你是否也和我一样？拥有一颗想要出走的心，却依然坐在一张不足一平的办公椅上？ 旅行不要再等，说走就走吧。 爱护自己的梦想，也要善待自己！！！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[安装impala]]></title>
      <url>%2F2017%2F02%2F23%2F%E5%AE%89%E8%A3%85impala%2F</url>
      <content type="text"><![CDATA[1.默认安装好hadoop并且能正常启动(只需hdfs即可)2.安装如下rpm包(需要root权限 注意顺序) bigtop-utils-0.7.0+cdh5.8.2+0-1.cdh5.8.2.p0.5.el6.noarch.rpmimpala-kudu-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmimpala-kudu-catalog-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmimpala-kudu-state-store-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmimpala-kudu-server-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmimpala-kudu-shell-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmimpala-kudu-udf-devel-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpm 安装命令如下: rpm -ivh ./bigtop-utils-0.7.0+cdh5.8.2+0-1.cdh5.8.2.p0.5.el6.noarch.rpmrpm -ivh ./impala-kudu-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpm –nodeps //需要取消依赖安装，不然安装不过rpm -ivh ./impala-kudu-catalog-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmrpm -ivh ./impala-kudu-state-store-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmrpm -ivh ./impala-kudu-server-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmrpm -ivh ./impala-kudu-shell-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpmrpm -ivh ./impala-kudu-udf-devel-2.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.11.el6.x86_64.rpm 其中catalog和state-store只能主节点一个(可安装于不同的主机) server和shell可以多台(可跟catalog和state-store不是同一台) 3.配置环境3.1.修改/etc/default/bigtop-utils文件1export JAVA_HOME=/usr/java/jdk1.8.0_65 //设置java home 3.2.修改/etc/default/impala文件123IMPALA_CATALOG_SERVICE_HOST=172.16.104.120 //为catalog主机Ip 也可以主机名 注意配置hostsIMPALA_STATE_STORE_HOST=172.16.104.120 //为state-store主机IpIMPALA_LOG_DIR=/var/log/impala //配置日志路径 默认为/var/log/impala 3.3.在/etc/impala/conf.dist目录下添加core-site.xml和hdfs-site.xml文件(建议从hadoop配置文件中拷贝)其中core-site.xml添加内容如下:12345678910111213&lt;!-- impala --&gt;&lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; hdfs-site.xml添加内容如下:12345678910111213&lt;!--impala--&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.block.local-path-access.user&lt;/name&gt; &lt;value&gt;impala&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.file-block-storage-locations.timeout.millis&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt;&lt;/property&gt; 4.启动服务123 service impala-catalog start service impala-state-store start service impala-server start 5.验证第一种方式:123ps -aux|grep impala-catalogps -aux|grep impala-stateps -aux|grep impalad 第二种方式:12impala-shell(默认连接本机的server)impala-shell -i 172.16.104.120 //连接指定ip的server impala-shell 如果是no connect状态 可以输入connect 172.16.104.120进行连接 第三种方式(webUI):123172.16.104.120:25000172.16.104.120:25010172.16.104.120:25020 6.其他Impala Daemon(Impala 守护进程前端端口):21000 &gt;&gt; impala-shell, Beeswax, Cloudera ODBC 1.2 驱动 用于传递命令和接收结果 Impala Daemon(Impala 守护进程前端端口):21050 &gt;&gt; 被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果 Impala Daemon(Impala 守护进程后端端口):22000 &gt;&gt; Impala 守护进程用该端口互相通讯 Impala Daemon(StateStore订阅服务端口):23000 &gt;&gt; Impala 守护进程监听该端口接收来源于 state store 的更新 StateStore Daemon(StateStore 服务端口):24000 &gt;&gt; State store 监听该端口的registration/unregistration 请求 Catalog Daemon(StateStore 服务端口):26000 &gt;&gt; 目录服务使用该端口与Imp Impala Daemon(HTTP 服务器端口):25000 &gt;&gt; Impala web 接口，管理员用于监控和故障排除 StateStore Daemon(HTTP 服务器端口):25010 &gt;&gt; StateStore web 接口，管理员用于监控和故障排除 Catalog Daemon(HTTP 服务器端口):25020 &gt;&gt; 目录服务 web 接口，管理员用于监控和故障排除，Impala 1.2 开始使用]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[安装kudu]]></title>
      <url>%2F2017%2F02%2F23%2F%E5%AE%89%E8%A3%85kudu%2F</url>
      <content type="text"><![CDATA[1.默认安装好yum2.需以root身份安装3.安装ntp yum install ntp -y4.启动ntp /etc/init.d/ntpd start|stop|restart5.添加安装包yum源123456[cloudera-kudu]# Packages for Cloudera&apos;s Distribution for kudu, Version 0, on RedHat or CentOS 6 x86_64name=Cloudera&apos;s Distribution for kudu, Version 0baseurl=http://archive.cloudera.com/beta/kudu/redhat/6/x86_64/kudu/0/gpgkey=http://archive.cloudera.com/beta/kudu/redhat/6/x86_64/kudu/RPM-GPG-KEY-clouderagpgcheck = 1 6.1安装kudu(yum 安装方式)yum install kudu kudu-master kudu-tserver kudu-client0 kudu-client-devel -y其中子节点可以不用安装kudu-master6.2安装kudu(rpm安装方式)12345rpm -ivh kudu-0.9.1+cdh5.4.0+0-1.kudu0.9.1.p0.32.el6.x86_64.rpmrpm -ivh kudu-master-0.9.1+cdh5.4.0+0-1.kudu0.9.1.p0.32.el6.x86_64.rpmrpm -ivh kudu-tserver-0.9.1+cdh5.4.0+0-1.kudu0.9.1.p0.32.el6.x86_64.rpmrpm -ivh kudu-client0-0.9.1+cdh5.4.0+0-1.kudu0.9.1.p0.32.el6.x86_64.rpmrpm -ivh kudu-client-devel-0.9.1+cdh5.4.0+0-1.kudu0.9.1.p0.32.el6.x86_64.rpm 7.配置参数(需要创建好相应目录)master:12345671./etc/default/kudu-master export FLAGS_log_dir=/opt/kudu-0.9.1/log //日志目录 export FLAGS_rpc_bind_addresses=dsj01:70512./etc/kudu/conf.dist/master.gflagfile --fs_wal_dir=/opt/kudu-0.9.1/data/master --fs_data_dirs=/opt/kudu-0.9.1/data/master --default_num_replicas=1 //设置备份数 不设置默认为3 tserver:12345671./etc/default/kudu-tserver export FLAGS_log_dir=/opt/kudu-0.9.1/log export FLAGS_rpc_bind_addresses=dsj02:70502./etc/kudu/conf.dist/tserver.gflagfile --fs_wal_dir=/opt/kudu-0.9.1/data/tserver --fs_data_dirs=/opt/kudu-0.9.1/data/tserver --tserver_master_addrs=dsj01:7051 //绑定master节点 8.启动kudu12service kudu-master start|stopservice kudu-tserver start|stop]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[flume+kafka+spark streaming整合]]></title>
      <url>%2F2017%2F02%2F22%2Fflume-kafka-spark-streaming%E6%95%B4%E5%90%88%2F</url>
      <content type="text"><![CDATA[1.安装好flume；2.安装好kafka；3.安装好spark；4.流程说明:日志文件-&gt;flume-&gt;kafka-&gt;spark streamingflume输入:文件flume输出:kafka的输入kafka输出:spark 输入5.整合步骤:(1).将插件jar拷贝到flume的lib目录下a. flumeng-kafka-plugin.jarb. metrics-annotation-2.2.0.jar (2).将配置文件producer.properties拷贝到flume的conf目录下配置文件内容如下:1234567891011121314151617181920212223242526272829#agentsectionproducer.sources=sproducer.channels=cproducer.sinks=r#sourcesectionproducer.sources.s.type=execproducer.sources.s.command=tail -f -n+1 /opt/apache-flume-1.6.0/data/testFlumeKafka.txtproducer.sources.s.channels=c# Eachsink&apos;s type must be definedproducer.sinks.r.type=org.apache.flume.plugins.KafkaSinkproducer.sinks.r.metadata.broker.list=namenode:19092,datanode1:19092,datanode2:19092producer.sinks.r.partition.key=0 producer.sinks.r.partitioner.class=org.apache.flume.plugins.SinglePartitionproducer.sinks.r.serializer.class=kafka.serializer.StringEncoderproducer.sinks.r.request.required.acks=0producer.sinks.r.max.message.size=1000000producer.sinks.r.producer.type=syncproducer.sinks.r.custom.encoding=UTF-8producer.sinks.r.custom.topic.name=test //需建好对应topic#Specifythe channel the sink should useproducer.sinks.r.channel=c# Eachchannel&apos;s type is defined.producer.channels.c.type=memoryproducer.channels.c.capacity=1000producer.channels.c.transactionCapacity=100 (3).启动flume-ngflume-ng agent -c . -f /opt/apache-flume-1.6.0/conf/producer.conf -n producer (4).启动kafka-serverbin/kafka-server-start.sh config/server.properties (5).启动kafka-consumer(默认已经创建了test topic)bin/kafka-console-consumer.sh --zookeeper namenode:12181,datanode1:12181,datanode2:12181 --topic test --from-beginning (6).启动sparksbin/start-all.sh (7).运行spark streaming Demorun-example org.apache.spark.examples.streaming.JavaKafkaWordCount namenode:12181 test-consumer-group test 3 &gt;&gt; test.log (8).在对应的日志文件中输入内容,则可以在test.log文件看到单词计数的结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[经验贴--kudu在小米的实践应用]]></title>
      <url>%2F2017%2F02%2F22%2F%E7%BB%8F%E9%AA%8C%E8%B4%B4-kudu%E5%9C%A8%E5%B0%8F%E7%B1%B3%E7%9A%84%E5%AE%9E%E8%B7%B5%E5%BA%94%E7%94%A8%2F</url>
      <content type="text"><![CDATA[1.前言本文是小米工程师常冰琳于2016年10月25日晚10点在“大数据产业联合会”微信群分享的内容，整理分享给大家。 2.小米使用kudu的背景小米大概在14年中开始和cloudera合作，作为kudu小白鼠用户，帮cloudera在生产环境验证kudu。kudu+Impala可以帮助我们解决实时数据的ad-hoc查询需求。在kudu之前，我们的大数据分析pipeline大概是有这几种： 数据源-&gt; scribe打日志到HDFS -&gt; MR/Hive/Spark -&gt; HDFS Parquet -&gt; Impala -&gt; 结果service这个数据流一般用来分析各种日志。 数据源 -&gt; 实时更新HBase/Mysql -&gt; 每天批量导出Parquet-&gt; Impala -&gt; 结果serve这个数据流一般用来分析状态数据，也就是一般需要随机更新的数据，比如用户profile之类。 这两条数据流主要由几个问题： 数据从生成到能够被高效查询的列存储，整个数据流延迟比较大，一般是小时级别到一天； 很多数据的日志到达时间和逻辑时间是不一致的，一般存在一些随机延迟。 比如很多mobile app统计应用，这些tracing event发生后，很可能过一段时间才被后端tracing server收集到。 我们经常看到一些hive查询，分析一天或者一小时的数据，但是要读2-3天或者多个小时的日志，然后过滤出实际想要的记录。 对于一些实时分析需求，有一些可以通过流处理来解决，不过他肯定没用SQL方便，另外流式处理只能做固定的数据分析，对ad-hoc查询无能为力 kudu的特点正好可以来配合impala搭建实时ad-hoc分析应用。 改进后的数据流大概是这个样子： 数据源 -&gt; kafka -&gt; ETL(Storm) -&gt; kudu -&gt; Impala 数据源 -&gt; kudu -&gt; Impala 数据流1 主要是为需要进一步做ETL的应用使用的，另外kafka可以当做一个buffer，当写吞吐有毛刺时，kafka可以做一个缓冲。 如果应用有严格的实时需求，就是只要数据源写入就必须能够查到，就需要使用数据流2。 3.引入kudu的目的引入kudu主要是用来替换 HDFS+parquet的。 4.kudu的列存和parquet列存有啥区别？从功能上说，kudu的列存除了提供跟parquet接近的scan速度，还支持随机读写。支持随机写，数据就可以实时灌入存储中，达到实时查询的效果；但是parquet文件只能批量写，所以一般只能定期生成，所以增大了延迟。kudu的存储类似hbase的lsm存储。 5.为什么说kudu的scan会比kylin快呢kylin是存储在hbase上的，kudu的scan为什么比hbase快，简单的说kudu是真正的列存储，hbase只是列簇存储。kudu是有schema的，每一列的数据是在文件中已数组的形式保存的，而hbase存储在hfile里面的还是sort好的(rowkey, column, timestamp, value)对，scan是开销要多很多，具体需要看kudu的paper了，在这里文字不好解释。 6.storm 写kudu的吞吐量能到多少，和storm写hbase比呢我们在71个节点的集群做了测试，随机写性能：随机写26亿条记录：每个节点大概4W 随机写性能。 大概的情况如下： 71 Node clusterHardwareCPU: E5-2620 2.1GHz 24 core Memory: 64GBNetwork: 1Gb Disk: 12 HDDSoftwareHadoop2.6/Impala 2.1/Kudu*3个大表，其中一个大表每天：~2.6 Billion rows~270 bytes/row17 columns, 5 key columns storm到kudu，按照每天26亿数据来算，每秒大概30000条记录吧。这个是我们的应用挑出的6个查询，做的查询性能对比。同样6个查询，查询parquet和查询kudu做的对比。当时kudu的设计目标是接近parquet的scan性能，惊喜的是，目前kudu的scan性能在生产环境下有时还比parquet快一些。 7.像hbase有coprocessor，kudu有类似的计算功能吗？kudud。kudu有predicatepushdown，目前有impala使用时，scan时是把一些过滤提交给kudu去做的。 8.你们是想用kudu替换hbase还是一起搭配用？感觉这两个工具目前用来解决不同的问题，hbase还是用来做OLTP类存储跟Mysql类似，kudu则用来升级我们现有的数据分析数据流，主要还是OLAP的workload。 9.Kudu支持随机增加列吗？只要不是primarykey的列，是可以随时增加的，而且不像mysql增加列时影响其他操作，kudu altertable是异步的，而且对性能影响不大。hbase是无schema的，所以可以成千上万个列，kudu不行的，列的数量也不能过多。我们目前也就试过30多列的，一些300+列的表还没有测试过。 10.Kudu目前有稳定版吗目前beta版本，不推荐现在在生产环境使用。(写此篇博客时(2017.2.22)已发布1.2.0版本) 11.能否介绍一下小米使用kudu过程中踩过的坑？目前踩的坑都还在开发阶段，其实都不算什么，而且从大方向上看，我们还是相信kudu这种方式对比之前的数据流优势很明显，对吞吐不是非常高的应用，这种方案是发展方向。其实我们在老的数据流上碰到很多问题，之前提到的数据延迟，数据无序，多个组件之间的兼容性，数据无schema导致灌入数据时缺少验证，其实都希望引入kudu后能够解决。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kudu学习资料总结]]></title>
      <url>%2F2017%2F02%2F22%2Fkudu%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[1.kudu是什么,为什么要用kudu？Kudu是Todd Lipcon@Cloudera带头开发的存储系统，其整体应用模式和Hbase比较接近，即支持行级别的随机读写，并支持批量顺序检索功能。 Hadoop 生态系统发展到现在，存储层主要由HDFS和HBase两个系统把持着，一直没有太大突破。在追求高吞吐的批处理场景下，我们选用HDFS，在追求低延迟，有随机读写需求的场景下，我们选用HBase，那么是否存在一种系统，能结合两个系统优点，同时支持高吞吐率和低延迟呢？有人尝试修改HBase内核构造这样的系统，即保留HBase的数据模型，而将其底层存储部分改为纯列式存储（目前HBase只能算是列簇式存储引擎），但这种修改难度较大。 Kudu的出现解决这一难题。 Kudu是Cloudera开源的列式存储引擎，具有以下几个特点优势： C++语言开发 高效处理类OLAP负载 与MapReduce，Spark以及Hadoop生态系统中其他组件进行友好集成 可与Cloudera Impala集成，替代目前Impala常用的HDFS+Parquet组合 强大而灵活的一致性模型，让你选择在每个请求的基础上一致性的要求，包括严格的序列化的一致性选择 顺序写和随机写并存的场景下，仍能达到良好的性能 高可用，使用Raft协议保证数据高可靠存储，只要确保只要一半以上的副本总数可用 结构化数据模型 易于管理，Cloudera管理 Kudu的出现，有望解决目前Hadoop生态系统难以解决的一大类问题，比如： 流式实时计算结果的更新 时间序列相关应用，具体要求有： 查询海量历史数据 查询个体数据，并要求快速返回 预测模型中，周期性更新模型，并根据历史数据快速做出决策 Kudu架构如下图所示： 2.kudu 的基本架构数据模型数据模型定义上，Kudu管理的是类似关系型数据库的结构化的表，表结构由类Sql的Schema进行定义，相比于HBase这样的NoSql类型的数据库，Kudu的行数据是由固定个数有明确类型定义的列组成，并且需要定义一个由一个或多个列组成的主键来对每行数据进行唯一索引，相比于传统的关系型数据库，kudu在索引上有更多的限制，比如暂时不支持二级索引，不支持主键的更新等等。 尽管表结构类似于关系型数据库，但是Kudu自身并不提供SQL类型的语法接口，而是由上层其他系统实现，比如目前通过Impala提供SQL语法支持。 Kudu底层API，主要面对简单的更新检索操作，Insert／Update／Delete等必须指定一个主键进行，而Scan检索类型的操作则支持条件过滤和投影等能力。 集群架构Kudu的集群架构基本和HBase类似，采用主从结构，Master节点管理元数据，Tablet节点负责分片管理数据。 和HBase不同的是，Kudu没有借助于HDFS存储实际数据，而是自己直接在本地磁盘上管理分片数据，包括数据的Replication机制，kudu的Tablet server直接管理Master分片和Slave分片，自己通过raft协议解决一致性问题等，多个Slave可以同时提供数据读取服务，相对于HBase依托HDFS进行Region数据的管理方式，自主性会强一些，不过比如Tablet节点崩溃，数据的迁移拷贝工作等，也需要Kudu自己完成。 存储结构因为数据是有严格Schema类型定义，所以Kudu底层可以使用列式存储的方案来提高存储和投影检索效率。 和HBase一样，Kudu也是通过Tablet的分区来支持水平扩展，与HBase不同的是，Kudu的分区策略除了支持按照Key Range来分区以外，还支持Hash based的策略，实际上，在主键上，Kudu可以混合使用这两种不同的策略。 Hash分区的策略在一些场合下可以更好的做到负载均衡，避免数据倾斜，但是它最大的问题就是分区数一旦确定就很难再调整，所以目前Kudu的分区数必须预先指定（对Range的分区策略也有这个要求，估计是先简单化统一处理），不支持动态分区分裂，合并等，因此表的分区一开始就需要根据负载和容量预先进行合理规划。 在处理随机写的效率问题方面，Kudu的基本流程和HBase的方案差不多，在内存中对每个Tablet分区维护一个MemRowSet来管理最新更新的数据，当尺寸超过一定大小后Flush到磁盘上形成DiskRowSet，多个DiskRowSet在适当的时候进行归并处理。 和HBase采用的LSM（LogStructured Merge）方案不同的是，Kudu对同一行的数据更新记录的合并工作，不是在查询的时候发生的（HBase会将多条更新记录先后Flush到不同的Storefile中，所以读取时需要扫描多个文件，比较rowkey，比较版本等），而是在更新的时候进行，在Kudu中一行数据只会存在于一个DiskRowSet中，避免读操作时的比较合并工作。那Kudu是怎么做到的呢？ 对于列式存储的数据文件，要原地变更一行数据是很困难的，所以在Kudu中，对于Flush到磁盘上的DiskRowSet（DRS）数据，实际上是分两种形式存在的，一种是Base的数据，按列式存储格式存在，一旦生成，就不再修改，另一种是Delta文件，存储Base数据中有变更的数据，一个Base文件可以对应多个Delta文件，这种方式意味着，插入数据时相比HBase，需要额外走一次检索流程来判定对应主键的数据是否已经存在。因此，Kudu是牺牲了写性能来换取读取性能的提升。既然存在Delta数据，也就意味着数据查询时需要同时检索Base文件和Delta文件，这看起来和HBase的方案似乎又走到一起去了，不同的地方在于，Kudu的Delta文件与Base文件不同，不是按Key排序的，而是按被更新的行在Base文件中的位移来检索的，号称这样做，在定位Delta内容的时候，不需要进行字符串比较工作，因此能大大加快定位速度。但是无论如何，Delta文件的存在对检索速度的影响巨大。因此Delta文件的数量会需要控制，需要及时的和Base数据进行合并。由于Base文件是列式存储的，所以Delta文件合并时，可以有选择性的进行，比如只把变化频繁的列进行合并，变化很少的列保留在Delta文件中暂不合并，这样做也能减少不必要的IO开销。 除了Delta文件合并，DRS自身也会需要合并，为了保障检索延迟的可预测性（这一点是HBase的痛点之一，比如分区发生Major Compaction时，读写性能会受到很大影响），Kudu的compaction策略和HBase相比，有很大不同，kudu的DRS数据文件的compaction，本质上不是为了减少文件数量，实际上Kudu DRS默认是以32MB为单位进行拆分的，DRS的compaction并不减少文件数量，而是对内容进行排序重组，减少不同DRS之间key的overlap，进而在检索的时候减少需要参与检索的DRS的数量。以32MB这样小的单位进行拆分，也是为了能够以有限的资源快速的完成compaction的任务，及时根据系统负载调整Compaction行为，而不至于像HBase一样，Major Compaction动作成为导致性能不稳定的一个重要因素。所以对于Kudu来说，IO操作可以是一个持续平缓的过程，这点对响应的可预测性至关重要。 总结总体来说，个人感觉，Kudu本质上是将性能的优化，寄托在以列式存储为核心的基础上，希望通过提高存储效率，加快字段投影过滤效率，降低查询时CPU开销等来提升性能。而其他绝大多数设计，都是为了解决在列式存储的基础上支持随机读写这样一个目的而存在的。比如类Sql的元数据结构，是提高列式存储效率的一个辅助手段，唯一主键的设定也是配合列式存储引入的定制策略，至于其他如Delta存储，compaction策略等都是在这个设定下为了支持随机读写，降低latency不确定性等引入的一些Tradeoff方案 官方测试结果上，如果是存粹的随机读写，或者单行的检索请求这类场景，由于这些Tradeoff的存在，HBASE的性能吞吐率是要优于Kudu不少的（2倍到4倍），kudu的优势还是在支持类SQL检索这样经常需要进行投影操作的批量顺序检索分析场合。 目前kudu还处在Incubator阶段，并且还没有成熟的线上应用（小米走在了前面，做了一些业务应用的尝试），在数据安全，备份，系统健壮性等方面也还要打个问号，所以是否使用kudu，什么场合，什么时间点使用，是个需要好好考量的问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[三识SpringBoot微框架--启动器Starter详解]]></title>
      <url>%2F2017%2F02%2F22%2F%E4%B8%89%E8%AF%86SpringBoot%E5%BE%AE%E6%A1%86%E6%9E%B6-%E5%90%AF%E5%8A%A8%E5%99%A8Starter%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[spring boot 配置时需要在pom文件中添加starter模块，那么starter是什么？有什么作用？分为哪几种？下面将进行详细讲解： starter模块，简单的说，就是一系列的依赖包组合。例如web starter模块，就是包含了Spring Boot预定义的一些Web开发的常用依赖，如： spring-web, spring-webmvc 即Spring WebMvc框架 tomcat-embed-* 即内嵌Tomcat容器 jackson 即处理json数据 spring-* 即Spring框架 spring-boot-autoconfigure 即Spring Boot提供的自动配置功能 换句话说，当你添加了相应的starter模块，就相当于添加了相应的所有必须的依赖包。注：可以在引入对应的starter启动器后使用 mvn dependency:tree 查看引入的依赖关系。如下图所示： spring Boot应用启动器基本的一共有44种，具体如下：1）spring-boot-starter这是Spring Boot的核心启动器，包含了自动配置、日志和YAML。 2）spring-boot-starter-actuator帮助监控和管理应用。 3）spring-boot-starter-amqp通过spring-rabbit来支持AMQP协议（Advanced Message Queuing Protocol）。 4）spring-boot-starter-aop支持面向切面的编程即AOP，包括spring-aop和AspectJ。 5）spring-boot-starter-artemis通过Apache Artemis支持JMS的API（Java Message Service API）。 6）spring-boot-starter-batch支持Spring Batch，包括HSQLDB数据库。 7）spring-boot-starter-cache支持Spring的Cache抽象。 8）spring-boot-starter-cloud-connectors支持Spring Cloud Connectors，简化了在像Cloud Foundry或Heroku这样的云平台上连接服务。 9）spring-boot-starter-data-elasticsearch支持ElasticSearch搜索和分析引擎，包括spring-data-elasticsearch。 10）spring-boot-starter-data-gemfire支持GemFire分布式数据存储，包括spring-data-gemfire。 11）spring-boot-starter-data-jpa支持JPA（Java Persistence API），包括spring-data-jpa、spring-orm、hibernate。 12）spring-boot-starter-data-MongoDB支持MongoDB数据，包括spring-data-mongodb。 13）spring-boot-starter-data-rest通过spring-data-rest-webmvc，支持通过REST暴露Spring Data数据仓库。 14）spring-boot-starter-data-solr支持Apache Solr搜索平台，包括spring-data-solr。 15）spring-boot-starter-freemarker支持FreeMarker模板引擎。 16）spring-boot-starter-groovy-templates支持Groovy模板引擎。 17）spring-boot-starter-hateoas通过spring-hateoas支持基于HATEOAS的RESTful Web服务。 18）spring-boot-starter-hornetq通过HornetQ支持JMS。 19）spring-boot-starter-integration支持通用的spring-integration模块。 20）spring-boot-starter-jdbc支持JDBC数据库。 21）spring-boot-starter-jersey支持Jersey RESTful Web服务框架。 22）spring-boot-starter-jta-atomikos通过Atomikos支持JTA分布式事务处理。 23）spring-boot-starter-jta-bitronix通过Bitronix支持JTA分布式事务处理。 24）spring-boot-starter-mail支持javax.mail模块。 25）spring-boot-starter-mobile支持spring-mobile。 26）spring-boot-starter-mustache支持Mustache模板引擎。 27）spring-boot-starter-Redis支持Redis键值存储数据库，包括spring-redis。 28）spring-boot-starter-security支持spring-security。 29）spring-boot-starter-social-facebook支持spring-social-facebook 30）spring-boot-starter-social-linkedin支持pring-social-linkedin 31）spring-boot-starter-social-twitter支持pring-social-twitter 32）spring-boot-starter-test支持常规的测试依赖，包括JUnit、Hamcrest、Mockito以及spring-test模块。 33）spring-boot-starter-thymeleaf支持Thymeleaf模板引擎，包括与Spring的集成。 34）spring-boot-starter-velocity支持Velocity模板引擎。 35）spring-boot-starter-web支持全栈式Web开发，包括Tomcat和spring-webmvc。 36）spring-boot-starter-websocket支持WebSocket开发。 37）spring-boot-starter-ws支持Spring Web Services。 Spring Boot应用启动器面向生产环境的还有2种，具体如下： 1）spring-boot-starter-actuator增加了面向产品上线相关的功能，比如测量和监控。 2）spring-boot-starter-remote-shell增加了远程ssh shell的支持。 最后，Spring Boot应用启动器还有一些替换技术的启动器，具体如下： 1）spring-boot-starter-jetty引入了Jetty HTTP引擎（用于替换Tomcat）。 2）spring-boot-starter-log4j支持Log4J日志框架。 3）spring-boot-starter-logging 引入了Spring Boot默认的日志框架Logback。 4）spring-boot-starter-tomcat引入了Spring Boot默认的HTTP引擎Tomcat。 5）spring-boot-starter-undertow引入了Undertow HTTP引擎（用于替换Tomcat）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[本地新项目同步到github的详细步骤]]></title>
      <url>%2F2017%2F02%2F21%2F%E6%9C%AC%E5%9C%B0%E6%96%B0%E9%A1%B9%E7%9B%AE%E5%90%8C%E6%AD%A5%E5%88%B0github%E7%9A%84%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
      <content type="text"><![CDATA[说明：本地项目分为两种情况： 自己本地创建的项目，从未与github产生关联； 从github上clone到本地的； 若是第1种情况，直接按下面的步骤操作即可；若是第2种情况，又分两种情况： 若想要保留之前的commit记录，可直接修改项目根目录下的.git/config 文件，将下图中的url改为自己的github仓库地址即可，然后直接 add、commit、push 即可； 若不想保留之前的commit记录，首先要把本地项目根目录下的 .git 文件夹删掉，然后再按下面的步骤操作。 1.初始化项目1git init 初始化之后会生成新的 .git 文件 2.将代码add到本地库中12git add .git commit -m &quot;初始化项目&quot; 3.创建github上的仓库，并与本地项目关联起来123git remote rm origingit remote add origin https://github.com/lzrlizhirong/cmdb.gitgit push -u origin master 然后在github上就可以看到同步上去的代码了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一步一步教你如何解锁iPhone]]></title>
      <url>%2F2017%2F02%2F21%2F%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E8%A7%A3%E9%94%81iPhone%2F</url>
      <content type="text"><![CDATA[即使你的iPhone 6s设置了六位数的密码，甚至还设置了touch ID，但我要告诉你的是：你的手机仍然能被犯罪分子解锁。 事件背景几天前，一位苹果用户的iPhone6S被偷了。随后，小偷重置了该用户某些在线服务密码以及Apple ID。 不仅如此，小偷还伪装成该用户与银行进行了联系，并试图重置该用户的银行账户密码。不过幸运的是，这个小偷并没有取出这些钱。 那么问题来了，犯罪分子是如何在手机锁屏的情况下重置AppleID密码的呢？ 为了让大家更清楚地了解此次事件，我们收集整理了一些关于此次事件的信息，具体如下： a）这是不是一次针对性的攻击？我的意思是，小偷是不是通过网络钓鱼等诈骗手段盗取了受害者的信息，然后再专门偷走他的手机？ 这不太可能。根据我们收集到的信息，小偷在偷走受害者的手机之前没干任何事。 b）在手机被盗之前，受害者其它的网络账户或个人信息被盗了吗？对于小偷来说，受害者的个人信息可是很重要的呢！ 然而，受害者的个人信息并没有被盗，小偷偷手机纯粹只是求财。 c）小偷在盗得手机多久之后就解锁了iPhone和SIM卡？ 大概在手机被盗2个小时左右。 d）iPhone的密码能猜出来吗？ 不太现实。六位数字密码并不是那么好猜的，而且受害者设置的密码与他的车牌号码或其它个人信息毫无关联。 鉴于此次事件是如此的“不可思议”，我们决定对此进行深入分析，并让大家了解这部iPhone到底是如何被解锁的。 事件脉络该用户的iPhone6S是在10月14日下午被偷的，我们对此次事件的发展脉络进行了梳理，具体如下： a)14:00-手机被盗； b)16:03-受害者激活了手机的“丢失模式”，并通过iCloud远程清除了手机数据； c)16:28-受害者的Google账户密码被修改了； d)16:37-受害者收到了一封电子邮件，邮件中包含了重置Apple ID密码的链接； e)16:38-受害者收到了一封新邮件，这封邮件通知他，他的Apple ID密码已被修改； f)16:43-受害者再次收到邮件，被告知他手机已成功定位； g)16:43-受害者收到了最后一封邮件，被告知这台手机中的数据已被清除； 正如我们所见，受害者的Google和Apple账户的密码都被小偷重置了。正如我们所知，在没有手机密码的情况下，要解锁这台iPhone是不太可能的。那么，小偷是如何做到的呢？ 以下是我们所做的一些假设1）如果你要更改Google账户的密码，首先你得要知道电子邮箱的地址。然而，犯罪分子是如何获得受害者的邮箱地址的呢？尽管手机在锁屏状态下收到的信息和通知会显示在手机屏幕上，但用户的Gmail邮箱地址并没有办法显示出来。 2）可以通过设备的IMEI码获取用户的Apple ID吗？我们在网上搜索了一下，确实发现了有些付费服务能够通过IMEI码获取Apple ID，但是得需要24-48小时才能获得你想要的信息。而犯罪分子只用了2个小时就将手机解锁了，由此可见，犯罪分子并不是通过设备的IMEI码获取到受害者的AppleID的。 3）犯罪分子仅根据手机号就能获取用户的Gmail邮箱账号吗？我们发现，只要有以下几个信息就能获取Gmail邮箱地址——与邮箱账号绑定的手机号码和用户的姓名。 既然手机偷到手了，手机号码自然也就知道了，通过手机号码获得用户的姓名也不是什么难事。因此，我们准备以此为切入点继续深入下去。 情景还原我们决定根据上述方法找到用户姓名以验证我们的猜想。受害者为了搞清楚事情的来龙去脉也参与其中，还购买了一部新的iPhone6S，并且将新手机的设置调整成被盗手机一样。这样一来，受害者手机被盗的场景就能被最大化地还原了。 获取关键信息为了获取手机号码，我们取出了iPhone中的SIM卡，然后把这张SIM卡插到了另一台手机中。与真实场景一样，SIM卡没有设置PIN码。所以，我们轻易地在另一台手机上获取了用户的手机号码。接下来，我们将用户的号码放到网上搜索，试图获取用户的姓名，但是这个方法行不通。一定还有根据手机号码就能获取用户姓名的办法，于是我就想到了WhatsApp！ 假如你在WhatsApp的一个群聊组中，并收到了陌生人的信息时，对方的名字和手机号就会显示在资料中（例如：9999-9999 ~MikeArnold）。所以，如果能用这个锁屏的iPhone向WhatsApp的聊天群发送一条信息，我们就能知道用户的姓名了。 首先，我们要确保在锁屏状态下，这台iPhone接收到的WhatsApp通知信息能在锁屏界面上显示。 于是我们向这台iPhone发了一条信息，这条信息果然显示在锁屏界面上了；下一步，我们需要在手机的锁屏状态下回复这条信息，只要使用3D touch功能就能实现这一步。 于是我们创建了一个聊天群，并把受害者手机号所绑定的WhatsApp账号加入到这个群里，由于进入新群不需要任何验证信息，所以我们便在锁屏界面上看到了这条进群的通知信息。此外，我们还在群里加了一些与受害者毫无关联的陌生人。 一切准备就绪，我们先在群里发了一条信息，这条信息也在锁屏界面上显示了；然后我们用3Dtouch功能回复了这条信息，果然不出所料，我们成功获取到了用户的姓名。下一步，只要将用户姓名和电话填到Google的表单中，我们就能获得用户的电子邮箱地址了。 修改Google账户的密码现在，我们来试着还原犯罪分子修改Google账户密码的场景。 进入Google的登录界面； 选择“忘记密码”选项； 在“你曾经使用过的密码”选项中随便填写一些数字或字母； 接下来，Google会让你填写与账户绑定的手机号码； 输入了手机号之后，Google会给绑定的手机发送一条验证码短信； 输入了验证码之后，Google会要求我们设置新密码。 由此看来，只要别人拿到了你的手机或SIM卡以及你的姓名，他就能轻松地修改你的Google账户密码了。 修改Apple ID的密码下一步就是修改Apple ID密码了。与修改Google账户密码一样，进入登录界面后选择“忘记密码”选项，然后系统会把重置密码的链接通过邮件发到你的Gmail邮箱中。剩下的操作就简单多了，我们成功地修改了用户的Apple ID密码。 解锁新的iPhone在iPhone手机被盗之后，大家第一时间想到的就是远程锁定手机并清除手机中的数据。但是，这几个步骤反而会帮助犯罪分子得到一台“新的”iPhone。 原因是，当iPhone的数据被远程清除后，iPhone会要求你输入与设备绑定的Apple ID和密码，但假如犯罪分子用我们上面所述的办法获取到了AppleID和密码的话，那么犯罪分子就能将这台iPhone当成新手机来使用了。 温馨提示以下是我们针对本文中涉及到的安全问题所提出来的建议： 1、禁止手机在锁屏状态下显示短信或其它通知的内容；2、为手机的SIM卡设置PIN码；3、为你所使用的各种网络服务设置双因素身份验证。 总结这次的事件值得我们深思，如果我们不为自己的智能手机做好足够的安全保护措施，那么我们损失的可不仅仅只是手机了。 来自：FreeBuf.COM链接：http://www.freebuf.com/news/117369.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过edu.cn邮箱申请免费试用jetbrains全套产品]]></title>
      <url>%2F2017%2F02%2F20%2F%E9%80%9A%E8%BF%87edu-cn%E9%82%AE%E7%AE%B1%E7%94%B3%E8%AF%B7%E5%85%8D%E8%B4%B9%E8%AF%95%E7%94%A8jetbrains%E5%85%A8%E5%A5%97%E4%BA%A7%E5%93%81%2F</url>
      <content type="text"><![CDATA[jetbrains在IDE界鼎鼎大名，产品有：IntelliJ IDEA、WebStorm、PhpStorm等等,如下图所示：Intellij的正版授权，还是比较贵的，大概人民币一千多。后来听说可以通过教育邮箱申请免费试用，步骤如下： 首先，访问Jetbrains students license，然后点击立即申请，按照邮件的提示即可完成。最后用你的账户登录IDE即可完成激活。从此以后，你就可以使用永久免费试用最新版的Jetbrains的所有软件了。 当然，当你经济条件允许的时候，我还是建议你买一波正版，支持一下~都是同行，莫坑队友。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[再识SpringBoot微框架--简单案例]]></title>
      <url>%2F2017%2F02%2F20%2F%E5%86%8D%E8%AF%86SpringBoot%E5%BE%AE%E6%A1%86%E6%9E%B6-%E6%90%AD%E5%BB%BA%E6%A1%88%E4%BE%8B%2F</url>
      <content type="text"><![CDATA[目的：让所有Spring开发变得更快，且让更多的人更快的进行Spring入门体验，提供“starter” POM来简化我们的Maven配置（也就是说使用Spring Boot只有配合maven/gradle等这种依赖管理工具才能发挥它的能力），不像以前，构建一个springmvc项目需要进行好多配置等。 提供一些非功能性的常见的大型项目类特性（如内嵌服务器、安全、度量、健康检查、外部化配置），如可以直接地内嵌Tomcat/Jetty（不需要单独去部署war包） 绝无代码生成，且无需XML配置 创建项目首先使用Maven创建一个普通Maven应用即可，不必是web的。 添加Spring Boot相关POM配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.com&lt;/groupId&gt; &lt;artifactId&gt;testboot&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;spring-boot-demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Camel BOM --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-parent&lt;/artifactId&gt; &lt;version&gt;2.18.2&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- logging --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- testing --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;defaultGoal&gt;install&lt;/defaultGoal&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- Allows the example to be run via &apos;mvn compile exec:java&apos; --&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.0&lt;/version&gt; &lt;configuration&gt; &lt;mainClass&gt;cn.com.MainApp&lt;/mainClass&gt; &lt;includePluginDependencies&gt;false&lt;/includePluginDependencies&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 继承spring-boot-starter-parent后我们可以继承一些默认的依赖，这样就无需添加一堆相应的依赖，把依赖配置最小化；spring-boot-starter-web提供了对web的支持，spring-boot-maven-plugin提供了直接运行项目的插件，我们可以直接mvn spring-boot:run运行。 实体123456789101112131415161718192021222324252627282930313233package cn.com.testboot.entity;/** * Created by lizhirong on 2017/2/20. */public class User &#123; private Long id; private String name; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; User user = (User) o; if (id != null ? !id.equals(user.id) : user.id != null) return false; return true; &#125; @Override public int hashCode() &#123; return id != null ? id.hashCode() : 0; &#125;&#125; 控制器12345678910111213141516171819202122232425package cn.com.testboot.controller;import cn.com.testboot.entity.User;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * Created by root on 2017/2/20. *///@EnableAutoConfiguration@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @RequestMapping(&quot;/&#123;id&#125;&quot;) public User view(@PathVariable(&quot;id&quot;) Long id) &#123; User user = new User(); user.setId(id); user.setName(&quot;zhang&quot;); return user; &#125; //public static void main(String[] args) &#123; // SpringApplication.run(UserController.class); //&#125;&#125; 运行 第一种方式通过在UserController中加上@EnableAutoConfiguration开启自动配置，然后通过SpringApplication.run(UserController.class);运行这个控制器；这种方式只运行一个控制器比较方便； 第二种方式通过@Configuration+@ComponentScan开启注解扫描并自动注册相应的注解Bean 12345678910111213141516171819package cn.com.testboot;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;/** * Created by root on 2017/2/20. */@Configuration@ComponentScan@EnableAutoConfigurationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class); &#125;&#125; 到此，一个基本的REST风格的web应用就构建完成了。地址栏输入 http://localhost:8080/user/1 即可看到json结果。 如果大家查看其依赖，会发现自动添加了需要相应的依赖（不管你用or不用），但是开发一个应用确实变得非常快速，对于想学习or体验Spring的新手，快速建立项目模型等可以考虑用这种方式。当然如果不想依赖这么多的jar包，可以去掉parent，然后自己添加依赖。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[初试SpringBoot微框架]]></title>
      <url>%2F2017%2F02%2F20%2FSpringBoot%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[1. Spring Boot是什么，解决哪些问题 1) Spring Boot使编码变简单 2) Spring Boot使配置变简单 3) Spring Boot使部署变简单 4) Spring Boot使监控变简单 2. Spring Boot在平台中的定位，相关技术如何融合 1) SpringBoot与SEDA +MicroService + RESTful 2) SpringBoot与Mock 3. 采用了SpringBoot之后，技术管理应该如何进行首先，我们来看一下spring boot是什么，它帮助我们解决了哪些问题：SpringBoot是伴随着Spring4.0诞生的； 从字面理解，Boot是引导的意思，因此SpringBoot帮助开发者快速搭建Spring框架； SpringBoot帮助开发者快速启动一个Web容器； SpringBoot继承了原有Spring框架的优秀基因； SpringBoot简化了使用Spring的过程。 Spring由于其繁琐的配置，一度被人认为“配置地狱”，各种XML、Annotation配置，让人眼花缭乱，而且如果出错了也很难找出原因。 Spring Boot更多的是采用Java Config的方式，对Spring进行配置。 可以看到，采用了spring-boot-start-actuator之后，直接以REST的方式，获取进程的运行期性能参数。 当然这些metrics有些是有敏感数据的，spring-boot-start-actuator为此提供了一些Basic Authentication认证的方案，这些方案在实际应用过程中也是不足的。 Spring Boot作为一个微框架，离微服务的实现还是有距离的。 没有提供相应的服务发现和注册的配套功能，自身的acturator所提供的监控功能，也需要与现有的监控对接。没有配套的安全管控方案，对于REST的落地，还需要自行结合实际进行URI的规范化工作。 下面，我们研究一下Spring Boot在平台中的定位，相关技术如何融合。 上图比较复杂，整体是采用SEDA，也就是Stage-EDA。可以看到，整体是以处理顺序进行展示的，响应过程类似。在处理过程中，主要会有前置过滤，核心功能处理，后置过滤几大部分。 图中的过滤器都是可插拔式的，并且可以根据实际场景进行扩展开发。每个过滤器都是Stage，比如ClientInstance合法性检查、调用鉴权、解密、限流等等。 一个请求Stage与Stage的转换，实现上是切换不同的线程池，并以EDA的方式驱动。 对于业务逻辑的开发者而言，只需要关心CORE部分的业务逻辑实现，其他的非功能都由框架进行统一实现。 Mock不应当再是测试的专有名词了，当然对于测试这个角色而言，mockito这样的工具，依然可以为他们提升不少效率。 SpringBoot为创建REST服务提供了简便的途径，相比之下，采用阿里的dubbo在做多团队、多进程联调时，mock的难度就陡增。 Mock是解耦并行开发的利器，在理性的情况下，软件从开发期Mock联调，到开发与开发的真实联调，只需要切换一个依赖的域名即可，比如：mockURI:http://mock.service.net/v1/function?param1=value1devURI:http://dev.service.net/v1/function?param1=value1 而上述的域名切换，只需要在开发期定义好一个配置项，在做环境切换的时候自动注入即可，省时、省心、省力。 如上图和docker的集成可以有AB两种方案： A方案的核心是，把docker作为操作系统环境的交付基线，也就是不同的fat jar 使用相同的操作系统版本、相同的JVM环境。但对于docker image来说都是一样的。 B方案的核心是，不同的fat jar，独立的编译为docker image，在启动时直接启动带有特定版本的image。 A相比与B方案的特点是对于docker registry（也就是docker的镜像仓库）的依赖性较低，对于前期编译过程的要求也较低。 采用了Spring Boot之后，技术管理应该如何进行？正因为Spring Boot是与Spring一脉相承的，所以对于广大的Java开发者而言，对于Spring的学习成本几乎为零。 在实践Spring Boot时学习重点，或者说思维方式改变的重点在于： 对于REST的理解，这一点尤为重要，需要从设计、开发多个角色达成共识，很多时候都是对于HTTP 1.1协议以及REST的精髓不理解，导致REST被「盲用」而产生一些不好的效果。 对于YAML的理解和对于JavaConfig的理解，这两点相对较为简单，本质上是简化了xml文件，并提供等价的配置表述能力。 丰富的工具链为SpringBoot的推广带来了利好。 SpringBoot的工具链主要来自于两个方面：1) 原有Spring积累的工具链；2) SpringMVC或者其他REST框架使用HTTP协议，使得HTTP丰富的工具成为SpringBoot天然的资源。 SpringBoot自身对于前面提到的配置文件：“application.yml”提供了多个「Profile」，可以便于开发者描述不同环境的配置，这些配置例如数据库的连接地址、用户名和密码。 但是对于企业用户而言，把不同环境的配置，写到同一个配置文件中，是极其不安全的，是一个非常危险的动作。 有一个经常被提及的例子是，随着开源的进行，很多互联网公司，都由于把相关的代码提交到github之类的开源代码社区，并且没有对代码进行严格的配置审查，导致一些”password”被公开。有些不良用心的人，就利用搜索工具，专门去挖掘这些关键字，进而导致数据库被「拖库」。 所以对于企业用户，更多的应该是采用集中式的配置管理系统，将不同环境的配置严格区分地存放。 虽然SpringBoot的actuator自身提供了基于「用户名+口令」的最简单的认证方式，但它保护的是对框架自身运行期的性能指标敏感数据的最基本的保护。这种保护在实际应用过程中，「用户名+口令」的管理是缺乏的，「用户名+口令」的安全配置过程是缺失的。 SpringBoot也不提供对于我们自己开发的功能的任何防护功能。 一般来讲，一个安全的信道（信息传输的通道），需要通信双方在进行正式的信息传输之前对对方进行身份认证，服务提供方还需要在此基础之上，对请求方的请求进行权限的校验，以确保业务安全。这些内容也需要基于SpringBoot进行外围的安全扩展，例如采用前面提到的S-EDA进行进程级别的安全管控。这些还需要配套的安全服务提供支持。 一般来说，只要企业与互联网对接，那么随便一个面向消费者的「市场活动」，就有可能为企业带来井喷的流量。 传统企业内，更多的系统是管理信息类的支撑系统，这类系统在设计时的主要用户是企业内部员工以及有限的外部供应商。这类系统存在于企业内部的时间一直很长，功能耦合也很多，在功能解耦前，是非常不适合的，或者说绝对不可以直接为互联网的用户进行服务的。 SpringBoot自身并没有提供这样的流控措施，所以需要结合前面提到的S-EDA进行流量的控制，并结合下层的水平扩展能力（例如，Kubernets）进行流量负载合理的动态扩容。 另外，在长业务流程的设计上，也尽可能地采用异步的方式，比如接口调用返回的是一个「受理号」，而不是业务的处理结果，避免井喷业务到来时，同步调用所带来的阻塞导致系统迅速崩溃，这些也都是SpringBoot自身并不解决的问题。 以上是我分享的主要内容，下面我们总结一下：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Apache Beam：一个开源的统一的分布式数据处理编程库]]></title>
      <url>%2F2017%2F02%2F16%2FApache-Beam%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E7%9A%84%E7%BB%9F%E4%B8%80%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%BC%96%E7%A8%8B%E5%BA%93%2F</url>
      <content type="text"><![CDATA[Apache Beam是一个开源的数据处理编程库，由Google贡献给Apache的项目，前不久刚刚成为Apache TLP项目。它提供了一个高级的、统一的编程模型，允许我们通过构建Pipeline的方式实现批量、流数据处理，并且构建好的Pipeline能够运行在底层不同的执行引擎上。刚刚接触该开源项目时，我的第一感觉就是：在编程API的设计上，数据集及其操作的抽象有点类似Apache Crunch（MapReduce Pipeline编程库）项目；而在支持统一数据处理模型上，能够让人想到Apache Flink项目。如果深入了解Apache Beam，你会发现未来Apache Beam很可能成为数据处理领域唯一一个能够将不同的数据应用统一起来的编程库。 Apache Beam架构概览Apache Beam目前最新版本为0.5.0-SNAPSHOT，最新的Release版本为0.4.0，很多特性还在开发中。在网上找到一个由Andrew Psaltis在2016年6月份演讲的《Apache Beam: The Case for Unifying Streaming API’s》，引用了其中一个Apache Beam的架构图，如下图所示：上图中，我们可以看到，Apache Beam核心的主要有两层： 1. Pipeline构建层 在Pipeline构建层，针对不同的编程语言，构建一组用于定义Pipeline相关抽象，提供编程API，这一层被称为Beam SDKs。最终的用户（具有不同编程语言技能的人员）可以基于这些抽象的Beam SDK来构建数据处理Pipeline。 2. Runner适配层 Runner适配层，主要是用来对接底层的计算引擎，用来执行上层用户开发好的Pipeline程序。 我们先根据官网文档，了解一下Apache Beam的Roadmap。首先，下面的三个特性，或者说是Apache Beam的目标： 1. 统一（UNIFIED） 基于单一的编程模型，能够实现批处理（Batch processing）、流处理（Streaming Processing），通常的做法是把待处理的数据集（Dataset）统一，一般会把有界（Bound）数据集作为无界（Unbound）数据集的一种特殊情况来看待，比如Apache Flink便是按照这种方式处理，在差异化的API层之上构建一个统一的API层。 2. 可移植（PORTABLE） 在多个不同的计算环境下，都能够执行已经定义好的数据处理Pipeline。也就是说，对数据集处理的定义（即构建的Data Pipeline），与最终所要Deploy的执行环境完全无关。这对实现数据处理的企业是非常友好的，当下数据处理新技术不断涌现，企业数据处理平台也为了能够与时俱进并提高处理效率，当然希望在底层计算平台升级的过程中无需重写上层已定义的Data Pipeline。目前，Apache Beam项目开发整体来看还处在初期，初步决定底层执行环境支持主流的计算平台：Apache Apex、Apache Flink、Apache Spark、Google Cloud Dataflow。实际上，Apache Beam的这种统一编程模型，可以支持任意的计算引擎，通过Data Pipeline层与执行引擎层之间开发一个类似Driver的连接器即可实现。 3. 可扩展（EXTENSIBLE） 实现任意可以共享的Beam SDK、IO connector、Transform库。 基本概念在使用Apache Beam构建数据处理程序，首先需要使用Beam SDK中的类创建一个Driver程序，在Driver程序中创建一个满足我们数据处理需求的Pipeline，Pipeline中包括输入（Inputs）、转换（Transformations）、输出（Outputs）三个核心的组件。然后，根据我们选择的Beam SDK来确定底层使用Pipeline Runner（执行引擎，或计算引擎），将我们定义好的Pipeline运行在Pipeline Runner上。Apache Beam SDKs提供一组抽象，用来简化大规模分布式数据处理。同一个Beam抽象，能够同时适应批量处理、流处理两种数据源。下面，我们了解一下Apache Beam的一些关键抽象： 1. Pipeline一个Pipeline是对一个数据处理任务抽象，它包含了我们在对给定数据集处理的全部逻辑，主要包括从数据源读取数据（可能从多个数据源读取）、在给定的数据集上执行Transform操作（中间可能是一个DAG图，通过多个Transform连接，而Transform的输出和输出都可能是一个数据集）、将Transform的数据结果写入到指定对的存储系统中。 2. PCollection一个PCollection是对分布式数据集的抽象，他可以是输入数据集、中间结果数据集、输出数据集。每一个由PCollection表征的数据集作为输入时，都会存在一个或多个Transform作用在其上（对数据集进行处理的逻辑）。 3. Transform一个Transform表示数据处理过程中一个步骤（Step），对应于Pipeline中一个操作，每一个Transform会以一个或多个PCollection作为输入，经过处理后输出一个或多个PCollection。 4. Source and SinkApache Beam提供了Source和Sink的API，用来表示读取和写入数据。Source表示从一个外部的数据源读入数据到Pipeline，而Sink表示经过Pipeline处理后将数据写入到外部存储系统 5. PipelineRunnerPipelineRunner是实际用来处理Pipeline逻辑的底层组件，它能够将用户构建的Pipeline翻译成底层计算引擎能够处理的Job，并执行Pipeline的处理逻辑。 API设计Apache Beam还在开发之中，后续对应的API设计可能会有所变化，不过从当前版本来看，基于对数据处理领域对象的抽象，API的设计风格大量使用泛型来定义，具有很高的抽象级别。下面我们分别对感兴趣的的设计来详细说明。 1. SourceSource表示数据输入的抽象，在API定义上分成两大类：一类是面向数据批处理的，称为BoundedSource，它能够从输入的数据集读取有限的数据记录，知道数据具有有限性的特点，从而能够对输入数据进行切分，分成一定大小的分片，进而实现数据的并行处理；另一类是面向数据流处理的，称为UnboundedSource，它所表示的数据是连续不断地进行输入，从而能够实现支持流式数据所特有的一些操作，如Checkpointing、Watermarks等。Source对应的类设计，如下类图所示：目前，Apache Beam支持BoundedSource的数据源主要有：HDFS、MongoDB、Elasticsearch、File等，支持UnboundedSource的数据源主要有：Kinesis、Pubsub、Socker等。未来，任何具有Bounded或Unbounded两类特性的数据源都可以在Apache Beam的抽象基础上实现对应的Source。 2. SinkSink表示任何经过Pipeline中一个或多个PTransform处理过的PCollection，最终会输出到特定的存储中。与Source对应，其实Sink主要也是具有两种类型：一种是直接写入特定存储的Bounded类型，如文件系统；另一种是写入具有Unbounded特性的存储或系统中，如Flink。在API设计上，Sink的类图如下所示：可见，基于Sink的抽象，可以实现任意可以写入的存储系统。 3. PipelineRunner下面，我们来看一下PipelineRunner的类设计以及目前开发中的PipelineRunner，如下图所示：目前，PipelineRunner有DirectRunner、DataflowRunner、SparkRunner、ApexRunner、FlinkRunner，待这些主流的PipelineRunner稳定以后，如果有其他新的计算引擎框架出现，可以在PipelineRunner这一层进行扩展实现。这些PipelineRunner中，DirectRunner是最简单的PipelineRunner，它非常有用，比如我们实现了一个从HDFS读取数据，但是需要在Spark集群上运行的ETL程序，使用DirectRunner可以在本地非常容易地调试ETL程序，调试到程序的数据处理逻辑没有问题了，再最终在实际的生产环境Spark集群上运行。如果特定的PipelineRunner所对应的计算引擎没有很好的支撑调试功能，使用DirectRunner是非常方便的。 4. PCollectionPCollection是对分布式数据集的抽象，主要用作输入、输出、中间结果集。其中，在Apache Beam中对数据及其数据集的抽象有几类，我们画到一张类图上，如下图所示：PCollection是对数据集的抽象，包括输入输出，而基于Window的数据处理有对应的Window相关的抽象，还有一类就是TupleTag，针对具有CoGroup操作的情况下用来标记对应数据中的Tuple数据，具体如何使用可以后面我们实现的Join的例子。 5. PTransform一个Pipeline是由一个或多个PTransform构建而成的DAG图，其中每一个PTransform都具有输入和输出，所以PTransform是Apache Beam中非常核心的组件，我按照PTransform的做了一下分类，如下类图所示：通过上图可以看出，PTransform针对不同输入或输出的数据的特征，实现了一个算子（Operator）的集合，而Apache Beam除了期望实现一些通用的PTransform实现来供数据处理的开发人员开箱即用，同时也在API的抽象级别上做的非常Open，如果你想实现自己的PTransform来处理指定数据集，只需要自定义即可。而且，随着社区的活跃及其在实际应用场景中推广和使用，会很快构建一个庞大的PTransform实现库，任何有数据处理需求的开发人员都可以共享这些组件。 6. Combine这里，单独把Combine这类合并数据集的实现拿出来，它的抽象很有趣，主要面向globally 和per-key这两类抽象，实现了一个非常丰富的PTransform算子库，对应的类图如下所示：通过上图可以看出，作用在一个数据集上具有Combine特征的基本操作：Max、Min、Top、Mean、Sum、Count等等。 7. WindowWindow是用来处理某一个Micro batch的数据记录可以进行Merge这种场景的需求，通常用在Streaming处理的情况下。Apache Beam也提供了对Window的抽象，其中对于某一个Window下的数据的处理，是通过WindowFn接口来定义的，与该接口相关的处理类，如下类图所示： 编程实战首先说明一下，为了简单起见，我直接在代码中显式配置指定PipelineRunner，示例代码片段如下所示：12PipelineOptions options = PipelineOptionsFactory.create();options.setRunner(DirectRunner.class); 如果要部署到服务器上，可以通过命令行的方式指定PipelineRunner，比如要在Spark集群上运行，类似如下所示命令行：1spark-submit --class org.test.beam.examples.MinimalWordCountBasedSparkRunner 2017-01-18 --master spark://myserver:7077 target/my-beam-apps-0.0.1-SNAPSHOT-shaded.jar --runner=SparkRunner 下面，我们从几个典型的例子来看（基于Apache Beam软件包的examples有所改动），Apache Beam如何构建Pipeline并运行在指定的PipelineRunner上： WordCount（Count/Source/Sink）我们根据Apache Beam的MinimalWordCount示例代码开始，看如何构建一个Pipeline，并最终执行它。 MinimalWordCount的实现，代码如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package org.test.beam.examples;import org.apache.beam.runners.direct.DirectRunner;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Count;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.values.KV;public class MinimalWordCount &#123; @SuppressWarnings(&quot;serial&quot;) public static void main(String[] args) &#123; PipelineOptions options = PipelineOptionsFactory.create(); options.setRunner(DirectRunner.class); // 显式指定PipelineRunner：DirectRunner（Local模式） Pipeline pipeline = Pipeline.create(options); pipeline.apply(TextIO.Read.from(&quot;/tmp/dataset/apache_beam.txt&quot;)) // 读取本地文件，构建第一个PTransform .apply(&quot;ExtractWords&quot;, ParDo.of(new DoFn&lt;String, String&gt;() &#123; // 对文件中每一行进行处理（实际上Split） @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split(&quot;[\\s:\\,\\.\\-]+&quot;)) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) .apply(Count.&lt;String&gt; perElement()) // 统计每一个Word的Count .apply(&quot;ConcatResultKVs&quot;, MapElements.via( // 拼接最后的格式化输出（Key为Word，Value为Count） new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + &quot;: &quot; + input.getValue(); &#125; &#125;)) .apply(TextIO.Write.to(&quot;wordcount&quot;)); // 输出结果 pipeline.run().waitUntilFinish(); &#125;&#125; Pipeline的具体含义，可以看上面代码的注释信息。下面，我们考虑以HDFS数据源作为Source，如何构建第一个PTransform，代码片段如下所示：123PCollection&lt;KV&lt;LongWritable, Text&gt;&gt; resultCollection = pipeline.apply(HDFSFileSource.readFrom( &quot;hdfs://myserver:8020/data/ds/beam.txt&quot;, TextInputFormat.class, LongWritable.class, Text.class)) 可以看到，返回的是具有键值分别为LongWritable、Text类型的KV对象集合，后续处理和上面处理逻辑类似。如果使用Maven构建Project，需要加上如下依赖（这里beam.version的值可以为最新Release版本0.4.0）：12345&lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;beam.version&#125;&lt;/version&gt;&lt;/dependency&gt; 去重（Distinct）去重也是对数据集比较常见的操作，使用Apache Beam来实现，示例代码如下所示：1234567891011121314151617181920212223package org.test.beam.examples;import org.apache.beam.runners.direct.DirectRunner;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Distinct;public class DistinctExample &#123; public static void main(String[] args) throws Exception &#123; PipelineOptions options = PipelineOptionsFactory.create(); options.setRunner(DirectRunner.class); // 显式指定PipelineRunner：DirectRunner（Local模式） Pipeline pipeline = Pipeline.create(options); pipeline.apply(TextIO.Read.from(&quot;/tmp/dataset/MY_ID_FILE.txt&quot;)) .apply(Distinct.&lt;String&gt; create()) // 创建一个处理String类型的PTransform：Distinct .apply(TextIO.Write.to(&quot;deduped.txt&quot;)); // 输出结果 pipeline.run().waitUntilFinish(); &#125;&#125; 分组（GroupByKey）对数据进行分组操作也非常普遍，我们拿一个最基础的PTransform实现GroupByKey来实现一个例子，代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package org..beam.examples;import org.apache.beam.runners.direct.DirectRunner;import org.apache.beam.runners.direct.repackaged.com.google.common.base.Joiner;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.GroupByKey;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.values.KV;public class GroupByKeyExample &#123; @SuppressWarnings(&quot;serial&quot;) public static void main(String[] args) &#123; PipelineOptions options = PipelineOptionsFactory.create(); options.setRunner(DirectRunner.class); // 显式指定PipelineRunner：DirectRunner（Local模式） Pipeline pipeline = Pipeline.create(options); pipeline.apply(TextIO.Read.from(&quot;/tmp/dataset/MY_INFO_FILE.txt&quot;)) .apply(&quot;ExtractFields&quot;, ParDo.of(new DoFn&lt;String, KV&lt;String, String&gt;&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; // file format example: 35451605324179 3G CMCC String[] values = c.element().split(&quot;\t&quot;); if(values.length == 3) &#123; c.output(KV.of(values[1], values[0])); &#125; &#125; &#125;)) .apply(&quot;GroupByKey&quot;, GroupByKey.&lt;String, String&gt;create()) // 创建一个GroupByKey实例的PTransform .apply(&quot;ConcatResults&quot;, MapElements.via( new SimpleFunction&lt;KV&lt;String, Iterable&lt;String&gt;&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Iterable&lt;String&gt;&gt; input) &#123; return new StringBuffer() .append(input.getKey()).append(&quot;\t&quot;) .append(Joiner.on(&quot;,&quot;).join(input.getValue())) .toString(); &#125; &#125;)) .apply(TextIO.Write.to(&quot;grouppedResults&quot;)); pipeline.run().waitUntilFinish(); &#125;&#125; 使用DirectRunner运行，输出文件名称类似于grouppedResults-00000-of-00002、grouppedResults-00001-of-00002等等。 连接（Join）最后，我们通过实现一个Join的例子，其中，用户的基本信息包含ID和名称，对应文件格式如下所示：123435451605324179 Jack35236905298306 Jim35236905519469 John35237005022314 Linda 另一个文件是用户使用手机的部分信息，文件格式如下所示：12335451605324179 3G 中国移动35236905298306 2G 中国电信35236905519469 4G 中国移动 我们希望通过Join操作后，能够知道用户使用的什么网络（用户名+网络），使用Apache Beam实现，具体实现代码如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package org.test.beam.examples;import org.apache.beam.runners.direct.DirectRunner;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.transforms.join.CoGbkResult;import org.apache.beam.sdk.transforms.join.CoGroupByKey;import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;import org.apache.beam.sdk.values.KV;import org.apache.beam.sdk.values.PCollection;import org.apache.beam.sdk.values.TupleTag;public class JoinExample &#123; @SuppressWarnings(&quot;serial&quot;) public static void main(String[] args) &#123; PipelineOptions options = PipelineOptionsFactory.create(); options.setRunner(DirectRunner.class); // 显式指定PipelineRunner：DirectRunner（Local模式） Pipeline pipeline = Pipeline.create(options); // create ID info collection final PCollection&lt;KV&lt;String, String&gt;&gt; idInfoCollection = pipeline .apply(TextIO.Read.from(&quot;/tmp/dataset/MY_ID_INFO_FILE.txt&quot;)) .apply(&quot;CreateUserIdInfoPairs&quot;, MapElements.via( new SimpleFunction&lt;String, KV&lt;String, String&gt;&gt;() &#123; @Override public KV&lt;String, String&gt; apply(String input) &#123; // line format example: 35451605324179 Jack String[] values = input.split(&quot;\t&quot;); return KV.of(values[0], values[1]); &#125; &#125;)); // create operation collection final PCollection&lt;KV&lt;String, String&gt;&gt; opCollection = pipeline .apply(TextIO.Read.from(&quot;/tmp/dataset/MY_ID_OP_INFO_FILE.txt&quot;)) .apply(&quot;CreateIdOperationPairs&quot;, MapElements.via( new SimpleFunction&lt;String, KV&lt;String, String&gt;&gt;() &#123; @Override public KV&lt;String, String&gt; apply(String input) &#123; // line format example: 35237005342309 3G CMCC String[] values = input.split(&quot;\t&quot;); return KV.of(values[0], values[1]); &#125; &#125;)); final TupleTag&lt;String&gt; idInfoTag = new TupleTag&lt;String&gt;(); final TupleTag&lt;String&gt; opInfoTag = new TupleTag&lt;String&gt;(); final PCollection&lt;KV&lt;String, CoGbkResult&gt;&gt; cogrouppedCollection = KeyedPCollectionTuple .of(idInfoTag, idInfoCollection) .and(opInfoTag, opCollection) .apply(CoGroupByKey.&lt;String&gt;create()); final PCollection&lt;KV&lt;String, String&gt;&gt; finalResultCollection = cogrouppedCollection .apply(&quot;CreateJoinedIdInfoPairs&quot;, ParDo.of(new DoFn&lt;KV&lt;String, CoGbkResult&gt;, KV&lt;String, String&gt;&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; KV&lt;String, CoGbkResult&gt; e = c.element(); String id = e.getKey(); String name = e.getValue().getOnly(idInfoTag); for (String opInfo : c.element().getValue().getAll(opInfoTag)) &#123; // Generate a string that combines information from both collection values c.output(KV.of(id, &quot;\t&quot; + name + &quot;\t&quot; + opInfo)); &#125; &#125; &#125;)); PCollection&lt;String&gt; formattedResults = finalResultCollection .apply(&quot;FormatFinalResults&quot;, ParDo.of(new DoFn&lt;KV&lt;String, String&gt;, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; c.output(c.element().getKey() + &quot;\t&quot; + c.element().getValue()); &#125; &#125;)); formattedResults.apply(TextIO.Write.to(&quot;joinedResults&quot;)); pipeline.run().waitUntilFinish(); &#125;&#125; 参考内容 https://beam.apache.org/ https://beam.apache.org/get-started/quickstart/ https://beam.apache.org/get-started/beam-overview https://beam.apache.org/documentation/programming-guide/ https://www.infoq.com/presentations/apache-beam]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[git常用命令总结]]></title>
      <url>%2F2017%2F02%2F16%2Fgit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[git branch 不带参数：列出本地已经存在的分支，并且在当前分支的前面加“*”号标记 git branch -r 列出远程分支 git branch -a 列出本地分支和远程分支 git branch 创建一个新的本地分支，需要注意，此处只是创建分支，不进行分支切换 git branch -m | -M oldbranch newbranch 重命名分支，如果newbranch名字分支已经存在，则需要使用-M强制重命名，否则，使用-m进行重命名。 git branch -d | -D branchname 删除branchname分支 git branch -d -r branchname 删除远程branchname分支]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac Retina解决gitk模糊的问题]]></title>
      <url>%2F2017%2F02%2F16%2FMac-Retina%E8%A7%A3%E5%86%B3gitk%E6%A8%A1%E7%B3%8A%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[个人是gitk和Git gui的忠实用户，但是在MAC下安装后，发现极其模糊git在Mac下其实早就适配了高分辨率了，Patch如下：https://gist.githubusercontent.com/cynthia/5f2355a87c2f15d96dbe/raw/6727e73a007b0efabf55dd065e588467ffccc016/wish_app_info_plist.patch我们只需要把这里面最为关键的12&lt;key&gt;NSHighResolutionCapable&lt;/key&gt;&lt;true/&gt; 复制到Info.plist文件里面就可以了1vim /System/Library/Frameworks/Tk.framework/Versions/Current/Resources/Wish.app/Contents/Info.plist 然后在前面加上上面的配置。这之后，我们还需要更新一下Wish.app程序我是这样做的：1touch Wish.app 注意，在OS X EI Capitan 版本开启了一个rootless的功能，即使是root用户也无法修改/System目录，我们需要进入安全模式后，执行如下命令关闭rootless 功能才能够修改1csrutil disable 详细了解OS X的rootless mac进入安全模式：开机按住 shift 键即可]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac下brew安装gitk遇到错误Error in startup script: unknown color name 'lime']]></title>
      <url>%2F2017%2F02%2F16%2FMac%E4%B8%8Bbrew%E5%AE%89%E8%A3%85gitk%E9%81%87%E5%88%B0%E9%94%99%E8%AF%AFError-in-startup-script-unknown-color-name-lime%2F</url>
      <content type="text"><![CDATA[gitk 用home-brew, brew install git 以后，如果报错误：123456789101112131415Error in startup script: unknown color name &quot;lime&quot; (processing &quot;-fore&quot; option) invoked from within&quot;$ctext tag conf m2 -fore [lindex $mergecolors 2]&quot; (procedure &quot;makewindow&quot; line 347) invoked from within&quot;makewindow&quot; (file &quot;/usr/local/bin/gitk&quot; line 12434) 解决方法一： brew cask install tcl 解决方法二： 编辑gitk文件，把相应的color删除或修改。 解决方法三： 在home目录下配一个.gitk文件，将相关color定义一下。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深入学习Git工作流]]></title>
      <url>%2F2017%2F02%2F16%2F%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0Git%E5%B7%A5%E4%BD%9C%E6%B5%81%2F</url>
      <content type="text"><![CDATA[个人在学习git工作流的过程中，从原有的 SVN 模式很难完全理解git的协作模式，直到有一天我看到了下面的文章，好多遗留在心中的困惑迎刃而解。于是我将这部分资料整理如下： 我们以使用SVN的工作流来使用git有什么不妥？ git 方便的branch在哪里，团队多人如何协作？冲突了怎么办？如何进行发布控制？ 经典的master-发布、develop-主开发、hotfix-不过修复如何避免代码不经过验证上线？ 如何在github上面与他人一起协作，star-fork-pull request是怎样的流程？ 原文链接：Git Workflows and Tutorials简体中文：由 oldratlee 翻译在 github 上 git-workflows-and-tutorials 一、译序工作流其实不是一个初级主题，背后的本质问题其实是有效的项目流程管理和高效的开发协同约定，不仅是Git或SVN等VCS或SCM工具的使用。 这篇指南以大家在SVN中已经广为熟悉使用的集中式工作流作为起点，循序渐进地演进到其它高效的分布式工作流，还介绍了如何配合使用便利的Pull Request功能，体系地讲解了各种工作流的应用。 行文中实践原则和操作示例并重，对于Git的资深玩家可以梳理思考提升，而新接触的同学，也可以跟着step-by-step操作来操练学习并在实际工作中上手使用。 关于Git工作流主题，网上体系的中文资料不多，主要是零散的操作说明，希望这篇文章能让你更深入理解并在工作中灵活有效地使用起来。 PS：文中Pull Request的介绍用的是Bitbucket代码托管服务，由于和GitHub基本一样，如果你用的是GitHub（我自己也主要使用GitHub托管代码），不影响理解和操作。 PPS：本指南循序渐进地讲解工作流，如果Git用的不多，可以从前面的讲的工作流开始操练。操作过程去感受指南的讲解：解决什么问题、如何解决问题，这样理解就深了，也方便活用。 Gitflow工作流是经典模型，体现了工作流的经验和精髓。随着项目过程复杂化，会感受到这个工作流中深思熟虑和威力！ Forking工作流是协作的（GitHub风格）可以先看看Github的Help：Fork A Repo和Using pull requests 。照着操作，给一个Github项目贡献你的提交，有操作经验再看指南容易意会。指南中给了自己实现Fork的方法：Fork就是服务端的克隆。在指南的操练中使用代码托管服务（如GitHub、Bitbucket），可以点一下按钮就让开发者完成仓库的fork操作。 二、Git工作流指南工作流有各式各样的用法，但也正因此使得在实际工作中如何上手使用变得很头大。这篇指南通过总览公司团队中最常用的几种Git工作流让大家可以上手使用。 在阅读的过程中请记住，本文中的几种工作流是作为方案指导而不是条例规定。在展示了各种工作流可能的用法后，你可以从不同的工作流中挑选或揉合出一个满足你自己需求的工作流。 集中式工作流如果你的开发团队成员已经很熟悉Subversion，集中式工作流让你无需去适应一个全新流程就可以体验Git带来的收益。这个工作流也可以作为向更Git风格工作流迁移的友好过渡。转到分布式版本控制系统看起来像个令人生畏的任务，但不改变已用的工作流你也可以用上Git带来的收益。团队可以用和Subversion完全不变的方式来开发项目。 但使用Git加强开发的工作流，Git有相比SVN的几个优势。首先，每个开发可以有属于自己的整个工程的本地拷贝。隔离的环境让各个开发者的工作和项目的其他部分修改独立开来 ——即自由地提交到自己的本地仓库，先完全忽略上游的开发，直到方便的时候再把修改反馈上去。 其次，Git提供了强壮的分支和合并模型。不像SVN，Git的分支设计成可以做为一种用来在仓库之间集成代码和分享修改的『失败安全』的机制。 工作方式像Subversion一样，集中式工作流以中央仓库作为项目所有修改的单点实体。相比SVN缺省的开发分支trunk，Git叫做master，所有修改提交到这个分支上。本工作流只用到master这一个分支。 开发者开始先克隆中央仓库。在自己的项目拷贝中像SVN一样的编辑文件和提交修改；但修改是存在本地的，和中央仓库是完全隔离的。开发者可以把和上游的同步延后到一个方便时间点。 要发布修改到正式项目中，开发者要把本地master分支的修改『推』到中央仓库中。这相当于svn commit操作，但push操作会把所有还不在中央仓库的本地提交都推上去。 冲突解决中央仓库代表了正式项目，所以提交历史应该被尊重且是稳定不变的。如果开发者本地的提交历史和中央仓库有分歧，Git会拒绝push提交否则会覆盖已经在中央库的正式提交。在开发者提交自己功能修改到中央库前，需要先fetch在中央库的新增提交，rebase自己提交到中央库提交历史之上。这样做的意思是在说，『我要把自己的修改加到别人已经完成的修改上。』最终的结果是一个完美的线性历史，就像以前的SVN的工作流中一样。 如果本地修改和上游提交有冲突，Git会暂停rebase过程，给你手动解决冲突的机会。Git解决合并冲突，用和生成提交一样的git status和git add命令，很一致方便。还有一点，如果解决冲突时遇到麻烦，Git可以很简单中止整个rebase操作，重来一次（或者让别人来帮助解决）。 示例让我们一起逐步分解来看看一个常见的小团队如何用这个工作流来协作的。有两个开发者小明和小红，看他们是如何开发自己的功能并提交到中央仓库上的。 先初始化好中央仓库第一步，在服务器上创建好中央仓库。如果是新项目，你可以初始化一个空仓库；否则你要导入已有的Git或SVN仓库。 中央仓库应该是个裸仓库（bare repository），即没有工作目录（working directory）的仓库。可以用下面的命令创建：12ssh user@hostgit init --bare /path/to/repo.git 确保写上有效的user（SSH的用户名），host（服务器的域名或IP地址），/path/to/repo.git（你想存放仓库的位置）。注意，为了表示是一个裸仓库，按照约定加上.git扩展名到仓库名上。 所有人克隆中央仓库下一步，各个开发者创建整个项目的本地拷贝。通过git clone命令完成：1git clone ssh://user@host/path/to/repo.git 基于你后续会持续和克隆的仓库做交互的假设，克隆仓库时Git会自动添加远程别名origin指回『父』仓库。 小明开发功能在小明的本地仓库中，他使用标准的Git过程开发功能：编辑、暂存（Stage）和提交。如果你不熟悉暂存区（Staging Area），这里说明一下：暂存区的用来准备一个提交，但可以不用把工作目录中所有的修改内容都包含进来。这样你可以创建一个高度聚焦的提交，尽管你本地修改很多内容。123git status # 查看本地仓库的修改状态git add # 暂存文件git commit # 提交文件 请记住，因为这些命令生成的是本地提交，小明可以按自己需求反复操作多次，而不用担心中央仓库上有了什么操作。对需要多个更简单更原子分块的大功能，这个做法是很有用的。 小红开发功能与此同时，小红在自己的本地仓库中用相同的编辑、暂存和提交过程开发功能。和小明一样，她也不关心中央仓库有没有新提交；当然更不关心小明在他的本地仓库中的操作，因为所有本地仓库都是私有的。 小明发布功能一旦小明完成了他的功能开发，会发布他的本地提交到中央仓库中，这样其它团队成员可以看到他的修改。他可以用下面的git push命令：1git push origin master 注意，origin是在小明克隆仓库时Git创建的远程中央仓库别名。master参数告诉Git推送的分支。由于中央仓库自从小明克隆以来还没有被更新过，所以push操作不会有冲突，成功完成。 小红试着发布功能一起来看看在小明发布修改后，小红push修改会怎么样？她使用完全一样的push命令：1git push origin master 但她的本地历史已经和中央仓库有分岐了，Git拒绝操作并给出下面很长的出错消息：12345error: failed to push some refs to &apos;/path/to/repo.git&apos;hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Merge the remote changes (e.g. &apos;git pull&apos;)hint: before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 这避免了小红覆写正式的提交。她要先pull小明的更新到她的本地仓库合并上她的本地修改后，再重试。 小红在小明的提交之上rebase小红用git pull合并上游的修改到自己的仓库中。这条命令类似svn update——拉取所有上游提交命令到小红的本地仓库，并尝试和她的本地修改合并：1git pull --rebase origin master –rebase选项告诉Git把小红的提交移到同步了中央仓库修改后的master分支的顶部，如下图所示：如果你忘加了这个选项，pull操作仍然可以完成，但每次pull操作要同步中央仓库中别人修改时，提交历史会以一个多余的『合并提交』结尾。对于集中式工作流，最好是使用rebase而不是生成一个合并提交。 小红解决合并冲突rebase操作过程是把本地提交一次一个地迁移到更新了的中央仓库master分支之上。这意味着可能要解决在迁移某个提交时出现的合并冲突，而不是解决包含了所有提交的大型合并时所出现的冲突。这样的方式让你尽可能保持每个提交的聚焦和项目历史的整洁。反过来，简化了哪里引入Bug的分析，如果有必要，回滚修改也可以做到对项目影响最小。 如果小红和小明的功能是互不相关的，不大可能在rebase过程中有冲突。如果有，Git在合并有冲突的提交处暂停rebase过程，输出下面的信息并带上相关的指令：1CONFLICT (content): Merge conflict in &lt;some-file&gt; Git很赞的一点是，任何人可以解决他自己的冲突。在这个例子中，小红可以简单的运行git status命令来查看哪里有问题。冲突文件列在Unmerged paths（未合并路径）一节中：12345# Unmerged paths:# (use &quot;git reset HEAD &lt;some-file&gt;...&quot; to unstage)# (use &quot;git add/rm &lt;some-file&gt;...&quot; as appropriate to mark resolution)## both modified: &lt;some-file&gt; 接着小红编辑这些文件。修改完成后，用老套路暂存这些文件，并让git rebase完成剩下的事：12git add &lt;some-file&gt; git rebase --continue 要做的就这些了。Git会继续一个一个地合并后面的提交，如其它的提交有冲突就重复这个过程。 如果你碰到了冲突，但发现搞不定，不要惊慌。只要执行下面这条命令，就可以回到你执行git pull –rebase命令前的样子：1git rebase --abort 小红成功发布功能小红完成和中央仓库的同步后，就能成功发布她的修改了：1git push origin master 如你所见，仅使用几个Git命令我们就可以模拟出传统Subversion开发环境。对于要从SVN迁移过来的团队来说这太好了，但没有发挥出Git分布式本质的优势。 如果你的团队适应了集中式工作流，但想要更流畅的协作效果，绝对值得探索一下 功能分支工作流 的收益。通过为一个功能分配一个专门的分支，能够做到一个新增功能集成到正式项目之前对新功能进行深入讨论。 功能分支工作流功能分支工作流以集中式工作流为基础，不同的是为各个新功能分配一个专门的分支来开发。这样可以在把新功能集成到正式项目前，用Pull Requests的方式讨论变更。一旦你玩转了集中式工作流，在开发过程中可以很简单地加上功能分支，用来鼓励开发者之间协作和简化交流。 功能分支工作流背后的核心思路是所有的功能开发应该在一个专门的分支，而不是在master分支上。这个隔离可以方便多个开发者在各自的功能上开发而不会弄乱主干代码。另外，也保证了master分支的代码一定不会是有问题的，极大有利于集成环境。 功能开发隔离也让pull requests工作流成功可能，pull requests工作流能为每个分支发起一个讨论，在分支合入正式项目之前，给其它开发者有表示赞同的机会。另外，如果你在功能开发中有问题卡住了，可以开一个pull requests来向同学们征求建议。这些做法的重点就是，pull requests让团队成员之间互相评论工作变成非常方便！ 工作方式功能分支工作流仍然用中央仓库，并且master分支还是代表了正式项目的历史。但不是直接提交本地历史到各自的本地master分支，开发者每次在开始新功能前先创建一个新分支。功能分支应该有个有描述性的名字，比如animated-menu-items或issue-#1061，这样可以让分支有个清楚且高聚焦的用途。 在master分支和功能分支之间，Git是没有技术上的区别，所以开发者可以用和集中式工作流中完全一样的方式编辑、暂存和提交修改到功能分支上。 另外，功能分支也可以（且应该）push到中央仓库中。这样不修改正式代码就可以和其它开发者分享提交的功能。由于master仅有的一个『特殊』分支，在中央仓库上存多个功能分支不会有任何问题。当然，这样做也可以很方便地备份各自的本地提交。 Pull Requests功能分支除了可以隔离功能的开发，也使得通过Pull Requests讨论变更成为可能。一旦某个开发完成一个功能，不是立即合并到master，而是push到中央仓库的功能分支上并发起一个Pull Request请求去合并修改到master。在修改成为主干代码前，这让其它的开发者有机会先去Review变更。 Code Review是Pull Requests的一个重要的收益，但Pull Requests目的是讨论代码一个通用方式。你可以把Pull Requests作为专门给某个分支的讨论。这意味着可以在更早的开发过程中就可以进行Code Review。比如，一个开发者开发功能需要帮助时，要做的就是发起一个Pull Request，相关的人就会自动收到通知，在相关的提交旁边能看到需要帮助解决的问题。 一旦Pull Request被接受了，发布功能要做的就和集中式工作流就很像了。首先，确定本地的master分支和上游的master分支是同步的。然后合并功能分支到本地master分支并push已经更新的本地master分支到中央仓库。 仓库管理的产品解决方案像Bitbucket或Stash，可以良好地支持Pull Requests。可以看看Stash的Pull Requests文档。 示例下面的示例演示了如何把Pull Requests作为Code Review的方式，但注意Pull Requests可以用于很多其它的目的。 小红开始开发一个新功能在开始开发功能前，小红需要一个独立的分支。使用下面的命令新建一个分支：1git checkout -b marys-feature master 这个命令检出一个基于master名为marys-feature的分支，Git的-b选项表示如果分支还不存在则新建分支。这个新分支上，小红按老套路编辑、暂存和提交修改，按需要提交以实现功能：123git statusgit add &lt;some-file&gt;git commit 小红要去吃个午饭早上小红为新功能添加一些提交。去吃午饭前，push功能分支到中央仓库是很好的做法，这样可以方便地备份，如果和其它开发协作，也让他们可以看到小红的提交。1git push -u origin marys-feature 这条命令push marys-feature分支到中央仓库（origin），-u选项设置本地分支去跟踪远程对应的分支。设置好跟踪的分支后，小红就可以使用git push命令省去指定推送分支的参数。 小红完成功能开发小红吃完午饭回来，完成整个功能的开发。在合并到master之前，她发起一个Pull Request让团队的其它人知道功能已经完成。但首先，她要确认中央仓库中已经有她最近的提交：1git push 然后，在她的Git GUI客户端中发起Pull Request，请求合并marys-feature到master，团队成员会自动收到通知。Pull Request很酷的是可以在相关的提交旁边显示评注，所以你可以很对某个变更集提问。 小黑收到Pull Request小黑收到了Pull Request后会查看marys-feature的修改。决定在合并到正式项目前是否要做些修改，且通过Pull Request和小红来回地讨论。 小红再做修改要再做修改，小红用和功能第一个迭代完全一样的过程。编辑、暂存、提交并push更新到中央仓库。小红这些活动都会显示在Pull Request上，小黑可以断续做评注。 如果小黑有需要，也可以把marys-feature分支拉到本地，自己来修改，他加的提交也会一样显示在Pull Request上。 小红发布她的功能一旦小黑可以的接受Pull Request，就可以合并功能到稳定项目代码中（可以由小黑或是小红来做这个操作）：1234git checkout mastergit pullgit pull origin marys-featuregit push 无论谁来做合并，首先要检出master分支并确认是它是最新的。然后执行git pull origin marys-feature合并marys-feature分支到和已经和远程一致的本地master分支。你可以使用简单git merge marys-feature命令，但前面的命令可以保证总是最新的新功能分支。最后更新的master分支要重新push回到origin。 这个过程常常会生成一个合并提交。有些开发者喜欢有合并提交，因为它像一个新功能和原来代码基线的连通符。但如果你偏爱线性的提交历史，可以在执行合并时rebase新功能到master分支的顶部，这样生成一个快进（fast-forward）的合并。 一些GUI客户端可以只要点一下『接受』按钮执行好上面的命令来自动化Pull Request接受过程。如果你的不能这样，至少在功能合并到master分支后能自动关闭Pull Request。 与此同时，小明在做和小红一样的事当小红和小黑在marys-feature上工作并讨论她的Pull Request的时候，小明在自己的功能分支上做完全一样的事。 通过隔离功能到独立的分支上，每个人都可以自主的工作，当然必要的时候在开发者之间分享变更还是比较繁琐的。 到了这里，但愿你发现了功能分支可以很直接地在 集中式工作流 的仅有的master分支上完成多功能的开发。另外，功能分支还使用了Pull Request，使得可以在你的版本控制GUI客户端中讨论某个提交。 功能分支工作流是开发项目异常灵活的方式。问题是，有时候太灵活了。对于大型团队，常常需要给不同分支分配一个更具体的角色。Gitflow工作流是管理功能开发、发布准备和维护的常用模式。 Gitflow工作流Gitflow工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。这节介绍的Gitflow工作流借鉴自在nvie的Vincent Driessen。 Gitflow工作流定义了一个围绕项目发布的严格分支模型。虽然比功能分支工作流复杂几分，但提供了用于一个健壮的用于管理大型项目的框架。 Gitflow工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。除了使用功能分支，在做准备、维护和记录发布也使用各自的分支。当然你可以用上功能分支工作流所有的好处：Pull Requests、隔离实验性开发和更高效的协作。 工作方式Gitflow工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并push分支到要中央仓库中。 历史分支相对使用仅有的一个master分支，Gitflow工作流使用2个分支来记录项目的历史。master分支存储了正式发布的历史，而develop分支作为功能的集成分支。这样也方便master分支上的所有提交分配一个版本号。剩下要说明的问题围绕着这2个分支的区别展开。 功能分支每个新功能位于一个自己的分支，这样可以push到中央仓库以备份和协作。但功能分支不是从master分支上拉出新分支，而是使用develop分支作为父分支。当新功能完成时，合并回develop分支。新功能提交应该从不直接与master分支交互。注意，从各种含义和目的上来看，功能分支加上develop分支就是功能分支工作流的用法。但Gitflow工作流没有在这里止步。 发布分支一旦develop分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从develop分支上fork一个发布分支。新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上——这个分支只应该做Bug修复、文档生成和其它面向发布任务。一旦对外发布的工作都完成了，发布分支合并到master分支并分配一个版本号打好Tag。另外，这些从新建发布分支以来的做的修改要合并回develop分支。 使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本4.0』，并且在仓库的目录结构中可以实际看到）。 常用的分支约定：123用于新建发布分支的分支: develop用于合并的分支: master分支命名: release-* 或 release/* 维护分支维护分支或说是热修复（hotfix）分支用于生成快速给产品发布版本（production releases）打补丁，这是唯一可以直接从master分支fork出来的分支。修复完成，修改应该马上合并回master分支和develop分支（当前的发布分支），master分支应该用新的版本号打好Tag。 为Bug修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。你可以把维护分支想成是一个直接在master分支上处理的临时发布。 示例下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。 创建开发分支第一步为master分支配套一个develop分支。简单来做可以本地创建一个空的develop分支，push到服务器上：12git branch developgit push -u origin develop 以后这个分支将会包含了项目的全部历史，而master分支将只包含了部分历史。其它开发者这时应该克隆中央仓库，建好develop分支的跟踪分支：12git clone ssh://user@host/path/to/repo.gitgit checkout -b develop origin/develop 现在每个开发都有了这些历史分支的本地拷贝。 小红和小明开始开发新功能这个示例中，小红和小明开始各自的功能开发。他们需要为各自的功能创建相应的分支。新分支不是基于master分支，而是应该基于develop分支：1git checkout -b some-feature develop 他们用老套路添加提交到各自功能分支上：编辑、暂存、提交：123git statusgit add &lt;some-file&gt;git commit 小红完成功能开发添加了提交后，小红觉得她的功能OK了。如果团队使用Pull Requests，这时候可以发起一个用于合并到develop分支。否则她可以直接合并到她本地的develop分支后push到中央仓库：12345git pull origin developgit checkout developgit merge some-featuregit pushgit branch -d some-feature 第一条命令在合并功能前确保develop分支是最新的。注意，功能决不应该直接合并到master分支。冲突解决方法和集中式工作流一样。 小红开始准备发布这个时候小明正在实现他的功能，小红开始准备她的第一个项目正式发布。像功能开发一样，她用一个新的分支来做发布准备。这一步也确定了发布的版本号：1git checkout -b release-0.1 develop 这个分支是清理发布、执行所有测试、更新文档和其它为下个发布做准备操作的地方，像是一个专门用于改善发布的功能分支。 只要小红创建这个分支并push到中央仓库，这个发布就是功能冻结的。任何不在develop分支中的新功能都推到下个发布循环中。 小红完成发布一旦准备好了对外发布，小红合并修改到master分支和develop分支上，删除发布分支。合并回develop分支很重要，因为在发布分支中已经提交的更新需要在后面的新功能中也要是可用的。另外，如果小红的团队要求Code Review，这是一个发起Pull Request的理想时机。1234567git checkout mastergit merge release-0.1git pushgit checkout developgit merge release-0.1git pushgit branch -d release-0.1 发布分支是作为功能开发（develop分支）和对外发布（master分支）间的缓冲。只要有合并到master分支，就应该打好Tag以方便跟踪。12git tag -a 0.1 -m &quot;Initial public release&quot; mastergit push --tags Git有提供各种勾子（hook），即仓库有事件发生时触发执行的脚本。可以配置一个勾子，在你push中央仓库的master分支时，自动构建好对外发布。 最终用户发现Bug对外发布后，小红回去和小明一起做下个发布的新功能开发，直到有最终用户开了一个Ticket抱怨当前版本的一个Bug。为了处理Bug，小红（或小明）从master分支上拉出了一个维护分支，提交修改以解决问题，然后直接合并回master分支：12345git checkout -b issue-#001 master# Fix the buggit checkout mastergit merge issue-#001git push 就像发布分支，维护分支中新加这些重要修改需要包含到develop分支中，所以小红要执行一个合并操作。然后就可以安全地删除这个分支了：1234git checkout developgit merge issue-#001git pushgit branch -d issue-#001 到了这里，但愿你对集中式工作流、功能分支工作流和Gitflow工作流已经感觉很舒适了。你应该也牢固的掌握了本地仓库的潜能，push/pull模式和Git健壮的分支和合并模型。 记住，这里演示的工作流只是可能用法的例子，而不是在实际工作中使用Git不可违逆的条例。所以不要畏惧按自己需要对工作流的用法做取舍。不变的目标就是让Git为你所用。 Forking工作流Forking工作流是分布式工作流，充分利用了Git在分支和克隆上的优势。可以安全可靠地管理大团队的开发者（developer），并能接受不信任贡献者（contributor）的提交。 Forking工作流和前面讨论的几种工作流有根本的不同，这种工作流不是使用单个服务端仓库作为『中央』代码基线，而让各个开发者都有一个服务端仓库。这意味着各个代码贡献者有2个Git仓库而不是1个：一个本地私有的，另一个服务端公开的。Forking工作流的一个主要优势是，贡献的代码可以被集成，而不需要所有人都能push代码到仅有的中央仓库中。开发者push到自己的服务端仓库，而只有项目维护者才能push到正式仓库。这样项目维护者可以接受任何开发者的提交，但无需给他正式代码库的写权限。 效果就是一个分布式的工作流，能为大型、自发性的团队（包括了不受信的第三方）提供灵活的方式来安全的协作。也让这个工作流成为开源项目的理想工作流。 工作方式和其它的Git工作流一样，Forking工作流要先有一个公开的正式仓库存储在服务器上。但一个新的开发者想要在项目上工作时，不是直接从正式仓库克隆，而是fork正式项目在服务器上创建一个拷贝。 这个仓库拷贝作为他个人公开仓库 ——其它开发者不允许push到这个仓库，但可以pull到修改（后面我们很快就会看这点很重要）。在创建了自己服务端拷贝之后，和之前的工作流一样，开发者执行git clone命令克隆仓库到本地机器上，作为私有的开发环境。 要提交本地修改时，push提交到自己公开仓库中 —— 而不是正式仓库中。然后，给正式仓库发起一个pull request，让项目维护者知道有更新已经准备好可以集成了。对于贡献的代码，pull request也可以很方便地作为一个讨论的地方。 为了集成功能到正式代码库，维护者pull贡献者的变更到自己的本地仓库中，检查变更以确保不会让项目出错，合并变更到自己本地的master分支，然后pushmaster分支到服务器的正式仓库中。到此，贡献的提交成为了项目的一部分，其它的开发者应该执行pull操作与正式仓库同步自己本地仓库。 正式仓库在Forking工作流中，『官方』仓库的叫法只是一个约定，理解这点很重要。从技术上来看，各个开发者仓库和正式仓库在Git看来没有任何区别。事实上，让正式仓库之所以正式的唯一原因是它是项目维护者的公开仓库。 Forking工作流的分支使用方式所有的个人公开仓库实际上只是为了方便和其它的开发者共享分支。各个开发者应该用分支隔离各个功能，就像在功能分支工作流和Gitflow工作流一样。唯一的区别是这些分支被共享了。在Forking工作流中这些分支会被pull到另一个开发者的本地仓库中，而在功能分支工作流和Gitflow工作流中是直接被push到正式仓库中。 示例项目维护者初始化正式仓库和任何使用Git项目一样，第一步是创建在服务器上一个正式仓库，让所有团队成员都可以访问到。通常这个仓库也会作为项目维护者的公开仓库。 公开仓库应该是裸仓库，不管是不是正式代码库。所以项目维护者会运行像下面的命令来搭建正式仓库：12ssh user@hostgit init --bare /path/to/repo.git Bitbucket和Stash提供了一个方便的GUI客户端以完成上面命令行做的事。这个搭建中央仓库的过程和前面提到的工作流完全一样。如果有现存的代码库，维护者也要push到这个仓库中。 开发者fork正式仓库其它所有的开发需要fork正式仓库。可以用git clone命令用SSH协议连通到服务器，拷贝仓库到服务器另一个位置 —— 是的，fork操作基本上就只是一个服务端的克隆。Bitbucket和Stash上可以点一下按钮就让开发者完成仓库的fork操作。 这一步完成后，每个开发都在服务端有一个自己的仓库。和正式仓库一样，这些仓库应该是裸仓库。 开发者克隆自己fork出来的仓库下一步，各个开发者要克隆自己的公开仓库，用熟悉的git clone命令。 在这个示例中，假定用Bitbucket托管了仓库。记住，如果这样的话各个开发者需要有各自的Bitbucket账号，使用下面命令克隆服务端自己的仓库：1git clone https://user@bitbucket.org/user/repo.git 相比前面介绍的工作流只用了一个origin远程别名指向中央仓库，Forking工作流需要2个远程别名 ——一个指向正式仓库，另一个指向开发者自己的服务端仓库。别名的名字可以任意命名，常见的约定是使用origin作为远程克隆的仓库的别名（这个别名会在运行git clone自动创建），upstream（上游）作为正式仓库的别名。1git remote add upstream https://bitbucket.org/maintainer/repo 需要自己用上面的命令创建upstream别名。这样可以简单地保持本地仓库和正式仓库的同步更新。注意，如果上游仓库需要认证（比如不是开源的），你需要提供用户：1git remote add upstream https://user@bitbucket.org/maintainer/repo.git 这时在克隆和pull正式仓库时，需要提供用户的密码。 开发者开发自己的功能在刚克隆的本地仓库中，开发者可以像其它工作流一样的编辑代码、提交修改和新建分支：123git checkout -b some-feature# Edit some codegit commit -a -m &quot;Add first draft of some feature&quot; 所有的修改都是私有的直到push到自己公开仓库中。如果正式项目已经往前走了，可以用git pull命令获得新的提交：1git pull upstream master 由于开发者应该都在专门的功能分支上工作，pull操作结果会都是快进合并。 开发者发布自己的功能一旦开发者准备好了分享新功能，需要做二件事。首先，通过push他的贡献代码到自己的公开仓库中，让其它的开发者都可以访问到。他的origin远程别名应该已经有了，所以要做的就是：1git push origin feature-branch 这里和之前的工作流的差异是，origin远程别名指向开发者自己的服务端仓库，而不是正式仓库。 第二件事，开发者要通知项目维护者，想要合并他的新功能到正式库中。Bitbucket和Stash提供了Pull Request按钮，弹出表单让你指定哪个分支要合并到正式仓库。一般你会想集成你的功能分支到上游远程仓库的master分支中。 项目维护者集成开发者的功能当项目维护者收到pull request，他要做的是决定是否集成它到正式代码库中。有二种方式来做： 直接在pull request中查看代码 pull代码到他自己的本地仓库，再手动合并第一种做法更简单，维护者可以在GUI中查看变更的差异，做评注和执行合并。但如果出现了合并冲突，需要第二种做法来解决。这种情况下，维护者需要从开发者的服务端仓库中fetch功能分支，合并到他本地的master分支，解决冲突：1234git fetch https://bitbucket.org/user/repo feature-branch# 查看变更git checkout mastergit merge FETCH_HEAD 变更集成到本地的master分支后，维护者要push变更到服务器上的正式仓库，这样其它的开发者都能访问到：1git push origin master 注意，维护者的origin是指向他自己公开仓库的，即是项目的正式代码库。到此，开发者的贡献完全集成到了项目中。 开发者和正式仓库做同步由于正式代码库往前走了，其它的开发需要和正式仓库做同步：1git pull upstream master 如果你之前是使用SVN，Forking工作流可能看起来像是一个激进的范式切换（paradigm shift）。但不要害怕，这个工作流实际上就是在功能分支工作流之上引入另一个抽象层。不是直接通过单个中央仓库来分享分支，而是把贡献代码发布到开发者自己的服务端仓库中。 示例中解释了，一个贡献如何从一个开发者流到正式的master分支中，但同样的方法可以把贡献集成到任一个仓库中。比如，如果团队的几个人协作实现一个功能，可以在开发之间用相同的方法分享变更，完全不涉及正式仓库。 这使得Forking工作流对于松散组织的团队来说是个非常强大的工具。任一开发者可以方便地和另一开发者分享变更，任何分支都能有效地合并到正式代码库中。 Pull RequestsPull requests是Bitbucket提供的让开发者更方便地进行协作的功能，提供了友好的Web界面可以在提议的修改合并到正式项目之前对修改进行讨论。开发者向团队成员通知功能开发已经完成，Pull Requests是最简单的用法。开发者完成功能开发后，通过Bitbucket账号发起一个Pull Request。这样让涉及这个功能的所有人知道要去做Code Review和合并到master分支。 但是，Pull Request远不止一个简单的通知，而是为讨论提交的功能的一个专门论坛。如果变更有任何问题，团队成员反馈在Pull Request中，甚至push新的提交微调功能。所有的这些活动都直接跟踪在Pull Request中。相比其它的协作模型，这种分享提交的形式有助于打造一个更流畅的工作流。SVN和Git都能通过一个简单的脚本收到通知邮件；但是，讨论变更时，开发者通常只能去回复邮件。这样做会变得杂乱，尤其还要涉及后面的几个提交时。Pull Requests把所有相关功能整合到一个和Bitbucket仓库界面集成的用户友好Web界面中。 解析Pull Request当要发起一个Pull Request，你所要做的就是请求（Request）另一个开发者（比如项目的维护者）来pull你仓库中一个分支到他的仓库中。这意味着你要提供4个信息以发起Pull Request：源仓库、源分支、目的仓库、目的分支。这几值多数Bitbucket都会设置上合适的缺省值。但取决你用的协作工作流，你的团队可能会要指定不同的值。上图显示了一个Pull Request请求合并一个功能分支到正式的master分支上，但可以有多种不同的Pull Request用法。 工作方式Pull Request可以和功能分支工作流、Gitflow工作流或Forking工作流一起使用。但一个Pull Request要求要么分支不同要么仓库不同，所以不能用于集中式工作流。在不同的工作流中使用Pull Request会有一些不同，但基本的过程是这样的： 开发者在本地仓库中新建一个专门的分支开发功能。 开发者push分支修改到公开的Bitbucket仓库中。 开发者通过Bitbucket发起一个Pull Request。 团队的其它成员review code，讨论并修改。 项目维护者合并功能到官方仓库中并关闭Pull Request。 本文后面内容说明，Pull Request在不同协作工作流中如何应用。 在功能分支工作流中使用Pull Request功能分支工作流用一个共享的Bitbucket仓库来管理协作，开发者在专门的分支上开发功能。但不是立即合并到master分支上，而是在合并到主代码库之前开发者应该开一个Pull Request发起功能的讨论。功能分支工作流只有一个公开的仓库，所以Pull Request的目的仓库和源仓库总是同一个。通常开发者会指定他的功能分支作为源分支，master分支作为目的分支。 收到Pull Request后，项目维护者要决定如何做。如果功能没问题，就简单地合并到master分支，关闭Pull Request。但如果提交的变更有问题，他可以在Pull Request中反馈。之后新加的提交也会评论之后接着显示出来。 在功能还没有完全开发完的时候，也可能发起一个Pull Request。比如开发者在实现某个需求时碰到了麻烦，他可以发一个包含正在进行中工作的Pull Request。其它的开发者可以在Pull Request提供建议，或者甚至直接添加提交来解决问题。 在Gitflow工作流中使用Pull RequestGitflow工作流和功能分支工作流类似，但围绕项目发布定义一个严格的分支模型。在Gitflow工作流中使用Pull Request让开发者在发布分支或是维护分支上工作时，可以有个方便的地方对关于发布分支或是维护分支的问题进行交流。Gitflow工作流中Pull Request的使用过程和上一节中完全一致：当一个功能、发布或是热修复分支需要Review时，开发者简单发起一个Pull Request，团队的其它成员会通过Bitbucket收到通知。 新功能一般合并到develop分支，而发布和热修复则要同时合并到develop分支和master分支上。Pull Request可能用做所有合并的正式管理。 在Forking工作流中使用Pull Request在Forking工作流中，开发者push完成的功能到他自己的仓库中，而不是共享仓库。然后，他发起一个Pull Request，让项目维护者知道他的功能已经可以Review了。 在这个工作流，Pull Request的通知功能非常有用，因为项目维护者不可能知道其它开发者在他们自己的仓库添加了提交。由于各个开发有自己的公开仓库，Pull Request的源仓库和目标仓库不是同一个。源仓库是开发者的公开仓库，源分支是包含了修改的分支。如果开发者要合并修改到正式代码库中，那么目标仓库是正式仓库，目标分支是master分支。 Pull Request也可以用于正式项目之外的其它开发者之间的协作。比如，如果一个开发者和一个团队成员一起开发一个功能，他们可以发起一个Pull Request，用团队成员的Bitbucket仓库作为目标，而不是正式项目的仓库。然后使用相同的功能分支作为源和目标分支。2个开发者之间可以在Pull Request中讨论和开发功能。完成开发后，他们可以发起另一个Pull Request，请求合并功能到正式的master分支。在Forking工作流中，这样的灵活性让Pull Request成为一个强有力的协作工具。 示例下面的示例演示了Pull Request如何在在Forking工作流中使用。也同样适用于小团队的开发协作和第三方开发者向开源项目的贡献。 在示例中，小红是个开发，小明是项目维护者。他们各自有一个公开的Bitbucket仓库，而小明的仓库包含了正式工程。 小红fork正式项目小红先要fork小明的Bitbucket仓库，开始项目的开发。她登陆Bitbucket，浏览到小明的仓库页面，点Fork按钮。然后为fork出来的仓库填写名字和描述，这样小红就有了服务端的项目拷贝了。 小红克隆她的Bitbucket仓库下一步，小红克隆自己刚才fork出来的Bitbucket仓库，以在本机上准备出工作拷贝。命令如下：1git clone https://user@bitbucket.org/user/repo.git 请记住，git clone会自动创建origin远程别名，是指向小红fork出来的仓库。 小红开发新功能在开始改代码前，小红要为新功能先新建一个新分支。她会用这个分支作为Pull Request的源分支。123git checkout -b some-feature# 编辑代码git commit -a -m &quot;Add first draft of some feature&quot; 在新功能分支上，小红按需要添加提交。甚至如果小红觉得功能分支上的提交历史太乱了，她可以用交互式rebase来删除或压制提交。对于大型项目，整理功能分支的历史可以让项目维护者更容易看出在Pull Request中做了什么内容。 小红push功能到她的Bitbucket仓库中小红完成了功能后，push功能到她自己的Bitbucket仓库中（不是正式仓库），用下面简单的命令：1git push origin some-branch 这时她的变更可以让项目维护者看到了（或者任何想要看的协作者）。 小红发起Pull RequestBitbucket上有了她的功能分支后，小红可以用她的Bitbucket账号浏览到她的fork出来的仓库页面，点右上角的【Pull Request】按钮，发起一个Pull Request。弹出的表单自动设置小红的仓库为源仓库，询问小红以指定源分支、目标仓库和目标分支。 小红想要合并功能到正式仓库，所以源分支是她的功能分支，目标仓库是小明的公开仓库，而目标分支是master分支。另外，小红需要提供Pull Request的标题和描述信息。如果需要小明以外的人审核批准代码，她可以把这些人填在【Reviewers】文本框中。创建好了Pull Request，通知会通过Bitbucket系统消息或邮件（可选）发给小明。 小明review Pull Request在小明的Bitbucket仓库页面的【Pull Request】Tab可以看到所有人发起的Pull Request。点击小红的Pull Request会显示出Pull Request的描述、功能的提交历史和每个变更的差异（diff）。 如果小明想要合并到项目中，只要点一下【Merge】按钮，就可以同意Pull Request并合并到master分支。 但如果像这个示例中一样小明发现了在小红的代码中的一个小Bug，要小红在合并前修复。小明可以在整个Pull Request上加上评注，或是选择历史中的某个提交加上评注。 小红补加提交如果小红对反馈有任何疑问，可以在Pull Request中响应，把Pull Request当作是她功能讨论的论坛。 小红在她的功能分支新加提交以解决代码问题，并push到她的Bitbucket仓库中，就像前一轮中的做法一样。这些提交会进入的Pull Request，小明在原来的评注旁边可以再次review变更。 小明接受Pull Request最终，小明接受变更，合并功能分支到master分支，并关闭Pull Request。至此，功能集成到项目中，其它的项目开发者可以用标准的git pull命令pull这些变更到自己的本地仓库中。 到了这里，你应该有了所有需要的工具来集成Pull Request到你自己的工作流。请记住，Pull Request并不是为了替代任何 基于Git的协作工作流，而是它们的一个便利的补充，让团队成员间的协作更轻松方便。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[提高命令效率之利器oh-my-zsh]]></title>
      <url>%2F2017%2F02%2F15%2F%E6%8F%90%E9%AB%98%E5%91%BD%E4%BB%A4%E6%95%88%E7%8E%87%E4%B9%8B%E5%88%A9%E5%99%A8oh-my-zsh%2F</url>
      <content type="text"><![CDATA[当我知道了zsh，并体验了5分钟的时候，我决定将zsh作为我的默认 shell 终端。为什么？效率提高实在是太简单啦！从这里你可能也就知道了zsh是 shell 的一种，当然还包括目前估计是你默认的bash ，输入下面的命令，就能看到你的系统中提供了多少的 shell ：1cat /etc/shells 前人已经有好多使用zsh的，所以这类的文章也很多，包括怎么安装、使用技巧等等，请看： 池建强-终极 Shell 使用 zsh 的九个理由 ZSH Tips by ZZapper-很全的zsh命令汇总 我在用的mac软件(2)-终端环境之zsh和z(*nix都适用) 详细介绍作者使用的一些命令，通俗简单。 a-beginners-guide-to-the-best-command-line-tools 我所使用的几个 plugin 如下 ： autojump git colored-man colorize copydir command-not-found history sublime brew 挑选你自己的 plugin…… 更多plugin查看 - awesome-zsh-plugins 使用方法很简单，在~/.zshrc文件的plugin下面添加上你想要的插件名称就ok1plugins=(git autojump colored-man colorize copydir history sublime command-not-found) 如果你想要定制化你自己的zsh，访问官网 http://ohmyz.sh/ ，上面有你需要的 plugin、theme，有意思的还有T恤……]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[iterm2技巧]]></title>
      <url>%2F2017%2F02%2F15%2Fiterm2%E6%8A%80%E5%B7%A7%2F</url>
      <content type="text"><![CDATA[Mac 上最适合开发使用的终极终端 iTerm2，比自带的 term 终端有很多特性，比如竖屏操作、历史剪贴板、选中即复制、搜索高亮自动复制搜索内容、像 secureRT 样子的复制会话，记住密码登录，tux 集成等等。至于 Shell，一定要选则 zsh ，Mac 下已经自带，安装 oh my zsh 不用复杂的配置即可使用一些常用的提升效率的快捷键和插件整理如下: 快捷键官方的介绍特点: ⌘ + 数字在各 tab 标签直接来回切换 选择即复制 + 鼠标中键粘贴，这个很实用 ⌘ + f 所查找的内容会被自动复制 ⌘ + d 横着分屏 / ⌘ + shift + d 竖着分屏 ⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏 ctrl + u 清空当前行，无论光标在什么位置 输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令 ⌘ + shift + h 会列出剪切板历史 可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现 一些常用的快捷键如下: 新建标签：command + t 关闭标签：command + w 切换标签：command + 数字 command + 左右方向键 切换全屏：command + enter 查找：command + f 垂直分屏：command + d 水平分屏：command + shift + d 切换屏幕：command + option + 方向键 command + [ 或 command + ] 查看历史命令：command + 查看剪贴板历史：command + shift + h 清除当前行：ctrl + u 到行首：ctrl + a 到行尾：ctrl + e 前进后退：ctrl + f/b (相当于左右方向键) 上一条命令：ctrl + p 搜索命令历史：ctrl + r 删除当前光标的字符：ctrl + d 删除光标之前的字符：ctrl + h 删除光标之前的单词：ctrl + w 删除到文本末尾：ctrl + k 交换光标处文本：ctrl + t 清屏1：command + r 清屏2：ctrl + l 推荐插件一些插件能显著提高效率，自己使用的一些在下面，方法很简单，在~/.zshrc文件的plugin 下面添加上你想要的插件名称就ok，oh-my-sh 自带了很多插件，可以通过 ls ~/.oh-my-zsh/plugins 来查看。1plugins=(git-extras git mvn svn osx brew brew-cask npm colored-man colorize copydir history sublime command-not-found zsh-syntax-highlighting Z) git：当你处于一个 git 受控的目录下时，Shell 会明确显示 「git」和 branch，如上图所示，另外对 git 很多命令进行了简化，例如 gco=’git checkout’、gd=’git diff’、gst=’git status’、g=’git’等等，熟练使用可以大大减少 git 的命令长度，命令内容可以参考~/.oh-my-zsh/plugins/git/git.plugin.zsh textmate：mr可以创建 ruby 的框架项目，tm finename 可以用 textmate 打开指定文件。 osx：tab 增强，quick-look filename 可以直接预览文件，man-preview grep 可以生成 grep手册 的pdf 版本等。 git-extras: Git extras 工具与 zsh 的继承，很方便，在 git 仓库目录下试试git summary即可看到整个仓库的汇总信息。 sublime : 此插件能够在终端下使用命令stt 在 SublimeText 中打开当前文件夹，使用 subl 或者 st 来编辑某个特定文件，比如 st 1.txt，当然前提你得安装了 SublimeText。 zsh-syntax-highlighting: 让你终端的每一条命令智能显示颜色，就像在 IDE 里面写代码一样，强烈推荐 z : Z is awesome ，让你在不同的目录中快速跳转，比如我想访问 ~/work/code/project/testApp ，只要是之前访问过，直接输入z testApp 按 tab 键直接显示完整目录，按 enter 键直接进入当前目录，即使只输入了z testa 也能完成同样的工作，大大提升效率，此插件是自带的可以直接使用。 ag: 终端里面快速搜索当前目录下所有文件中所匹配的关键字的命令，类似与 awk，但是速度极快，速度极快，速度极快，使用brew install ，其实就是组件 the_silver_searcher 。 tree : mac 下的 tree 命令，方便排查问题，直接 brew install tree即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac键盘快捷键]]></title>
      <url>%2F2017%2F02%2F15%2FMac%E9%94%AE%E7%9B%98%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
      <content type="text"><![CDATA[要使用键盘快捷键，请按住一个或多个修饰键，同时按快捷键的最后一个键。例如，要使用快捷键 Command-C（拷贝），请按住 Command 键并按 C 键，然后同时松开这两个键。Mac 菜单和键盘通常使用某些按键的符号，其中包括以下修饰键： Command ⌘ Shift ⇧ Option ⌥ Control ⌃ Caps Lock ⇪ Fn 如果你使用的是 Windows PC 专用键盘，请用 Alt 键代替 Option 键，用 Windows 标志键代替 Command 键。有些 Mac 键盘在顶行中设有特殊按键，快捷键中也会用到它们；这些按键上有音量图标、显示屏亮度图标和其他功能图标。按下图标键可执行相应功能，将其与 Fn 键组合可用作 F1、F2、F3 或其他标准功能键。 常用快捷键1234567891011121314151617181920212223快捷键 | 描述Command-X | 剪切：删除所选项并将其拷贝到剪贴板。Command-C | 将所选项拷贝到剪贴板。这同样适用于 Finder 中的文件。Command-V | 将剪贴板的内容粘贴到当前文稿或 app 中。这同样适用于 Finder 中的文件。Command-Z | 撤销前一个命令。随后你可以按 Command-Shift-Z 来重做，从而反向执行撤销命令。在某些 app 中，你可以撤销和重做多个命令。Command-A | 全选各项。Command-F | 查找：打开“查找”窗口，或在文稿中查找项目。Command-G | 再次查找：查找之前所找到项目出现的下一个位置。要查找出现的上一个位置，请按 Command-Shift-G。Command-H | 隐藏最前面的 app 的窗口。要查看最前面的 app 但隐藏所有其他 app，请按 Command-Option-H。Command-M | 将最前面的窗口最小化至 Dock。要最小化最前面的 app 的所有窗口，请按 Command-Option-M。Command-N | 新建：打开一个新文稿或窗口。Command-O | 打开所选项，或打开一个对话框以选择要打开的文件。Command-P | 打印当前文稿。Command-S | 存储当前文稿。Command-W | 关闭最前面的窗口。要关闭该 app 的所有窗口，请按 Command-Option-W。Command-Q | 退出 app。空格键 | 快速查看：使用快速查看预览所选项。Command-Tab | 切换 app：在打开的 app 中切换到下一个最近使用的 app。Shift-Command-3 | 屏幕快照：拍摄整个屏幕的屏幕快照。了解更多屏幕快照快捷键。Command-逗号 (,) | 偏好设置：打开最前面的 app 的偏好设置。Option-Command-Esc | 强制退出：选择要强制退出的 app。或者，按住 Command-Shift-Option-Esc 3 秒钟来仅强制最前面的 app 退出。Command–空格键 | Spotlight：显示或隐藏 Spotlight 搜索栏。要从 Finder 窗口执行 Spotlight 搜索，请按 Command–Option–空格键。如果你使用多个输入源以便用不同的语言键入内容，这些快捷键会更改输入源而非显示 Spotlight。Shift-Command-波浪号 (~) | 切换窗口：切换到最前端应用中下一个最近使用的窗口。 文档快捷键123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 快捷键 | 描述Command-B | 以粗体显示所选文本，或者打开或关闭粗体显示功能。 Command-I | 以斜体显示所选文本，或者打开或关闭斜体显示功能。Command-U | 对所选文本加下划线，或者打开或关闭加下划线功能。Command-T | 显示或隐藏“字体”窗口.Command-D | 从“打开”对话框或“存储”对话框中选择“桌面”文件夹。Control-Command-D | 显示或隐藏所选字词的定义。Shift-Command-冒号 (:) | 显示“拼写和语法”窗口。Command-分号 (;) | 查找文稿中拼写错误的字词。Option-Delete | 删除插入点左边的字词。Control-H | 删除插入点左边的字符。也可以使用 Delete 键。Control-D | 删除插入点右边的字符。也可以使用 Fn-Delete。Fn-Delete | 在没有向前删除 键的键盘上向前删除。也可以使用 Control-D。Control-K | 删除插入点与行或段落末尾处之间的文本。Command-Delete | 在包含“删除”或“不存储”按钮的对话框中选择“删除”或“不存储”。Fn–上箭头 | 向上翻页：向上滚动一页。 Fn–下箭头 | 向下翻页：向下滚动一页。Fn–左箭头 | 开头：滚动到文稿开头。Fn–右箭头 | 结尾：滚动到文稿末尾。Command–上箭头 | 将插入点移至文稿开头。Command–下箭头 | 将插入点移至文稿末尾。Command–左箭头 | 将插入点移至当前行的行首。Command–右箭头 | 将插入点移至当前行的行尾。Option–左箭头 | 将插入点移至上一字词的词首。Option–右箭头 | 将插入点移至下一字词的词尾。Shift–Command–上箭头 | 选中插入点与文稿开头之间的文本。Shift–Command–下箭头 | 选中插入点与文稿末尾之间的文本。Shift–Command–左箭头 | 选中插入点与当前行行首之间的文本。Shift–Command–右箭头 | 选中插入点与当前行行尾之间的文本。Shift–上箭头 | 将文本选择范围扩展到上一行相同水平位置的最近字符处。Shift–下箭头 | 将文本选择范围扩展到下一行相同水平位置的最近字符处。Shift–左箭头 | 将文本选择范围向左扩展一个字符。Shift–右箭头 | 将文本选择范围向右扩展一个字符。Option–Shift–上箭头 | 将文本选择范围扩展到当前段落的段首，再按一次则扩展到下一段落的段首。Option–Shift–下箭头 | 将文本选择范围扩展到当前段落的段尾，再按一次则扩展到下一段落的段尾。Option–Shift–左箭头 | 将文本选择范围扩展到当前字词的词首，再按一次则扩展到后一字词的词首。Option–Shift–右箭头 | 将文本选择范围扩展到当前字词的词尾，再按一次则扩展到后一字词的词尾。Control-A | 移至行或段落的开头。Control-E | 移至行或段落的末尾。Control-F | 向前移动一个字符。Control-B | 向后移动一个字符。Control-L | 将光标或所选内容置于可见区域中央。Control-P | 上移一行。Control-N | 下移一行。Control-O | 在插入点后插入一行。Control-T | 将插入点后面的字符与插入点前面的字符交换。Command–左花括号 (&#123;) | 左对齐。Command–右花括号 (&#125;) | 右对齐。Shift–Command–竖线 | 居中对齐。Option-Command-F | 前往搜索栏。 Option-Command-T | 显示或隐藏应用中的工具栏。Option-Command-C | 拷贝样式：将所选项的格式设置拷贝到剪贴板。Option-Command-V | 粘贴样式：将拷贝的样式应用到所选项。Option-Shift-Command-V | 粘贴并匹配样式：将周围内容的样式应用到粘贴在该内容中的项目。Option-Command-I | 显示或隐藏检查器窗口。Shift-Command-P | 页面设置：显示用于选择文稿设置的窗口。Shift-Command-S | 显示“存储为”对话框或复制当前文稿。Shift–Command–减号 (-) | 缩小所选项。Shift–Command–加号 (+) | 放大所选项。Command–等号 (=) 可执行相同的功能。Shift–Command–问号 (?) | 打开“帮助”菜单。 截图操作123456截图快捷键 | 含义command+shift+3 | 全屏截图，保存截图到桌面文件command+shift+4 | 鼠标选定区域截图，保存截图到桌面文件command+shift+control+3 | 全屏截图，保存到剪贴板command+shift+control+4 | 鼠标选定区域截图，保存到剪贴板command+shift(+control)+4 | 然后按下空格键，鼠标变成小相机，选择某一窗口后点击鼠标左键对单个窗口截图。不必担心其它窗口的遮挡。 Finder 快捷键12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364快捷键 | 描述Command-D | 复制所选文件。Command-E | 推出所选磁盘或宗卷。Command-F | 在 Finder 窗口中开始 Spotlight 搜索。Command-I | 显示所选文件的“显示简介”窗口。Shift-Command-C | 打开“电脑”窗口。Shift-Command-D | 打开“桌面”文件夹。Shift-Command-F | 打开“我的所有文件”窗口。Shift-Command-G | 打开“前往文件夹”窗口。Shift-Command-H | 打开当前 macOS 用户帐户的个人文件夹。Shift-Command-I | 打开 iCloud Drive。Shift-Command-K | 打开“网络”窗口。Option-Command-L| 打开“下载”文件夹。Shift-Command-O | 打开“文稿”文件夹。Shift-Command-R | 打开“AirDrop”窗口。Shift-Command-T | 将所选的 Finder 项目添加到 Dock（OS X Mountain Lion 或较早版本）Control-Shift-Command-T| 将所选的 Finder 项目添加到 Dock（OS X Mavericks 或更高版本）Shift-Command-U | 打开“实用工具”文件夹。Option-Command-D | 显示或隐藏 Dock。即使您未打开 Finder，此快捷键通常也有效。Control-Command-T | 将所选项添加到边栏（OS X Mavericks 或更高版本）。Option-Command-P | 隐藏或显示 Finder 窗口中的路径栏。Option-Command-S | 隐藏或显示 Finder 窗口中的边栏。Command–斜线 (/) | 隐藏或显示 Finder 窗口中的状态栏。Command-J | 调出“显示”选项。Command-K | 打开“连接服务器”窗口。Command-L | 为所选项制作替身。Command-N | 打开一个新的 Finder 窗口。Shift-Command-N | 新建文件夹。Option-Command-N | 新建智能文件夹。Command-R | 显示所选替身的原始文件。Command-T | 在当前 Finder 窗口中打开单个标签时显示或隐藏标签栏。Shift-Command-T | 显示或隐藏 Finder 标签。Option-Command-T | 在当前 Finder 窗口中打开单个标签时显示或隐藏工具栏。Option-Command-V | 移动：将剪贴板中的文件从其原始位置移动到当前位置。Option-Command-Y | 显示所选文件的快速查看幻灯片显示。Command-Y| 使用“快速查看”预览所选文件。Command-1| 以图标方式显示 Finder 窗口中的项目。Command-2| 以列表方式显示 Finder 窗口中的项目。Command-3| 以分栏方式显示 Finder 窗口中的项目。 Command-4| 以 Cover Flow 方式显示 Finder 窗口中的项目。Command–左中括号 ([) | 前往上一文件夹。Command–右中括号 (]) | 前往下一文件夹。Command–上箭头 | 打开包含当前文件夹的文件夹。Command–Control–上箭头 | 在新窗口中打开包含当前文件夹的文件夹。Command–下箭头 | 打开所选项。Command–Mission Control | 显示桌面。即使您未打开 Finder，此快捷键也有效。Command–调高亮度 | 开启或关闭目标显示器模式。Command–调低亮度 | 当 Mac 连接到多个显示器时打开或关闭显示器镜像功能。右箭头 | 打开所选文件夹。此快捷键仅在列表视图中有效。左箭头 | 关闭所选文件夹。此快捷键仅在列表视图中有效。Option-连按 | 在单独窗口中打开文件夹，并关闭当前窗口。Command-连按 | 在单独标签或窗口中打开文件夹。Command-Delete | 将所选项移到废纸篓。Shift-Command-Delete| 清倒废纸篓。Option-Shift-Command-Delete| 清倒废纸篓（不显示确认对话框）。Command-Y | 使用“快速查看”预览文件。Option–调高亮度 | 打开“显示器”偏好设置。此快捷键可与任一亮度键搭配使用。Option–Mission Control | 打开“Mission Control”偏好设置。Option–调高音量 | 打开“声音”偏好设置。此快捷键可与任一音量键搭配使用。拖移时按 Command 键| 将拖移的项目移到其他宗卷或位置。拖移项目时指针会随之变化。拖移时按 Option 键 | 拷贝拖移的项目。拖移项目时指针会随之变化。拖移时按下 Option-Command | 为拖移的项目制作替身。拖移项目时指针会随之变化。Option-点按伸缩三角形| 打开所选文件夹内的所有文件夹。此快捷键仅在列表视图中有效。Command-点按窗口标题 | 查看包含当前文件夹的文件夹。 睡眠、注销和关机快捷键12345678快捷键 | 描述电源按钮 | 轻点可打开 Mac 或将 Mac 从睡眠状态唤醒。 当 Mac 处于唤醒状态时，按住此按钮 1.5 秒钟会显示一个对话框，询问您是要重新启动、睡眠还是关机。如果您不想等待 1.5 秒钟，请按下 Control–电源按钮或 Control–介质推出键 。按住此按钮 5 秒钟会强制 Mac 关机。Control–Command–电源按钮 | 强制 Mac 重新启动。Control–Shift–（电源按钮或介质推出键 ） | 将显示器置于睡眠状态。Control–Command–介质推出键 | 退出所有 app，然后重新启动 Mac。如果任何打开的文稿有未存储的更改，系统将询问您是否要存储这些更改。Control–Option–Command–（电源按钮或介质推出键 ） | 退出所有 app，然后关闭 Mac。如果任何打开的文稿有未存储的更改，系统将询问您是否要存储这些更改。Shift-Command-Q | 注销您的 macOS 用户帐户。系统将提示您确认。Option-Shift-Command-Q | 立即注销您的 macOS 用户帐户，且系统不提示您确认。 Mac 的启动组合键在启动期间按住某些键可以使用一些 Mac 功能。 请在 Mac 开机并听到启动声后立即按住这些键。请一直按住，直至所述行为出现。以下组合适用于基于 Intel 的 Mac 电脑。12345678910111213141516在启动期间按住 | 描述Shift ⇧ | 以安全模式启动。Option ⌥ | 启动进入启动管理器。C | 从可引导的 CD、DVD 或 USB 闪存驱动器（如 OS X 安装介质）启动。D | 启动进入 Apple Hardware Test 或 Apple Diagnostics，具体取决于您正在使用的 Mac。Option-D | 通过互联网启动进入 Apple Hardware Test 或 Apple Diagnostics。N | 从兼容的 NetBoot 服务器启动。Option-N | 使用默认的启动映像从 NetBoot 服务器启动。T | 以目标磁盘模式启动。X | 从 OS X 启动宗卷启动，否则 Mac 将从非 OS X 启动宗卷启动。Command (⌘)-R | 从 OS X 恢复功能启动。Command-Option-R | 通过互联网从 OS X 恢复功能启动。Command-Option-P-R | 重置 NVRAM。当再次听到启动声后，请松开这些键。Command-S | 以单用户模式启动。Command-V | 以详细模式启动。推出键 (⏏)、F12、鼠标键或触控板按钮 | 推出可移动介质，如光盘。 整理了那么多快捷键，一时半时根本记不住，怎么办？除了有意识的经常使用、练习外，还有一款神奇软件 CheatSheet 在任何应用程序下面长按Command ⌘ 键，即可以查看这款软件的快捷键操作。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(8)--分布式文档存储]]></title>
      <url>%2F2017%2F02%2F14%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-8-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E6%A1%A3%E5%AD%98%E5%82%A8-wait-for-active-shards%E6%96%B0%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[学完ES分布式集群的工作原理以及一些基本的将数据放入索引然后检索它们的所有方法，我们可以继续学习在分布式系统中，每个分片的文档是被如何索引和查询的。 路由首先，我们需要明白，文档和分片之间是如何匹配的，这就是路由。当你索引一个文档，它被存储在单独一个主分片上。Elasticsearch是如何知道文档属于哪个分片的呢？当你创建一个新文档，它是如何知道是应该存储在分片1还是分片2上的呢？ 进程不能是随机的，因为我们将来要检索文档。事实上，它根据一个简单的算法决定：1shard = hash(routing) % number_of_primary_shards routing值是一个任意字符串，它默认是_id但也可以自定义。这个routing字符串通过哈希函数生成一个数字，然后除以主切片的数量得到一个余数(remainder)，余数的范围永远是0到number_of_primary_shards - 1，这个数字就是特定文档所在的分片。 这也解释了为什么主分片的数量只能在创建索引时定义且不能修改：如果主分片的数量在未来改变了，所有先前的路由值就失效了，文档也就永远找不到了。 所有的文档API（get、index、delete、bulk、update、mget）都接收一个routing参数，它用来自定义文档到分片的映射。自定义路由值可以确保所有相关文档——例如属于同一个人的文档——被保存在同一分片上。 例如，可以这样设置参数：123456POST twitter/tweet?routing=kimchy&#123; &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125; 主分片和复制分片如何交互在文档确认存储到哪个主分片以后，接下来就是主分片将数据复制到复制分片的任务，为了阐述意图，我们假设有三个节点的集群。它包含一个叫做blogs的索引并拥有两个主分片。每个主分片有两个复制分片。相同的分片不会放在同一个节点上，所以我们的集群是这样的：我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点，所以也可以将请求转发到需要的节点。下面的例子中，我们将发送所有请求给Node 1，这个节点我们将会称之为请求节点(requesting node)。一般情况下，当我们发送请求，最好的做法是循环通过所有节点请求，这样可以平衡负载。 新建、索引和删除文档新建、索引和删除请求都是写(write)操作，它们必须在主分片上成功完成才能复制到相关的复制分片上。 下面是在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤： 客户端给Node 1发送新建、索引或删除请求。 节点使用文档的_id确定文档属于分片0。它转发请求到Node 3，分片0位于这个节点上。 Node 3在主分片上执行请求，如果成功，它转发请求到相应的位于Node 1和Node 2的复制节点上。当所有的复制节点报告成功，Node 3报告成功到请求的节点，请求的节点再报告给客户端。客户端接收到成功响应的时候，文档的修改已经被应用于主分片和所有的复制分片。你的修改生效了。 有很多可选的请求参数允许你更改这一过程。你可能想牺牲一些安全来提高性能。这些选项很少使用因为Elasticsearch已经足够快。 注意：下面的参数只对ElasticSearch 5.0以下的版本有效，在ElasticSearch 5.0之后貌似使用wait_for_active_shards代替了consistency。所以之前的参数了解即可，实际可以参考：Create Index—Wait For Active Shards。 replication（注意在ElasticSearch 5.0开始被废弃）复制默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。 如果你设置replication为async，请求在主分片上被执行后就会返回给客户端。它依旧会转发请求给复制节点，但你将不知道复制节点成功与否。 上面的这个选项不建议使用。默认的sync复制允许Elasticsearch强制反馈传输。async复制可能会因为在不等待其它分片就绪的情况下发送过多的请求而使Elasticsearch过载。 consistency（注意在ElasticSearch 5.0开始被废弃）默认主分片在尝试写入时需要规定数量(quorum)或过半的分片（可以是主节点或复制节点）可用。这是防止数据被写入到错的网络分区。规定的数量计算公式如下：1int( (primary + number_of_replicas) / 2 ) + 1 consistency允许的值为one（只有一个主分片），all（所有主分片和复制分片）或者默认的quorum或过半分片。 注意number_of_replicas是在索引中的的设置，用来定义复制分片的数量，而不是现在活动的复制节点的数量。如果你定义了索引有3个复制节点，那规定数量是：1int( (primary + 3 replicas) / 2 ) + 1 = 3 但如果你只有2个节点，那你的活动分片不够规定数量，也就不能索引或删除任何文档。 注意： 新索引默认有1个复制分片，这意味着为了满足quorum的要求需要两个活动的分片。当然，这个默认设置将阻止我们在单一节点集群中进行操作。为了避开这个问题，规定数量只有在number_of_replicas大于一时才生效。 一个疑惑，是不是primary值一直都只会是1？？？ wait_for_active_shards（新参数）在ElasticSearch 5.0中可以用wait_for_active_shards参数表示：等待活动的分片，具体的值和consistency类似，下面用wait_for_active_shards演示一个实际使用的例子。 开始我们先设置一个新的索引：1234567PUT /active&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 3 &#125;&#125; 我们默认先只打开两个节点，等下我们设置wait_for_active_shards值为3，按照上面讲解的我们如果只有两个节点，那么活动的分片最多也就2个，所以是不够的，会等待新的活动节点的到来。（这里我们只能通过少一个节点的方法演示缺少活动分片，因为我们不方便演示出让某个分片处于不活动的状态。）因为我们只有两个节点，所以活动的分片最多也只有两个。下面我们执行文档存储操作，并且添加参数wait_for_active_shards=3：可以发现，确实开始处于等待状态，没有马上返回结果，下面我们参数开启第三个节点，让索引拥有第三个活动分片：可以看到一旦我们的节点开启，文档的存储马上就会返回成功。 教程中关于这部分网上很多朋友不太理解，我们可以通过查看官方文档和实践去证明自己的想法，希望上面的分析大家可以理解一些，还有不对的地方大家可以一起学习。 timeout当分片副本不足时会怎样？Elasticsearch会等待更多的分片出现。默认等待一分钟。如果需要，你可以设置timeout参数让它终止的更早：100表示100毫秒，30s表示30秒。 检索文档文档能够从主分片或任意一个复制分片被检索。下面我们罗列在主分片或复制分片上检索一个文档必要的顺序步骤： 客户端给Node 1发送get请求。 节点使用文档的_id确定文档属于分片0。分片0对应的复制分片在三个节点上都有。此时，它转发请求到Node 2。 Node 2返回文档(document)给Node 1然后返回给客户端。对于读请求，为了平衡负载，请求节点会为每个请求选择不同的分片——它会循环所有分片副本（包括主分片）。 可能的情况是，一个被索引的文档已经存在于主分片上却还没来得及同步到复制分片上。这时复制分片会报告文档未找到，主分片会成功返回文档。一旦索引请求成功返回给用户，文档则在主分片和复制分片都是可用的。 更新文档update API结合了之前提到的读和写的模式。下面我们罗列执行局部更新必要的顺序步骤： 客户端给Node 1发送更新请求。 它转发请求到主分片所在节点Node 3。 Node 3从主分片检索出文档，修改_source字段的JSON，然后在主分片上重建索引。如果有其他进程修改了文档，它以retry_on_conflict设置的次数重复步骤3，都未成功则放弃。 如果Node 3成功更新文档，它同时转发文档的新版本到Node 1和Node 2上的复制节点以重建索引。当所有复制节点报告成功，Node 3返回成功给请求节点，然后返回给客户端。 update API还接受routing、replication（弃）、consistency（弃）和timout参数。 基于文档的复制当主分片转发更改给复制分片时，并不是转发更新请求，而是转发整个文档的新版本。记住这些修改转发到复制节点是异步的，它们并不能保证到达的顺序与发送相同。如果Elasticsearch转发的仅仅是修改请求，修改的顺序可能是错误的，那得到的就是个损坏的文档。 多文档模式mget和bulk API与单独的文档类似。差别是请求节点知道每个文档所在的分片。它把多文档请求拆成每个分片的对文档请求，然后转发每个参与的节点。 一旦接收到每个节点的应答，然后整理这些响应组合为一个单独的响应，最后返回给客户端。下面我们将罗列通过一个mget请求检索多个文档的顺序步骤： 客户端向Node 1发送mget请求。 Node 1为每个分片构建一个多条数据检索请求，然后转发到这些请求所需的主分片或复制分片上。当所有回复被接收，Node 1构建响应并返回给客户端。 routing 参数可以被docs中的每个文档设置。下面我们将罗列使用一个bulk执行多个create、index、delete和update请求的顺序步骤： 客户端向Node 1发送bulk请求。 Node 1为每个分片构建批量请求，然后转发到这些请求所需的主分片上。 主分片一个接一个的按序执行操作。当一个操作执行完，主分片转发新文档（或者删除部分）给对应的复制节点，然后执行下一个操作。一旦所有复制节点报告所有操作已成功完成，节点就报告success给请求节点，后者(请求节点)整理响应并返回给客户端。 bulk API还可以在最上层使用replication（弃）和consistency（弃）参数，routing参数则在每个请求的元数据中使用。 总结以上就是关于在分布式系统中，每个分片的文档是被如何索引和查询的。虽然版本的更新有一些参数会更新，但是整体的内部实现应该不会有太大的变化，分享一个学习方法，学习的时候把新旧的版本内容通过对比，不仅可以更好理解知识，而且可以加深印象。更何况旧的不会被很快淘汰，学了又何妨！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(7)——分布式集群学习2]]></title>
      <url>%2F2017%2F02%2F13%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-7-%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AD%A6%E4%B9%A02%2F</url>
      <content type="text"><![CDATA[前面主要学习了ElasticSearch分布式集群的存储过程中集群、节点和分片的知识（ES-5.0.2-学习(6)——分布式集群学习1），下面主要分享应对故障的一些实践。 应对故障前面说了很多关于复制分片可以应对节点失效，很好保证集群的安全性，下面我们可以尝试杀掉第一个节点的进程，我们的集群变化成如下（所有的操作都是ElasticSearch自动处理）：我们杀掉的节点是一个主节点。一个集群必须要有一个主节点才能使其功能正常，所以集群做的第一件事就是各节点选举了一个新的主节点：Node 2。 主分片1和2在我们杀掉Node 1时已经丢失，我们的索引在丢失主分片时不能正常工作。如果此时我们检查集群健康，我们将看到状态red：不是所有主分片都可用！ 幸运的是丢失的两个主分片的完整拷贝存在于其他节点上，所以新主节点做的第一件事是把这些在Node 2和Node 3上的复制分片升级为主分片，这时集群健康回到yellow状态。这个提升是瞬间完成的，就好像按了一下开关。 为什么集群健康状态是yellow而不是green？我们有三个主分片，但是我们指定了每个主分片对应两个复制分片，当前却只有一个复制分片被分配，这就是集群状态无法达到green的原因，不过不用太担心这个：当我们杀掉Node 2，我们的程序依然可以在没有丢失数据的情况下继续运行，因为Node 3还有每个分片的拷贝。 如果我们重启Node 1，集群将能够重新分配丢失的复制分片，集群状况与上一节的图5：增加number_of_replicas到2 类似。如果Node 1依旧有旧分片的拷贝，它将会尝试再利用它们，它只会从主分片上复制在故障期间有数据变更的那一部分。 故障实践1上面是关于ElasticSearch在遇到故障时候的理论部分，下面我们开始实际操作。 查看目前集群状态我们回顾一下之前的blogs索引，在结束最后的状态：1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, (主分片个数) &quot;number_of_replicas&quot; : 2, (每个主分片的复制分片个数) &#125;&#125; 切断节点为了模拟这种情况，在我们自己的电脑上，直接用kill命令即可:12ps -ef | grep elasticsearch #获取到elasticsearch的进程id，例如5891kill 5891 查看集群的状态很正确，就是理论内容所描述中间会存在red的瞬间。等·等·等·可是等了半天，结果一直是red状态的结果，这是为什么呢？注意看提示无法连接到http://localhost:9200，突然意识到，我们关闭的节点正好是9200端口的Node 1节点。所以我们需要修改kibana.yml配置文件的elasticsearch.url项： 再次查看集群的状态终于，可以看到我们想要的结果，ElasticSearch集群正如上面所说的重新选Node 2作为新的主节点：我们还可以注意到集群的健康状况从绿色变成了黄色，这是因为我们设置每个主节点2个复制分片，而现在还有一个复制节点处于不可用状态。 故障实践2回顾之前的一个集群状态，blogs索引只设置一个复制分片的情况下：如果在这种情况下，我们把其中的任何一个节点关闭，会出现什么效果呢？我们分析看，至少我们关闭任何一个节点都能保所有的分片都还能存在。比如我们删除Node 2节点，正常情况下，Node 2中的分片0作为主分片被删除后，主节点会分配Node 1节点下复制分片0重新作为主分片0，而Node 2中的分片1本身是复制分片，直接删除即可，但是ElasticSearch集群，除此之外还会不会有其他操作。那就是，从新在两个节点中把所有的复制分片都置为可用。下面我们看结果：首先我们看到的和我们前面分析的一样，主节点会分配Node 1节点下复制分片0重新作为主分片0，但是也可以看到现在集群的健康状况是黄色，因为存在复制节点处于不可用状态。我们继续等。。。：终于我们可以看到，ElasticSearch集群确实会把所有的复制节点又都置为可用状态，因为节点存在它不拥有的分片，就可以创建这个节点，最大程度的保证高可用性。 实践注意点在测试过程中，ElasticSearch集群确实可以帮助我们重新分配分片的状态，但是需要注意的是，每次一个节点关闭的时候，集群需要一定的时间去管理，如果这时候我们很快的将两个节点关闭，ElasticSearch集群将无法挽救回没有主分片，也没有复制分片的那些数据，所以测试的时候需要知道这一点。 不过这也反映我们在学习分享1中描述的，如果我们的复制节点足够多的话，我们可以保证高可用的能力就却强大，因为允许节点故障的次数更多，而且我们的节点故障以后，运维又可以将节点重启，继续斗争！！！ 总结现在我们对分片如何使Elasticsearch可以水平扩展并保证数据安全有了一个清晰的认识。真正感受到Elasticsearch天生就是分布式的，确实很强大！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(6)——分布式集群学习1]]></title>
      <url>%2F2017%2F02%2F13%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-6-%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AD%A6%E4%B9%A01%2F</url>
      <content type="text"><![CDATA[在使用中我们把文档存入ElasticSearch，但是如果能够了解ElasticSearch内部是如何存储的，将会对我们学习ElasticSearch有很清晰的认识。本文中的所使用的ElasticSearch集群环境，可以通过查看 ES-5.0.2-学习(3)——单台服务器部署多个节点 搭建学习。ElasticSearch用于构建高可用和可扩展的系统。扩展的方式可以是购买更好的服务器(纵向扩展(vertical scale or scaling up))或者购买更多的服务器（横向扩展(horizontal scale or scaling out)）。 Elasticsearch虽然能从更强大的硬件中获得更好的性能，但是纵向扩展有它的局限性。真正的扩展应该是横向的，它通过增加节点来均摊负载和增加可靠性。 对于大多数数据库而言，横向扩展意味着你的程序将做非常大的改动才能利用这些新添加的设备。对比来说，Elasticsearch天生就是分布式的：它知道如何管理节点来提供高扩展和高可用。这意味着你的程序不需要关心这些。 下面的例子主要围绕着集群(cluster)、节点(node)和分片(shard)讲解，相信学习以后，对于学习Elasticsearch会有很大收获。 空集群如果我们启动一个单独的节点，它还没有数据和索引，这个集群看起来如下图：只有一个空节点的集群。一个节点(node)就是一个Elasticsearch实例，而一个集群(cluster)由一个或多个节点组成，它们具有相同cluster.name，它们协同工作，分享数据和负载。当加入新的节点或者删除一个节点时，集群就会感知到并平衡数据。 集群中一个节点会被选举为主节点(master)，它将临时管理集群级别的一些变更，例如新建或删除索引、增加或移除节点等。主节点不参与文档级别的变更或搜索，这意味着在流量增长的时候，该主节点不会成为集群的瓶颈。任何节点都可以成为主节点。我们例子中的集群只有一个节点，所以它会充当主节点的角色。 作为用户，我们能够与集群中的任何节点通信，包括主节点。每一个节点都知道文档存在于哪个节点上，它们可以转发请求到相应的节点上。我们访问的节点负责收集各节点返回的数据，最后一起返回给客户端。这一切都由Elasticsearch处理。 集群健康在Elasticsearch集群中可以监控统计很多信息，但是只有一个是最重要的：集群健康(cluster health)。集群健康有三种状态：green、yellow或red，健康状况在后面会有很多体现。1GET /_cluster/health 在一个没有索引的空集群中运行如上查询，将返回这些信息：123456789101112&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 0, &quot;active_shards&quot;: 0, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0&#125; status： 是我们最感兴趣的字段。status字段提供一个综合的指标来表示集群的的服务状况。三种颜色各自的含义： green：所有主要分片和复制分片都可用。 yellow：所有主要分片可用，但不是所有复制分片都可用。 red：不是所有的主要分片都可用。 添加索引为了将数据添加到Elasticsearch，我们需要索引(index)——一个存储关联数据的地方。实际上，索引只是一个用来指向一个或多个分片(shards)的“逻辑命名空间(logical namespace)”. 一个分片(shard)是一个最小级别“工作单元(worker unit)”,它只是保存了索引中所有数据的一部分。并且先初步知道分片就是一个Lucene实例，它本身就是一个完整的搜索引擎。我们的文档存储在分片中，并且在分片中被索引，但是我们的应用程序不会直接与它们通信，取而代之的是，直接与索引通信。 分片是Elasticsearch在集群中分发数据的关键。把分片想象成数据的容器。文档存储在分片中，然后分片分配到你集群中的节点上。当你的集群扩容或缩小，Elasticsearch将会自动在你的节点间迁移分片，以使集群保持平衡。 分片可以是主分片(primary shard)或者是复制分片(replica shard)。你索引中的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整。 复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的分片取回文档。 让我们在集群中唯一一个空节点上创建一个叫做blogs的索引。默认情况下，一个索引被分配5个主分片，但是为了演示的目的，我们只分配3个主分片和一个复制分片（每个主分片都有一个复制分片）：1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 我们的集群现在看起来就像上图~~三个主分片都被分配到Node 1。如果我们现在检查集群健康(cluster-health)，我们将见到以下信息：123456789101112&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;yellow&quot;, &lt;1&gt; &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 3, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 3&#125; 集群的状态现在是 yellow 我们的三个复制分片还没有被分配到节点上 下面我们可以看下，在Kibana监控工具中查看具体情况，如下图：集群的健康状态yellow表示所有的主分片(primary shards)启动并且正常运行了——集群已经可以正常处理任何请求——但是复制分片(replica shards)还没有全部可用。事实上所有的三个复制分片现在都是unassigned状态——它们还未被分配给节点。在同一个节点上保存相同的数据副本是没有必要的，如果这个节点故障了，那所有的数据副本也会丢失。 现在我们的集群已经功能完备，但是依旧存在因硬件故障而导致数据丢失的风险。 添加故障转移在单一节点上运行意味着有单点故障的风险——没有数据备份。幸运的是，要防止单点故障，我们唯一需要做的就是启动另一个节点。 具体启动方式可以查看 ES-5.0.2-学习(3)——单台服务器部署多个节点 。 如果我们启动了第二个节点，这个集群看起来就像下图。双节点集群——所有的主分片和复制分片都已分配:第二个节点已经加入集群，三个复制分片(replica shards)也已经被分配了——分别对应三个主分片，这意味着在丢失任意一个节点的情况下依旧可以保证数据的完整性。 文档的索引将首先被存储在主分片中，然后并发复制到对应的复制节点上。这可以确保我们的数据在主节点和复制节点上都可以被检索。 cluster-health 现在的状态是 green，这意味着所有的6个分片（三个主分片和三个复制分片）都已可用：123456789101112&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, &quot;number_of_data_nodes&quot;: 2, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 6, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0&#125; 集群的状态是green，我们的集群不仅是功能完备的，而且是高可用的。同样我们可以看下实际的操作结果： 横向扩展随着应用需求的增长，我们该如何扩展？如果我们启动第三个节点，集群会重新组织，包含3个节点的集群，分片已经被重新分配以平衡负载：Node 3包含了分别来自Node 1和Node 2的一个分片，这样每个节点就有两个分片，和之前相比少了一个，这意味着每个节点上的分片将获得更多的硬件资源（CPU、RAM、I/O）。 分片本身就是一个完整的搜索引擎，它可以使用单一节点的所有资源。我们拥有6个分片（3个主分片和三个复制分片），最多可以扩展到6个节点，每个节点上有一个分片，每个分片可以100%使用这个节点的资源。 同样我们可以看下实际的操作结果： 继续扩展如果我们要扩展到6个以上的节点，要怎么做？ 主分片的数量在创建索引时已经确定。实际上，这个数量定义了能存储到索引里数据的最大数量（实际的数量取决于你的数据、硬件和应用场景）。然而，主分片或者复制分片都可以处理读请求——搜索或文档检索，所以数据的冗余越多，我们能处理的搜索吞吐量就越大。 复制分片的数量可以在运行中的集群中动态地变更，这允许我们可以根据需求扩大或者缩小规模。让我们把复制分片的数量从原来的1增加到2：1234PUT /blogs/_settings&#123; &quot;number_of_replicas&quot; : 2&#125; 从图中可以看出，blogs索引现在有9个分片：3个主分片和6个复制分片。这意味着我们能够扩展到9个节点，再次变成每个节点一个分片。这样使我们的搜索性能相比原始的三节点集群增加“三倍”。 实际操作也是同样的效果：注意：可以看到上面的“三倍”我们用加了引号，因为在同样数量的节点上增加更多的复制分片并不一定提高性能，因为这样做的话平均每个分片的所占有的硬件资源就减少了（大部分请求都聚集到了分片少的节点，导致一个节点吞吐量太大，反而降低性能），你需要增加硬件来提高吞吐量。所以说添加复制分片和添加节点，在保证成本的情况下，需要有一个平衡点。 不过这些额外的复制节点还是有另外一个好处，使我们有更多的冗余：通过以上对节点的设置，我们能够承受两个节点故障而不丢失数据。 总结对于ES分布式集群如果对节点、分片的处理基本学习完毕，可以感受到ES分布式集群的自动化，对于用户来说几乎完全透明化。但是，一个分布式集群主要看它的高性能、高并发和高可用。上面的内容虽然体现了一些，但是还包括对故障的处理能力，这部分将在 ES-5.0.2-学习(7)——分布式集群学习2 继续和大家分享。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(5)--第一个ES例子]]></title>
      <url>%2F2017%2F02%2F10%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-3-%E7%AC%AC%E4%B8%80%E4%B8%AAES%E4%BE%8B%E5%AD%90%2F</url>
      <content type="text"><![CDATA[想要知道ElasticSearch是如何使用的，最快的方式就是通过一个简单的例子，第一个例子将会包括基本概念如索引、搜索、和聚合等，需求是关于公司管理员工的一些业务。# 员工文档索引业务首先需要存储员工数据。这将采取一个员工文档的形式：单个文档表示单个员工。在Elasticsearch中存储数据的行为称为索引，但是在索引文档之前，我们需要决定在哪里存储它。在Elasticsearch中，文档属于某个类型，这些类型位于索引中。可以绘制一些（粗略）与传统关系数据库的对比： Relational DB ⇒ Databases ⇒ Tables ⇒ Rows ⇒ Columns Elasticsearch ⇒ Indices ⇒ Types ⇒ Documents ⇒ FieldsElasticsearch集群可以包含多个索引（数据库），这些索引又包含多个类型（表）。这些类型包含多个文档（行），每个文档都有多个字段（列）。你可能已经注意到，在Elasticsearch的上下文中，索引被重载了几个含义。如下： 索引（名词）：正如前面所解释的那样，索引就像传统的关系数据库中的数据库一样。它是存储相关文档的地方。index的复数形式是indices或indexes。 索引（动词）：索引一个文档是将一个文档存储在索引（名词）中，以便它可以检索和查询。它很像插入关键词SQL。此外，如果文档已经存在，新的文档将取代旧的。 倒排索引：关系数据库中增加一个索引，如B-树索引，对特定列为了提高数据检索的速度。Elasticsearch和Lucene提供相同目的的索引称为倒排索引。默认情况下，文档中的每个字段索引（有一个倒排索引）这样的搜索。一个没有倒排索引字段不可搜索因此我们的员工目录，我们需要处理如下事情： 索引的每个文档，包含每个员工的所有细节。 每个文档都属于**employee类型。 类型都包含在*megacop索引中。 该索引将驻留我们Elasticsearch集群内。下面通过命令去索引第一个员工：12345678PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125;注意 */megacorp/employee/1 包含的信息:&gt; megacorp：索引的名称 emplogee：类型的名称 1：*员工的id成功执行返回的是一个JSON文本，包含所有关于该员工的信息。 注意：1. 如果执行过程中失败了，可能存在的原因是elasticsearch默认配置中不允许自动创建索引，所以我们可以先简单在elasticsearch.yml配置文件添加action.auto_create_index：true，允许自动创建索引。2. 没有必要首先执行任何管理任务，如创建一个索引或指定每个字段所包含的数据类型。我们可以直接索引一个文档。Elasticsearch附带默认的一切，因此所有必要的管理任务都会使用默认值在后台处理。在目录中添加更多的员工：1234567891011121314151617PUT /megacorp/employee/2&#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125;PUT /megacorp/employee/3&#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ]&#125;# 检索文档现在我们有一些数据存储在Elasticsearch中，我们可以开始处理这个应用程序的业务需求。第一个要求是检索单个员工数据的能力。这在Elasticsearch中很容易。我们只需执行HTTP GET请求并指定文档的地址——索引，类型和ID。使用这三个信息，我们可以返回原始的JSON文档，并且响应包含有关文档的一些元数据，以及Douglas Fir的原始JSON文档作为_source字段：可以查看：ES-5.0.2-学习(4)–简单搜索以同样的方式，我们将HTTP动词从PUT更改为GET以便检索文档，我们可以使用DELETE动词删除文档，并使用HEAD动词检查文档是否存在。要用更新的版本替换现有文档，我们只需再次PUT。GET很简单，可以得到要求的文件。尝试一些更高级的东西，我们可以搜索所有员工，请求：1GET /megacorp/employee/_search你可以看到我们仍在使用索引megacorp和类型employee，但是我们现在使用_search端点，而不是指定文档ID。响应包括我们在hits数组中的所有三个文档。默认情况下，搜索将返回前10个结果。响应不仅告诉我们哪些文档匹配，而且还包括整个文档本身，以便向用户显示搜索结果的所有信息。接下来，让我们尝试搜索在其姓氏中有“Smith”的员工。为此，我们将使用一个轻松的搜索方法，它很容易从命令行使用。此方法通常称为查询字符串搜索，因为我们将搜索作为URL查询字符串参数传递：# DSL查询查询字符串搜索对于从命令行进行搜索非常方便，但它有其局限性。Elasticsearch提供了一种丰富，灵活的查询语言，称为查询DSL，它允许我们构建更复杂，更健壮的查询。使用JSON请求正文指定域特定语言（DSL）。我们可以代表所有以前的搜索，像这样：12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125;这将返回与上一个查询相同的结果。可以看到一些事情已经改变。例如，我们不再使用查询字符串参数，而是使用请求正文。此请求体是使用JSON构建的，并使用匹配查询。让我们让搜索更复杂一点。我们仍然希望找到所有名字为Smith的员工，但我们只想要30岁以上的员工。我们的查询将稍微改变一点，以容纳一个过滤器，这使我们能够有效地执行结构化搜索：1234567891011121314151617GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;smith&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125; &#125; &#125; &#125; &#125;&#125;我们添加了一个过滤器，执行范围搜索，并重复使用与以前相同的匹配查询。现在我们的结果显示只有一个员工刚好是32并被命名为smith：*注意：**关于过滤器在Elasticsearch2.0开始有很大的更新，所以有些过滤操作可能会报错。例如：filtered query已经被废弃。# 全文搜索（Full-Text Search）到目前为止的搜索很简单：单个名字，按年龄过滤。让我们尝试更高级的全文搜索，传统数据库真正难以胜任的任务。我们将寻找所有喜欢攀岩的员工：12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125;您可以看到我们使用与之前相同的匹配查询来搜索关于“攀岩”字段。我们得到两个匹配的文档：默认情况下，Elasticsearch按匹配结果的相关性分值（即每个文档与查询匹配程度）对匹配结果进行排序。第一个和最高分的结果是显而易见的：John·Smith关于字段清楚地说“攀岩”。但为什么Jane·Smith也返回了？她的文档被返回的原因是因为在她的字段中提到了“rock”这个词。因为只有“岩石”被提及，而不是“攀登”，她的分数低于John的。这是Elasticsearch如何在全文字段中进行搜索并返回最相关的结果的一个很好的例子。这种相关性的概念对于Elasticsearch很重要，并且是一个完全与传统关系数据库无关的概念，其中记录匹配或不匹配。# 精确字段搜索在字段中查找单个字词是很好的，但有时你想要匹配字词或短语的确切序列。例如，我们可以执行一个查询，该查询将仅匹配包含“rock”和“climbing”的员工记录，并在短语“rock climbing”中显示彼此相邻的单词。为此，我们使用改为match_phrase查询：12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125;仅返回John Smith的文档# 高亮搜索结果许多应用程序喜欢从每个搜索结果突出显示文本片段，以便用户可以看到文档与查询匹配的原因。在Elasticsearch中检索突出显示的片段很容易。让我们重新运行我们以前的查询，但添加一个新的highlight参数：12345678910111213GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot; : &#123; &quot;about&quot; : &#123;&#125; &#125; &#125;&#125;当我们运行此查询时，将返回与之前相同的返回，但现在我们在响应中得到一个新的部分，称为突出显示。这包含来自about字段的文字片段，其中包含在 HTML标记中包含的匹配单词：# 分析最后，我们来到我们的最后一个业务需求：允许管理员在员工目录上运行分析。Elasticsearch具有称为聚合的功能，允许您对数据生成复杂的分析。它类似于GROUP BY中的SQL，但功能更强大。例如，让我们找到我们的员工最喜欢的兴趣：12345678GET /megacorp/employee/_search&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125;如果Elasticsearch 5版本以前，将会返回：12345678910111213141516171819202122&#123; ... &quot;hits&quot;: &#123; ... &#125;, &quot;aggregations&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125;我们可以看到，两个员工对音乐感兴趣，一个在林业，一个在体育。这些聚合不是预先计算的，它们是从与当前查询匹配的文档即时生成的。然而如果我们使用的是Elasticsearch 5版本以上的话，将会出现如下异常：我们可以查看 Elasticsearch 5.0文档——Fielddata is disabled on text fields by default*大概的意思是：Fielddata可以消耗大量的堆空间，特别是在加载高基数文本字段时。一旦fielddata已经加载到堆中，它在该段的生存期内保持。此外，加载fielddata是一个昂贵的过程，可以导致用户体验延迟命中。所以fielddata默认禁用。如果尝试对文本字段上的脚本进行排序，聚合或访问值，就会看到这个异常，具体使用可以参考手册。# 总结这个小例子是一个很好的演示了什么是Elasticsearch。它只是很肤浅的介绍了简单的使用，许多功能被省略，以保持简短。但是这也突出了开始构建高级搜索功能是多么容易。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(4)--简单搜索]]></title>
      <url>%2F2017%2F02%2F10%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-3-%E7%AE%80%E5%8D%95%E6%90%9C%E7%B4%A2%2F</url>
      <content type="text"><![CDATA[空搜索1GET /_search 注：以下操作均是在kibana中，如图所示hits： total 总数 hits 前10条数据 hits 数组中的每个结果都包含_index、_type和文档的_id字段，被加入到_source字段中这意味着在搜索结果中我们将可以直接使用全部文档。 每个节点都有一个_score字段，这是相关性得分(relevance score)，它衡量了文档与查询的匹配程度。默认的，返回的结果中关联性最大的文档排在首位；这意味着，它是按照_score降序排列的。没有指定任何查询，所以所有文档的相关性是一样的，因此所有结果的_score都是取得一个中间值1。 took：整个搜索请求花费的毫秒数。_shards：节点告诉我们参与查询的分片数（total字段），有多少是成功的（successful），有多少的是失败的（failed）。time_out：告诉我们查询超时与否。一般的，搜索请求不会超时。如果响应速度比完整的结果更重要，你可以定义timeout参数为10或者10ms（10毫秒），或者1s（1秒）1GET /_search?timeout=10ms Elasticsearch将返回在请求超时前收集到的结果。注意： timeout不会停止执行查询，它仅仅告诉你目前顺利返回结果的节点然后关闭连接。在后台，其他分片可能依旧执行查询，尽管结果已经被发送。使用超时是因为对于你的业务需求来说非常重要，而不是因为你想中断执行长时间运行的查询。 多索引和多类别在所有索引的所有类型中搜索：/_search在索引gb的所有类型中搜索：/gb/_search在索引gb和us的所有类型中搜索：/gb,us/_search在以g或u开头的索引的所有类型中搜索：/g*,u*/_search在索引gb的类型user中搜索：/gb/user/_search在索引gb和us的类型为user和tweet中搜索：/gb,us/user,tweet/_search在所有索引的user和tweet中搜索：/_all/user,tweet/_search当你搜索包含单一索引时，Elasticsearch转发搜索请求到这个索引的主分片或每个分片的复制分片上，然后聚集每个分片的结果。搜索包含多个索引也是同样的方式——只不过会有更多的分片被关联。 分页如果你想每页显示5个结果，页码从1到3，那请求如下：123GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 应该当心分页太深或者一次请求太多的结果。结果在返回前会被排序。但是记住一个搜索请求常常涉及多个分片。每个分片生成自己排好序的结果，它们接着需要集中起来排序以确保整体排序正确。现在假设我们请求第1000页——结果10001到10010。工作方式都相同，不同的是每个分片都必须产生顶端的10010个结果。然后请求节点排序这50050个结果并丢弃50040个！ 简易搜索search API有两种表单：一种是“简易版”的查询字符串(query string)将所有参数通过查询字符串定义，另一种版本使用JSON完整的表示请求体(request body)，这种富搜索语言叫做结构化查询语句（DSL）。查询字符串搜索对于在命令行下运行特定情况下查询特别有用。例如这个语句查询所有类型为tweet并在tweet字段中包含elasticsearch字符的文档：1GET /_all/tweet/_search?q=tweet:elasticsearch 下一个语句查找name字段中包含”john”和tweet字段包含”mary”的结果。实际的查询只需要：1+name:john +tweet:mary 但是url编码需要将查询字符串参数变得更加神秘：1GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary “+”前缀表示语句匹配条件必须被满足。类似的”-“前缀表示条件必须不被满足。所有条件如果没有+或-表示是可选的——匹配越多，相关的文档就越多。 _all字段返回包含”mary”字符的所有文档的简单搜索：1GET /_search?q=mary 当你索引一个文档，Elasticsearch把所有字符串字段值连接起来放在一个大字符串中，它被索引为一个特殊的字段_all。例如，当索引这个文档：123456&#123; &quot;tweet&quot;: &quot;However did I manage before Elasticsearch?&quot;, &quot;date&quot;: &quot;2014-09-14&quot;, &quot;name&quot;: &quot;Mary Jones&quot;, &quot;user_id&quot;: 1&#125; 这好比我们增加了一个叫做_all的额外字段值：1&quot;However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1&quot; 若没有指定字段，查询字符串搜索（即q=xxx）使用_all字段搜索。 更复杂的语句下一个搜索的语句：_all field name字段包含”mary”或”john” date晚于2014-09-10 _all字段包含”aggregations”或”geo” 1+name:(mary john) +date:&gt;2014-09-10 +(aggregations geo) 编码后的查询字符串变得不太容易阅读1?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo) 就像你上面看到的例子，简单查询字符串搜索惊人的强大。允许我们简洁明快的表示复杂的查询。这对于命令行下一次性查询或者开发模式下非常有用。然而，你可以看到简洁带来了隐晦和调试困难。而且它很脆弱——查询字符串中一个细小的语法错误，像-、:、/或”错位就会导致返回错误而不是结果。最后，查询字符串搜索允许任意用户在索引中任何一个字段上运行潜在的慢查询语句，可能暴露私有信息甚至使你的集群瘫痪。取而代之的，生产环境我们一般依赖全功能的请求体搜索API，它能完成前面所有的事情，甚至更多。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(3)——单台服务器部署多个节点]]></title>
      <url>%2F2017%2F02%2F10%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-3-%E2%80%94%E2%80%94%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E5%A4%9A%E4%B8%AA%E8%8A%82%E7%82%B9%2F</url>
      <content type="text"><![CDATA[一般情况下单台服务器只会部署一个ElasticSearch node，但是在学习过程中，很多情况下会需要实现ElasticSearch的分布式效果，所以需要启动多个节点，但是学习开发环境（不想开多个虚拟机实现多个服务器的效果），所以就想着在一台服务器上部署多个结点（下文以2个结点作为例子），两个节点分别称为实例一、二。 1、首先将elasticsearch-5.0.2文件夹再复制一份1cp -R elasticsearch-5.0.2 elasticsearch-5.0.2-node-2 2、主要工作就是修改elasticsearch.yml配置文件。实例二：config目录下的elasticsearch.yml内容将node.name: node-1 修改为 node-2 3、分别开启两个节点1./bin/elasticsearch 4、查询是否成功浏览器访问 http://localhost:9200/_cluster/health?pretty若出现类似如下则表示成功123456789101112131415161718&#123; &quot;cluster_name&quot;: &quot;es-lzr&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, &quot;number_of_data_nodes&quot;: 2, &quot;active_primary_shards&quot;: 17, &quot;active_shards&quot;: 34, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot;: 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 100.0&#125; 也可以通过Kibana查看节点效果 http://localhost:5601 账号 elastic 密码 changeme 踩过的坑1.如果修改了配置文件的 http.port、transport.tcp.port项，一定要将各个节点的值设置不同，否则会出现占用的情况。正常如果不修改，默认会分配值。2.示例二开启时，实例一报警告（实际操作中可以忽略）： [2016-12-11T18:06:43,678][WARN ][o.e.d.z.ElectMasterService] [node-1] value for setting “discovery.zen.minimum_master_nodes” is too low. This can result in data loss! Please set it to at least a quorum of master-eligible nodes (current value: [-1], total number of master-eligible nodes used for publishing in this round: [2])是因为默认情况下 discovery.zen.minimum_master_nodes=1 一台服务器只能有一个主节点，所以在实例二的配置文件中可以添加 node.master: false 。 3.示例二不能开启，报如下错误： [2016-12-11T16:53:02,711][INFO ][o.e.d.z.ZenDiscovery ] [node-2] failed to send join request to master [{node-1}{vP19PMOyT2ilJKRAqgn78w}{jDULCExERXGHp4VXpbyuJA}{127.0.0.1}{127.0.0.1:9300}], reason [RemoteTransportException[[node-1][127.0.0.1:9300][internal:discovery/zen/join]]; nested: IllegalArgumentException[can’t add node {node-2}{vP19PMOyT2ilJKRAqgn78w}{qhDDVzwZQ0GXZXhIMmpGKA}{127.0.0.1}{127.0.0.1:9301}, found existing node {node-1}{vP19PMOyT2ilJKRAqgn78w}{jDULCExERXGHp4VXpbyuJA}{127.0.0.1}{127.0.0.1:9300} with the same id but is a different node instance]; ][2016-12-11T16:53:02,911][INFO ][o.e.x.m.e.Exporters ] [node-2] skipping exporter [default_local] as it isn’t ready yet[2016-12-11T16:53:02,912][ERROR][o.e.x.m.AgentService ] [node-2] exception when exporting documentsorg.elasticsearch.xpack.monitoring.exporter.ExportException: exporters are either not ready or faulty at org.elasticsearch.xpack.monitoring.exporter.Exporters.export(Exporters.java:188) ~[x-pack-5.0.2.jar:5.0.2] at org.elasticsearch.xpack.monitoring.AgentService$ExportingWorker.run(AgentService.java:208) [x-pack-5.0.2.jar:5.0.2] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111] 是因为复制的elasticsearch文件夹下包含了data文件中示例一的节点数据，需要把示例二data文件下的文件清空。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为了保持逼格，不要停止写作]]></title>
      <url>%2F2017%2F02%2F10%2F%E4%B8%BA%E4%BA%86%E4%BF%9D%E6%8C%81%E9%80%BC%E6%A0%BC%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%81%9C%E6%AD%A2%E5%86%99%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[“逼格”一词，网络上常见的解释是“装逼的格调”，听起来有一些嘲讽的味道。但我认为，“逼格”应该是一种大隐隐于市的淡定，一种出淤泥而不染的清高，一种观望世事变迁而不为之动容的气质，一种身处外部世界的混乱却依然能听从内部世界的召唤的能力。你觉得我这番解释是装逼也无所谓，因为我的理想一直以来就是做一个有逼格的人。 追求逼格的过程不是一蹴而就的，更不是一劳永逸的，而更像是逆水行舟，一刻都万万不可以松懈。忍不住让我想起柏邦妮说过的一句话：“哪有什么胜利可言，挺住就是一切。” 所以，追求逼格可以被看做是一种坚持不懈的抗争，一种保护内部世界的完整饱足而不被外部世界侵蚀的抗争。这种抗争，本身就足以令人肃然起敬。如此看来，逼格恰恰存在于追求逼格的过程之中。 保护内部世界之完整饱足的方法有很多，而我独爱写作。我一直信奉，虽然不是每个人都可以把写作当成自己的职业，但所有人都一定可以把写作当成自己的一种状态。 在我眼中，世间的写作不过两种，对内的和对外的。今天来聊聊我理想中的写作吧。 对内的写作，是内部世界在巩固自己的版图，是往下扎根。典型的形式包括写日记——你是作者，你也是唯一的读者。如果，你在落笔的当下决定，读者除了你自己还可以包括其他人，即使再少数的其他人都好，那都不能被归为“对内的写作”了。当然，如果你已经完成了写作的这个部分，只不过在之后的阅读过程中觉得，公布出来被他人看见也无妨，那又另当别论。这里讨论的，只是写作这个动作进行之时的内心向度。 对内的写作应该是繁杂凌乱的、忠诚于自己所念所想的，应该完完全全原汁原味地呈现一个自由的内心世界，所以没有必要进行自我审查。想起以前学摄影的时候，教授叮嘱我们拿起相机就一定要无所顾忌、疯狂地拍，把照片导出来之后才开始进行筛选，千万不要把创作和编辑的过程混在一块儿，千万不要在拍的时候就缩手缩脚，或者直接在相机上删掉所谓的不满意的照片。同理，对内的写作应该是零编辑、只关注在创作之上的，这是至关重要的资本原始积累，所以格局一定要大。 对内的写作是一个认识自己的过程，所以要诚实地面对内心的所有情感，尤其是那些不足为外人道也的阴暗的部分。比如说，跟你走得很近的人里有一个你很鄙视的人，但是你又不得不装作和ta相处起来很愉快，那你不妨可以在对内的写作过程中梳理这种情感。挖掘自己的邪恶天赋，并承认它们，是帮助自己成长为一个丰富的人的必经之路。 对内的写作也是一个用来讨好自己的过程。在我以前还很热衷于写日记的时候，我的日记本是一本红色软皮抄。我喜欢它封面的颜色，更喜欢它米色的空白内页。内页纸张的触感很好，而且不会晕墨，真的是难能可贵的品质。我喜欢用墨蓝或者深绿的墨水写日记，尤其喜欢用英雄616钢笔。偏好的场所包括图书馆的静音区和自己的房间。我尤其喜欢物理意义上的下笔，虽然有的时候也在电脑上写日记，但总感觉动笔的信息量更大，毕竟笔迹可以直白地展示当时的心理状态，而且我总喜欢看自己在不同时期的笔迹的变化。这些癖好让我觉得对内的写作是一种仪式，是一种褒奖，是一种可以独享的时光。那本红色软皮抄用完之后我仿佛丧失了写日记的兴趣，因为再也没有找到媲美它的日记本了，不得不说，很可惜。 对外的写作，是内部世界朝外部的扩张。我个人主张它必须是功利的。所谓功利，是指功效和实用性，说直白一点就是，既然你选择了对外写作，就应该对你传播的这些信息负责，让它们尽可能高效地被读者所接收。所以对外的写作应该是方便理解的、易于消化的。要呈现这种特性，在对外写作的过程中就务必有相当一部分的精力花在编辑之上。如果说对内的写作单纯需要用心，对外的写作则更需要用脑。 写到这里，不得不提写作与朗读之间的密不可分的关系。 我在上写作课的时候，教授给我们讲过这样一个有趣的故事。说是有一个写作界的大神，看人很准，曾经试过一连好几年的普利策获奖者都是他曾经相中过表扬过的新人。于是有人去问大神，他眼光为什么这么厉害。大神说，我去编辑部里面看啊，看到那些写稿子的时候嘴里在念念有词的人，我就知道他们有戏了。 教授给我们讲这个故事，意图是要让我们爱上朗读自己的作品，不但如此，更要用朗读的方式去体会其他优秀的写作。朗读，实际上是一场非常严肃的检阅。从编辑方面的效用来看，在朗读的过程中，很容易就能挑出错别字、语病、累赘用词等等小毛病，也可以发现段落间逻辑衔接有没有跳脱这样的结构上的问题。 曾经有人评价我的行文是轻松的、不费力的，大概也是因为我总是以口语化的要求来规范自己的写作吧。也有人曾经给我提意见说，你能不能不要念，专心写，因为你的文笔远胜于你的口头表达，要念出来的话等于限制了你文采的发挥。我当然承认，为了口语化理念的贯彻，不得不让渡出一部分高深的辞藻和逻辑。但口语化的理念可以让写作者更多地采用主谓宾的语序，减少繁杂的修饰语，抛弃冗长的句式结构，实际上是一种更生活、更亲近的表达方式。这种亲近可以让写作者的内部世界建立与外部世界友好的交流，而信息的顺畅流通恰好又可以维持一个更饱满鲜活的内部世界。出于这种理念，我个人也非常不喜欢故作傲慢和刻意艰涩的写作，总让人感到一种封闭和敌意。 我本科的最后一年花了整整两个学期在上写作课，但结果却是对写作像一个新朋友一样。 我并不恐慌，相反的，我对这个变化感到无比的兴奋。想到之前写作课教授给我们援引过的一句话，那是美国著名诗人、作家卡尔·桑德堡在72岁的时候说的： I’m still studying verbs and the mystery of how they connect nouns. I am more suspicious of adjectives than at any other time in all my born days. “我仍旧在学习动词，以及它们和名词连接的奥秘。现在的我比任何时候都更怀疑形容词。” 就像是一个幼儿园小朋友在堆积木的时候的有趣的发现。真是一种有美感的状态。 我希望，我也能活出那样的一天。 转自新浪微博 @恢复吃素的F小姐。 说一个我认为最有逼格的故事吧：某日跟某死党微信对话如下:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(2)——Kibana+X-Pack介绍使用]]></title>
      <url>%2F2017%2F02%2F09%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-1-%E2%80%94%E2%80%94Kibana-X-Pack%E4%BB%8B%E7%BB%8D%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[Kibana是一个为 ElasticSearch 提供的数据分析的 Web 接口。可使用它对日志进行高效的搜索、可视化、分析等各种操作。Kibana目前最新的版本5.0.2，回顾一下Kibana 3和Kibana 4的界面。下面的图展示的是Kibana 3的界面，所有的仪表盘直接放置主页。 下面的图展示的是Kibana 4的界面，和Kibana 3最大的区别是将原来的主体分成三个部分，分别是发现页、可视化、仪表盘。下面是目前Kibana 5最新版本的界面。相比较Kibana 4除了界面的风格变化，最主要是功能栏上添加了Timeline、Management和Dev Tools选项。 Discover You can interactively explore your data from the Discover page. You have access to every document in every index that matches the selected index pattern. You can submit search queries, filter the search results, and view document data. You can also see the number of documents that match the search query and get field value statistics. If a time field is configured for the selected index pattern, the distribution of documents over time is displayed in a histogram at the top of the page. 从发现页可以交互地探索ES的数据。可以访问与所选索引模式相匹配的每一个索引中的每一个文档。您可以提交搜索查询、筛选搜索结果和查看文档数据。还可以看到匹配搜索查询和获取字段值统计的文档的数量。如果一个时间字段被配置为所选择的索引模式，则文档的分布随着时间的推移显示在页面顶部的直方图中。 Visualize Visualize enables you to create visualizations of the data in your Elasticsearch indices. You can then build dashboards that display related visualizations.Kibana visualizations are based on Elasticsearch queries. By using a series of Elasticsearch aggregations to extract and process your data, you can create charts that show you the trends, spikes, and dips you need to know about.You can create visualizations from a search saved from Discover or start with a new search query. 可视化能使你创造你的Elasticsearch指标数据的可视化。然后你可以建立仪表板显示相关的可视化。Kibana的可视化是基于Elasticsearch查询。通过一系列的Elasticsearch聚合提取和处理您的数据，您可以创建图表显示你需要知道的关于趋势，峰值和骤降。您可以从搜索保存的搜索中创建可视化或从一个新的搜索查询开始。 Dashboard A Kibana dashboard displays a collection of saved visualizations. You can arrange and resize the visualizations as needed and save dashboards so they be reloaded and shared. 一个仪表板显示Kibana保存的一系列可视化。你可以根据需要安排和调整可视化，并保存仪表盘，可以被加载和共享。 Monitoring从图中可以发现，默认Kibana是没有该选项的。其实，Monitoring是由X-Pack集成提供的。 The X-Pack monitoring components enable you to easily monitor Elasticsearch through Kibana. You can view cluster health and performance in real time as well as analyze past cluster, index, and node metrics. In addition, you can monitor the performance of Kibana itself.When you install X-Pack on your cluster, a monitoring agent runs on each node to collect and index metrics from Elasticsearch. With X-Pack installed in Kibana, you can then view the monitoring data through a set of specialized dashboards. 该X-pack监控组件使您可以通过Kibana轻松地监控ElasticSearch。您可以实时查看集群的健康和性能，以及分析过去的集群、索引和节点度量。此外，您可以监视Kibana本身性能。当你安装X-pack在群集上，监控代理运行在每个节点上收集和指数指标从Elasticsearch。安装在X-pack在Kibana上，您可以查看通过一套专门的仪表板监控数据。回顾安装过程：ES-5-0-2-学习-1-——安装Elasticsearch、Kibana和X-Pack/，可以发现，在安装X-pack的时候分别在ElasticSearch根目录和Kibana根目录下操作。 Graph The X-Pack graph capabilities enable you to discover how items in an Elasticsearch index are related. You can explore the connections between indexed terms and see which connections are the most meaningful. This can be useful in a variety of applications, from fraud detection to recommendation engines.For example, graph exploration could help you uncover website vulnerabilities that hackers are targeting so you can harden your website. Or, you might provide graph-based personalized recommendations to your e-commerce customers.X-Pack provides a simple, yet powerful graph exploration API, and an interactive graph visualization tool for Kibana. Both work with out of the box with existing Elasticsearch indices—you don’t need to store any additional data to use the X-Pack graph features. X-Pack图的能力使你发现一个Elasticsearch索引项是如何相关联的。你可以探索索引条款之间的连接，看看哪些连接是最有意义的。从欺诈检测到推荐引擎，对各种应用中这都是有用的，例如，图的探索可以帮助你发现网站上黑客的目标的漏洞，所以你可以硬化你的网站。或者，您可以为您的电子商务客户提供基于图表的个性化推荐。X-pack提供简单，但功能强大的图形开发API，和Kibana交互式图形可视化工具。使用X-pack图有工作与开销与现有Elasticsearch指标你不需要任何额外的数据存储的特征。 Timelion Timelion is a time series data visualizer that enables you to combine totally independent data sources within a single visualization. It’s driven by a simple expression language you use to retrieve time series data, perform calculations to tease out the answers to complex questions, and visualize the results. Timelion是一个时间序列数据的可视化，可以结合在一个单一的可视化完全独立的数据源。它是由一个简单的表达式语言驱动的，你用来检索时间序列数据，进行计算，找出复杂的问题的答案，并可视化的结果。这个功能由一系列的功能函数组成，同样的查询的结果，也可以通过Dashboard显示查看。 Management The Management application is where you perform your runtime configuration of Kibana, including both the initial setup and ongoing configuration of index patterns, advanced settings that tweak the behaviors of Kibana itself, and the various “objects” that you can save throughout Kibana such as searches, visualizations, and dashboards.This section is pluginable, so in addition to the out of the box capabitilies, packs such as X-Pack can add additional management capabilities to Kibana.管理中的应用是在你执行你的运行时配置kibana，包括初始设置和指标进行配置模式，高级设置，调整自己的行为和Kibana，各种“对象”，你可以查看保存在整个Kibana的内容如发现页，可视化和仪表板。这部分是pluginable，除此之外，X-pack可以给Kibana增加额外的管理能力。You can use X-Pack Security to control what Elasticsearch data users can access through Kibana.When you install X-Pack, Kibana users have to log in. They need to have the kibana_user role as well as access to the indices they will be working with in Kibana.If a user loads a Kibana dashboard that accesses data in an index that they are not authorized to view, they get an error that indicates the index does not exist. X-Pack Security does not currently provide a way to control which users can load which dashboards. 你可以使用X-pack安全控制哪些用户可以访问Elasticsearch数据通过Kibana。当你安装X-pack，Kibana用户登录。他们需要有kibana_user作用以及获得的指标，他们将在Kibana的工作。如果用户加载Kibana仪表板，访问数据的一个索引，他们未被授权查看，他们得到一个错误，表明指数不存在。X-pack安全目前并不提供一种方法来控制哪些用户可以负荷的仪表板。 Dev Tools原先的交互式控制台Sense，使用户方便的通过浏览器直接与Elasticsearch进行交互。从Kibana 5开始改名并直接内建在Kibana，就是Dev Tools选项。注意如果是Kibana 5以上，不能通过以下命令安装Sense。(踩过的坑)1./bin/kibana plugin --install elastic/sense 或者1./bin/kibana-plugin install elastic/sense instead 总结内容比较简单，主要是对Kibana工具的整体功能总结，方便接下来对ElasticSearch 5的学习，其中X-Pack主要是添加身份权限的验证，以及原先需要安装其他各种Marvel、Hand等各种功能插件添加到Kibana上使用才能使用的功能。学习链接：X-Pack：https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.htmlKibana：https://www.elastic.co/guide/en/kibana/current/introduction.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Markdown——入门指南]]></title>
      <url>%2F2017%2F02%2F09%2FMarkdown%E2%80%94%E2%80%94%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%2F</url>
      <content type="text"><![CDATA[导语：Markdown 是一种轻量级的「标记语言」，它的优点很多，目前也被越来越多的写作爱好者，撰稿者广泛使用。看到这里请不要被「标记」、「语言」所迷惑，Markdown 的语法十分简单。常用的标记符号也不超过十个，这种相对于更为复杂的 HTML 标记语言来说，Markdown 可谓是十分轻量的，学习成本也不需要太多，且一旦熟悉这种语法规则，会有一劳永逸的效果。 一、认识 Markdown在刚才的导语里提到，Markdown 是一种用来写作的轻量级「标记语言」，它用简洁的语法代替排版，而不像一般我们用的字处理软件 Word 或 Pages 有大量的排版、字体设置。它使我们专心于码字，用「标记」语法，来代替常见的排版格式。例如此文从内容到格式，甚至插图，键盘就可以通通搞定了。目前来看，支持 Markdown 语法的编辑器有很多，包括很多网站（例如简书）也支持了 Markdown 的文字录入。Markdown 从写作到完成，导出格式随心所欲，你可以导出 HTML 格式的文件用来网站发布，也可以十分方便的导出 PDF 格式，这种格式写出的简历更能得到 HR 的好感。甚至可以利用 CloudApp 这种云服务工具直接上传至网页用来分享你的文章，全球最大的轻博客平台 Tumblr，也支持 Mou 这类 Markdown 工具的直接上传。 Markdown 官方文档这里可以看到官方的 Markdown 语法规则文档，当然，后文我也会用自己的方式阐述这些语法的具体用法。 创始人 John Gruber 的 Markdown 语法说明 Markdown 中文版语法说明 使用 Markdown 的优点 专注你的文字内容而不是排版样式，安心写作。 轻松的导出 HTML、PDF 和本身的 .md 文件。 纯文本内容，兼容所有的文本编辑器与字处理软件。 随时修改你的文章版本，不必像字处理软件生成若干文件版本导致混乱。 可读、直观、学习成本低。 使用 Markdown 的误区 We believe that writing is about content, about what you want to say – not about fancy formatting. 我们坚信写作写的是内容，所思所想，而不是花样格式。 — Ulysses for Mac Markdown 旨在简洁、高效，也由于 Markdown 的易读易写，人们用不同的编程语言实现了多个版本的解析器和生成器，这就导致了目前不同的 Markdown 工具集成了不同的功能（基础功能大致相同），例如流程图与时序图，复杂表格与复杂公式的呈现，虽然功能的丰富并没有什么本质的缺点，但终归有些背离初衷，何况在编写的过程中很费神，不如使用专业的工具撰写来的更有效率，所以如果你需实现复杂功能，专业的图形界面工具会更加方便。当然，如果你对折腾这些不同客户端对 Markdown 的定制所带来高阶功能感到愉悦的话，那也是无可厚非的。 二、Markdown 语法的简要规则标题标题是每篇文章都需要也是最常用的格式，在 Markdown 中，如果一段文字被定义为标题，只要在这段文字前加 # 号即可。 #一级标题 ##二级标题 ###三级标题 以此类推，总共六级标题，建议在井号后加一个空格，这是最标准的 Markdown 语法。 列表熟悉 HTML 的同学肯定知道有序列表与无序列表的区别，在 Markdown 下，列表的显示只需要在文字前加上 - 或 * 即可变为无序列表，有序列表则直接在文字前加1. 2. 3. 符号要和文字之间加上一个字符的空格。 引用如果你需要引用一小段别处的句子，那么就要用引用的格式。只需要在文本前加入 &gt; 这种尖括号（大于号）即可 图片与链接注：使用markdown写文章，插入图片的格式为图片名称，这里要说的是链接地址怎么写。对于hexo，有两种方式：使用本地路径：在hexo/source目录下新建一个img文件夹，将图片放入该文件夹下，插入图片时链接即为/img/图片名称。使用微博图床，地址http://weibotuchuang.sinaapp.com/，将图片拖入区域中，会生成图片的URL，这就是链接地址。 粗体与斜体Markdown 的粗体和斜体也非常简单，用两个 包含一段文本就是粗体的语法，用一个 包含一段文本就是斜体的语法。例如：这里是粗体 这里是斜体 表格表格是我觉得 Markdown 比较累人的地方，例子如下：这种语法生成的表格如下： 代码框如果你是个程序猿，需要在文章里优雅的引用代码框，在 Markdown下实现也非常简单，只需要用两个 把中间的代码包裹起来。this is code`使用 tab 键即可缩进。 块代码两种方式1.代码每一行的前面都加4个空格或一个tab2.第一行和最后一行都是3个，中间的行是代码12this is code block 1this is code block 2 分割线分割线的语法只需要三个 * 号，例如： 三、相关推荐:图床工具用来上传图片获取 URL 地址 Droplr Cloudapp ezShare for Mac 围脖图床修复计划]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES-5.0.2-学习(1)——安装Elasticsearch、Kibana和X-Pack]]></title>
      <url>%2F2017%2F02%2F09%2FES-5-0-2-%E5%AD%A6%E4%B9%A0-1-%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85Elasticsearch%E3%80%81Kibana%E5%92%8CX-Pack%2F</url>
      <content type="text"><![CDATA[安装准备：安装Elasticsearch唯一的要求是安装官方新版的Java，包括对应的Jdk。 安装Elasticsearch首先到官网下载最新版本的Elasticsearch压缩包。可以使用命令，注意将最新的可用的下载链接填入：123curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/ elasticsearch-5.0.2.zipunzip elasticsearch-5.0.2.zipcd elasticsearch-5.0.2 运行ElasticsearchElasticsearch已经准备就绪，执行以下命令可在前台启动：1./bin/elasticsearch 如果想在后台以守护进程模式运行，添加-d参数。打开另一个终端进行测试：1curl &apos;http://localhost:9200/?pretty&apos; 你能看到以下返回信息：12345678910111213&#123;&quot;name&quot;: &quot;vP19PMO&quot;,&quot;cluster_name&quot;: &quot;elasticsearch&quot;,&quot;cluster_uuid&quot;: &quot;IMKMfkMsSrKODIYg5gxgeQ&quot;,&quot;version&quot;: &#123; &quot;number&quot;: &quot;5.0.2&quot;, &quot;build_hash&quot;: &quot;f6b4951&quot;, &quot;build_date&quot;: &quot;2016-11-24T10:07:18.101Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;6.2.1&quot;&#125;,&quot;tagline&quot;: &quot;You Know, for Search&quot;&#125; 这说明你的ELasticsearch集群已经启动并且正常运行。 安装KiabnaKibana是一个为 ElasticSearch 提供的数据分析的 Web 接口。可使用它对日志进行高效的搜索、可视化、分析等各种操作。首先到官网下载最新版本的Kiabna压缩包。可以使用如下命令，注意将最新的可用的下载链接填入：1234wget https://artifacts.elastic.co/downloads/kibana/kibana-5.1.1-linux-x86_64.tar.gzsha1sum kibana-5.1.1-linux-x86_64.tar.gztar -xzf kibana-5.1.1-linux-x86_64.tar.gzcd kibana/ 注意：https://www.elastic.co/downloads/kibana 可以在该地址获取下载链接，一定要选择对于系统和版本。 按照文档的要求，一般情况下kibana的版本必须和Elasticsearch安装的版本一致。 安装X-PackX-Pack是一个Elastic Stack的扩展，将安全，警报，监视，报告和图形功能包含在一个易于安装的软件包中。在Elasticsearch 5.0.0之前，您必须安装单独的Shield，Watcher和Marvel插件才能获得在X-Pack中所有的功能。 下载前提Elasticsearch 5.0.2Kibana 5.0.2 Elasticsearch下载X-Pack在Es的根目录（每个节点），运行 bin/elasticsearch-plugin 进行安装。1bin/elasticsearch-plugin install x-pack/Users/root/Downloads/markdown/763363-20161209181135163-1294099591.png 安装过程中跳出选项现在y即可。如果你在Elasticsearch已禁用自动索引的创建，在elasticsearch.yml配置action.auto_create_index允许X-pack创造以下指标：1action.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history* 运行Elasticsearch。1bin/elasticsearch Kibana下载X-Pack在Kibana根目录运行 bin/kibana-plugin 进行安装。1bin/kibana-plugin install x-pack/Users/root/Downloads/markdown/1.png 安装过程会比较久，耐心等待。运行Kibana。1bin/kibana 验证X-Pack在浏览器上输入：http://localhost:5601/ ，可以打开Kibana，此时需要输入用户名和密码登录，默认分别是 elastic 和 changeme。 安装参考：每个操作系统安装Elasticsearch的文件选择不同，参考：https://www.elastic.co/downloads/elasticsearch，选择对应的文件下载。 安装Kiabna需要根据操作系统做选择，参考：https://www.elastic.co/guide/en/kibana/current/install.html，选择对应的文件下载。*安装X-Pack需要根据Elasticsearch安装不同的方式提供不同的安装方法，参考：https://www.elastic.co/guide/en/x-pack/5.0/installing-xpack.html#installing-xpack。 名词解释在刚接触Elasticsearch的时候，会有很多名词不能理解，或者不知道其中的关系。其中很多是为不同版本的Elasticsearch而存在的。 MarvelMarvel插件：在簇中从每个节点汇集数据。这个插件必须每个节点都得安装。Marvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了Sense。 Sense交互式控制台，使用户方便的通过浏览器直接与Elasticsearch进行交互。 Hand在学习Elasticsearch的过程中，必不可少需要通过一些工具查看es的运行状态以及数据。如果都是通过rest请求，未免太过麻烦，而且也不够人性化。此时，Head插件可以实现基本信息的查看，rest请求的模拟，数据的检索等等。 X-packx-pack是elasticsearch的一个扩展包，将安全，警告，监视，图形和报告功能捆绑在一个易于安装的软件包中，也是官方推荐的。 Kibanakibana是一个与elasticsearch一起工作的开源的分析和可视化的平台。使用kibana可以查询、查看并与存储在elasticsearch索引的数据进行交互操作。使用kibana能执行高级的数据分析，并能以图表、表格和地图的形式查看数据。kibana使得理解大容量的数据变得非常容易。它非常简单，基于浏览器的接口使我们能够快速的创建和分享显示elasticsearch查询结果实时变化的仪表盘。在Elasticsearch 5版本之前，一般都是通过安装Kibana，而后将Marvel、Hand等各种功能插件添加到Kibana上使用。在Elasticsearch 5版本之后，一般情况下只需要安装一个官方推荐的X-pack扩展包即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为什么要写博客]]></title>
      <url>%2F2017%2F02%2F08%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%86%99%E5%8D%9A%E5%AE%A2%2F</url>
      <content type="text"><![CDATA[“为什么要写博客？”似乎是每一位博主都会在博客里提及的一个老生常谈的问题，上到博客老手，下到像我这种文（zhuang）艺（bi）少年都会在博客里侃一侃博客本身。 为什么要写博客？第一次听到这个问题，脑子里便下意识的弹出来一个答案：逼格甚高。不得不承认，能在同龄人中拥有一个自己的博客是个挺酷炫的事情，在别人刷着 QQ 空间，转着脑残日志，脑残说说的时候，我有一片自己可以任意支配的领地，我可以在这片里标新立异的领地里竖起自己思维的旗帜，没有任何干扰，没有任何限制，我能自由的写下自己的想法，吐槽一切我看不惯的事物，是一件很有诱惑力的事情，也是一个可以迅速提升逼格的方法，何乐而不为？（中二气息浓厚） 俗话说的好：“生活是一个不断发现自己以前是个傻逼的过程”，现在的我就觉得以前的我很傻逼，“好好的一个博客竟然被当成了装逼工具”、“毕竟图样图森破”、“黑历史神烦”等等。所以说，到底为什么要写博客？ 为了与过去和未来的自己打照面有位智者(网络上的智者太多了)说过：“写博客是为了和过去以及未来的自己对话”。博客可以作为一个见证，它既可以见证过去也可以见证未来，看自己以前的文章，可以回顾自己的成长，就算是黑历史也可以和现在的我打个照面，看看自己哪些变了，哪些没变。同时，目前正在写的博文也将是未来我的“参考历史”，看看现在的自己和未来的自己又有什么不同，在对比中成长完善自己。 总之，博客可以作为一个时间的媒介，它可以历尽沧海仍不变，供它的主人来了解其本身。 为了表达自己人是会思考的动物，因而美其名曰“高等动物”。听、说、读、写都会调动起大脑不停的思考，不停的运转，为了遵循守恒定律，运转的动能便转化成了一种无形的财富：思维。 人一生思绪万千，思考出来的产物的价值更是不可计其数，为了将无形变为有形，把思考的价值从思维的银行中提现，我选择把它记录下来，而博客便成了提现后财富的保险箱，抽象的思维也有了一种具象的存在——文字。 我是不吐槽会死喵星人，有的话憋着不说出来真的很难受，但有时表达欲过旺，在所有的思绪呼之欲出之时，又会发现自己表达的贫乏和无力，难道思考中也有文字承载不了的东西在里面？认真分析，还是将其归咎于自己的表达能力欠弱，但就是因为弱，所以更要写，憋久了，容易出内伤，所以不吐不快！ 为了交流有时候思绪一来，便会坐到电脑边开始码字，于是洋洋洒洒一大篇文章便发了出去，然后便开始等待人们的赞赏，想想还有点小期待呢，……艾玛，评论来了，打开看看，我靠，这哥们怎么回事，上来就批斗，想砸场子啊，待我把它的评论删了……等等，做人不能这么狭隘，写文章的第一初衷是为了表达自己，其次不就是为了交流想法吗，没有人看的信息就像一座孤岛，无人问津，闭关锁国，只沉浸在自己世界里，满足于自己狭隘的思考。博客总结了你的想法，他人可以交流完善你的想法，获得他人的反馈和反思。一个有时间沉淀的博客，也就成了你的 Mind Palace，进入其中，你的三观，你的思辨都一览无余，人常说看人先看脸其次看内在（不得不说当今还是一个看脸的时代，你们继续读，我先去照照镜子……），博客就是我的内在！ 接下来该写点什么我个人喜欢收藏别人的博客，有些高质量的博客我会反复阅读里面的文章，反复咀嚼，总会有自己的东西。阅读这些博客的过程中，不难发现，好的博客都有共一些共性： 见解 信息量 更新 第一是见解，博主有自己三观，对每一件事有自己独到的见解，这在我看来是最重要的，所以我把他排在第一位，最可怕的事莫过于跟风的人云亦云，久而久之独立思考能力便丧失了。 第二是信息量，好的博客应该给大家带来有价值的东西，毫无信息量的文章读起来便味同嚼蜡，嚼到头来既没味道也浪费时间。 最后一个就是坚持的更新，互联网普及的今天，知识的迭代更新速度呈指数增长，如果跟不上时代的列车，思维的老火车也会生锈抛锚，停滞不前。 写在最后凡事都是由兴趣开头，成功的人会把兴趣坚持下去，能真正做好的事情才算自己的兴趣，不然的话只算是“三分钟热度”，我也希望我能把写博客这件事坚持下去，做好这件事，上面提到的一个好博客的共性就是我努力的标准。 最后，希望自己可以在互联网上留下自己独特的印记吧！]]></content>
    </entry>

    
  
  
</search>
